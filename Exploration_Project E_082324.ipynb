{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBnEuK1Av5OPwjjJtZBM+t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/timothy2077/1st-Rep/blob/master/Exploration_Project%20E_082324.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XbJu8tmvtR6",
        "outputId": "95e610c3-c009-4b51-9da2-67a875e0cf8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.17.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow\n",
        "\n",
        "print(tensorflow.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1. 데이터 수집하기"
      ],
      "metadata": {
        "id": "o0WwBPqDwu-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 라이브러리 설치\n",
        "!pip install pandas\n",
        "\n",
        "# 데이터 다운로드\n",
        "!wget https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\n",
        "\n",
        "# 데이터 읽기\n",
        "import pandas as pd\n",
        "data = pd.read_csv('ChatbotData.csv')\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h44iqNFkwmdm",
        "outputId": "b3774b5f-dbe7-4cae-e8cb-6dcc500e0936",
        "collapsed": true
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "--2024-08-22 23:56:26--  https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 889842 (869K) [text/plain]\n",
            "Saving to: ‘ChatbotData.csv’\n",
            "\n",
            "ChatbotData.csv     100%[===================>] 868.99K  4.90MB/s    in 0.2s    \n",
            "\n",
            "2024-08-22 23:56:26 (4.90 MB/s) - ‘ChatbotData.csv’ saved [889842/889842]\n",
            "\n",
            "                 Q            A  label\n",
            "0           12시 땡!   하루가 또 가네요.      0\n",
            "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
            "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
            "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
            "4          PPL 심하네   눈살이 찌푸려지죠.      0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2. 데이터 전처리하기\n"
      ],
      "metadata": {
        "id": "vMio_N6gw1Ts"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A. 전처리 작업들\n",
        "\n",
        "#### a. 결측값 제거: 데이터셋에서 결측값을 제거하기\n",
        "#### b. 한글 외의 문자 제거: 정규 표현식을 사용하여 한글 외의 문자를 제거하기\n",
        "#### c. 공백 제거: 문자열의 앞뒤 공백을 제거하기\n",
        "#### d. 데이터셋 분할: 데이터를 훈련 데이터와 테스트 데이터로 분할하기"
      ],
      "metadata": {
        "id": "8VvE8LXGyI2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 데이터 로드\n",
        "data = pd.read_csv('ChatbotData.csv')\n",
        "\n",
        "# 데이터 확인\n",
        "print(data.head())\n",
        "\n",
        "# 결측값 제거\n",
        "data = data.dropna()\n",
        "\n",
        "# 한글 외의 문자 제거\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]', '', text)\n",
        "    return text\n",
        "\n",
        "data['Q'] = data['Q'].apply(clean_text)\n",
        "data['A'] = data['A'].apply(clean_text)\n",
        "\n",
        "# 소문자 변환 (한글 데이터에는 필요 없을 수 있음)\n",
        "# data['Q'] = data['Q'].str.lower()\n",
        "# data['A'] = data['A'].str.lower()\n",
        "\n",
        "# 공백 제거\n",
        "data['Q'] = data['Q'].str.strip()\n",
        "data['A'] = data['A'].str.strip()\n",
        "\n",
        "# 데이터셋 분할 (훈련 데이터와 테스트 데이터)\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# 전처리된 데이터 확인\n",
        "print(train_data.head())\n",
        "print(test_data.head())\n",
        "\n",
        "# 전처리된 데이터를 저장 (필요한 경우)\n",
        "train_data.to_csv('train_data.csv', index=False)\n",
        "test_data.to_csv('test_data.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beYSWWLFxBuy",
        "outputId": "14e50111-65ae-4ca9-f315-db18b67fe3a9",
        "collapsed": true
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 Q            A  label\n",
            "0           12시 땡!   하루가 또 가네요.      0\n",
            "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
            "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
            "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
            "4          PPL 심하네   눈살이 찌푸려지죠.      0\n",
            "                      Q                 A  label\n",
            "10512           엄청 로맨틱해       생각만해도 달콤하네요      2\n",
            "1199         대리님이 너무 갈궈      더 웃으면서 대해보세요      0\n",
            "9841   사내커플인데 비밀연애임 답답해  비밀연애가 말도 못하고 힘들죠      2\n",
            "5595        그 사람이 참 그리워            사랑했나봐요      1\n",
            "7228                  왜             궁금하네요      1\n",
            "               Q             A  label\n",
            "8169      죽을거 같네  나쁜 생각 하지 마세요      1\n",
            "900      내일 시험이야    컨디션 조절 하세요      0\n",
            "8075  정말내 자신이 싫다    자신은 사랑해주세요      1\n",
            "7625     이별후 네달째  바쁘게 살면서 잊어가요      1\n",
            "2816     쌍커풀 해볼까       눈은 기본이죠      0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis:\n",
        "\n",
        "일반적인 대화 (label 0):\n",
        "예시:\n",
        "Q: “12시 땡!”\n",
        "A: “하루가 또 가네요.”\n",
        "Q: “1지망 학교 떨어졌어”\n",
        "A: “위로해 드립니다.”\n",
        "일반적인 일상 대화나 위로의 말을 포함\n",
        "\n",
        "감정적인 대화 (label 1):\n",
        "예시:\n",
        "Q: “그 사람이 참 그리워”\n",
        "A: “사랑했나봐요.”\n",
        "Q: “죽을거 같네”\n",
        "A: “나쁜 생각 하지 마세요.”\n",
        "감정적인 표현이나 위로의 말을 포함\n",
        "\n",
        "로맨틱한 대화 (label 2):\n",
        "예시:\n",
        "Q: “엄청 로맨틱해”\n",
        "A: “생각만해도 달콤하네요.”\n",
        "Q: “사내커플인데 비밀연애임 답답해”\n",
        "A: “비밀연애가 말도 못하고 힘들죠.”\n",
        "로맨틱한 감정이나 연애 관련 대화를 포함"
      ],
      "metadata": {
        "id": "RN-NRH2pzg31"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3. SubwordTextEncoder 사용하기"
      ],
      "metadata": {
        "id": "71JV1tsjz5wu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A.내부단어 토크나이저인 SubwordTextEncoder 사용하기\n",
        "\n",
        "#### 1. TensorFlow Datasets SubwordTextEncoder 를 토크나이저로 사용한다.  단어보다 더 작은 단위인 Subword를 기준으로 토크나이징하고,  각 토큰을 고유한 정수로 인코딩 한다.\n",
        "\n",
        "#### 2. 각 문장을 토큰화하고 각 문장의 시작과 끝을 나타내는 START_TOKEN 및 END_TOKEN을 추가한다.\n",
        "\n",
        "#### 3. 최대 길이 MAX_LENGTH 인 40을 넘는 문장들은 필터링한다.\n",
        "\n",
        "#### 4. MAX_LENGTH보다 길이가 짧은 문장들은 40에 맞도록 패딩한다.\n"
      ],
      "metadata": {
        "id": "UIz3ujPJyoQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 데이터 로드\n",
        "data = pd.read_csv('ChatbotData.csv')\n",
        "\n",
        "# 데이터 확인\n",
        "print(data.head())\n",
        "\n",
        "# 결측값 제거\n",
        "data = data.dropna()\n",
        "\n",
        "# 한글 외의 문자 제거\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]', '', text)\n",
        "    return text\n",
        "\n",
        "data['Q'] = data['Q'].apply(clean_text)\n",
        "data['A'] = data['A'].apply(clean_text)\n",
        "\n",
        "# 공백 제거\n",
        "data['Q'] = data['Q'].str.strip()\n",
        "data['A'] = data['A'].str.strip()\n",
        "\n",
        "# 데이터셋 분할 (훈련 데이터와 테스트 데이터)\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# 전처리된 데이터 확인\n",
        "print(train_data.head())\n",
        "print(test_data.head())\n",
        "\n",
        "# TensorFlow Datasets SubwordTextEncoder를 사용하여 토크나이저 정의\n",
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    train_data['Q'].tolist() + train_data['A'].tolist(), target_vocab_size=2**13)\n",
        "\n",
        "# 시작과 끝을 나타내는 토큰 정의\n",
        "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
        "\n",
        "# 최대 길이 정의\n",
        "MAX_LENGTH = 40\n",
        "\n",
        "# 문장을 토큰화하고 시작과 끝 토큰 추가\n",
        "def tokenize_and_filter(inputs, outputs):\n",
        "    tokenized_inputs, tokenized_outputs = [], []\n",
        "\n",
        "    for (sentence1, sentence2) in zip(inputs, outputs):\n",
        "        sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
        "        sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
        "\n",
        "        if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
        "            tokenized_inputs.append(sentence1)\n",
        "            tokenized_outputs.append(sentence2)\n",
        "\n",
        "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
        "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "    return tokenized_inputs, tokenized_outputs\n",
        "\n",
        "# 훈련 데이터 토큰화 및 필터링\n",
        "train_inputs, train_outputs = tokenize_and_filter(train_data['Q'], train_data['A'])\n",
        "\n",
        "# 테스트 데이터 토큰화 및 필터링\n",
        "test_inputs, test_outputs = tokenize_and_filter(test_data['Q'], test_data['A'])\n",
        "\n",
        "# 전처리된 데이터 확인\n",
        "print(train_inputs[:5])\n",
        "print(train_outputs[:5])\n",
        "print(test_inputs[:5])\n",
        "print(test_outputs[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O436GtFq1lqF",
        "outputId": "4a721dae-79bd-4a02-8024-90ebd2315614",
        "collapsed": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 Q            A  label\n",
            "0           12시 땡!   하루가 또 가네요.      0\n",
            "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
            "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
            "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
            "4          PPL 심하네   눈살이 찌푸려지죠.      0\n",
            "                      Q                 A  label\n",
            "10512           엄청 로맨틱해       생각만해도 달콤하네요      2\n",
            "1199         대리님이 너무 갈궈      더 웃으면서 대해보세요      0\n",
            "9841   사내커플인데 비밀연애임 답답해  비밀연애가 말도 못하고 힘들죠      2\n",
            "5595        그 사람이 참 그리워            사랑했나봐요      1\n",
            "7228                  왜             궁금하네요      1\n",
            "               Q             A  label\n",
            "8169      죽을거 같네  나쁜 생각 하지 마세요      1\n",
            "900      내일 시험이야    컨디션 조절 하세요      0\n",
            "8075  정말내 자신이 싫다    자신은 사랑해주세요      1\n",
            "7625     이별후 네달째  바쁘게 살면서 잊어가요      1\n",
            "2816     쌍커풀 해볼까       눈은 기본이죠      0\n",
            "[[8718  409 7724 8719    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718 2675 2686   13    3  581 4197 8719    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718 3086 2409  234 7322  630 8494 1531 8719    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718   53   36  165 1757 8719    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718 2947 8719    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]]\n",
            "[[8718 3063  435 8698 8651 8626 1280 8719    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718    6 4777 4084 8719    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718 7322   29 1135 1131  859 8719    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718 2222 8719    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718 1150 8719    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]]\n",
            "[[8718 4571    5 1396 8719    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718  314  213 1784  759 8719    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718 2111   51  427 1111 8719    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718 1089   37 2679 8719    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718 8698 8602 8603 1415 1588 8494  239 8719    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]]\n",
            "[[8718  418  345  140   31 8719    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718 2408 6365   77 8719    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718 1309   35  718 8719    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718 2250 7240 1188  522 8719    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718 5548 8208 8719    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B. 토큰화 및 패딩 결과 분석\n",
        "\n",
        "### 1. 토큰화된 입력 데이터 (train_inputs): 각 문장은 SubwordTextEncoder를 사용하여 토큰화 실행\n",
        "예시:\n",
        "[8718, 409, 7724, 8719, 0, 0, 0, …]\n",
        "여기서 8718은 시작 토큰, 8719는 끝 토큰을 나타내며 길이가 40보다 짧은 문장들은 0으로 패딩되었다.\n",
        "\n",
        "### 2. 토큰화된 출력 데이터 (train_outputs):\n",
        "입력 데이터와 동일한 방식으로 토큰화 및 패딩되었다.\n",
        "예시:\n",
        "[8718, 3063, 435, 8698, 8651, 8626, 1280, 8719, 0, 0, 0, …]\n",
        "\n",
        "### 3. 테스트 데이터 (test_inputs, test_outputs):\n",
        "훈련 데이터와 동일한 방식으로 토큰화 및 패딩되었다.\n",
        "예시:\n",
        "[8718, 4571, 5, 1396, 8719, 0, 0, 0, …]"
      ],
      "metadata": {
        "id": "dzfE82TL4ZNR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C. 노드 스텝들을 통해 결과물 한 번 더 확인하기"
      ],
      "metadata": {
        "id": "f-1aJo8x6uOT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 단어장(Vocabulary) 만들기"
      ],
      "metadata": {
        "id": "_C3Awr3x64tL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "print(\"Success is walking from failure to failure with no loss of enthusiasm! 👐\")\n",
        "\n",
        "# Define 'questions' and 'answers' here. For example:\n",
        "questions = [\"How are you?\", \"What's your name?\"]\n",
        "answers = [\"I'm doing well, thanks!\", \"I'm a large language model.\"]\n",
        "\n",
        "# 질문과 답변 데이터셋에 대해서 Vocabulary 생성\n",
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)\n",
        "print(\"슝=3 \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7UdcNFC9AQK",
        "outputId": "eb95c37f-ba4e-4d97-dad0-36f7d8a3bf4b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success is walking from failure to failure with no loss of enthusiasm! 👐\n",
            "슝=3 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### a. 디코더의 문장 생성 과정에서 사용할 '시작 토큰'과 '종료 토큰'에 대해서도 임의로 단어장에 추가하여서 정수를 부여하기\n"
      ],
      "metadata": {
        "id": "3GcO7JZq9FSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 시작 토큰과 종료 토큰에 고유한 정수를 부여합니다.\n",
        "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
        "print(\"슝=3\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X32Z62pT9W9v",
        "outputId": "3eb31512-0988-4b7d-f323-bc52690cdc3b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('START_TOKEN의 번호 :' ,[tokenizer.vocab_size])\n",
        "print('END_TOKEN의 번호 :' ,[tokenizer.vocab_size + 1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9da4gLiDpNX",
        "outputId": "64cfccbe-0b72-479e-ebca-fca7d1e9f74f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "START_TOKEN의 번호 : [273]\n",
            "END_TOKEN의 번호 : [274]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "각각 273과 274라는 점에서 현재 단어장의 크기가 273(0번부터 272번)이라는 의미이다."
      ],
      "metadata": {
        "id": "bqoI4BOuD3d1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 시작 토큰과 종료 토큰을 고려하여 +2를 하여 단어장의 크기를 산정합니다.\n",
        "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
        "print(VOCAB_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_U6wHx2DyBF",
        "outputId": "c862251a-e1e3-43e7-f19c-b01c6536489a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 각 단어를 고유한 정수로 인코딩(Integer encoding) & 패딩(Padding)"
      ],
      "metadata": {
        "id": "Y_DJjxLKEUGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 임의의 22번째 샘플에 대해서 정수 인코딩 작업을 수행.\n",
        "# 각 토큰을 고유한 정수로 변환\n",
        "# 리스트는 0부터 시작하므로 21번째는 index 20입니다.\n",
        "print('정수 인코딩 후의 21번째 질문 샘플: {}'.format(tokenizer.encode(questions[0])))  # 0 또는 1로 변경\n",
        "print('정수 인코딩 후의 21번째 답변 샘플: {}'.format(tokenizer.encode(answers[0])))  # 0 또는 1로 변경"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1V8FlVHErqe",
        "outputId": "3075381f-7a5e-4dde-f2ed-d3ae94ac57a6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정수 인코딩 후의 21번째 질문 샘플: [15, 12, 3, 80]\n",
            "정수 인코딩 후의 21번째 답변 샘플: [90, 56, 1, 11, 4, 16, 5, 50]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 샘플의 최대 허용 길이 또는 패딩 후의 최종 길이\n",
        "MAX_LENGTH = 40\n",
        "print(MAX_LENGTH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5LCmVKbEv3-",
        "outputId": "f14f853c-7b89-43ac-82ce-b1a7b0ebcf94"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 정수 인코딩, 최대 길이를 초과하는 샘플 제거, 패딩\n",
        "def tokenize_and_filter(inputs, outputs):\n",
        "  tokenized_inputs, tokenized_outputs = [], []\n",
        "\n",
        "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
        "    # 정수 인코딩 과정에서 시작 토큰과 종료 토큰을 추가\n",
        "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
        "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
        "\n",
        "    # 최대 길이 40 이하인 경우에만 데이터셋으로 허용\n",
        "    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
        "      tokenized_inputs.append(sentence1)\n",
        "      tokenized_outputs.append(sentence2)\n",
        "\n",
        "  # 최대 길이 40으로 모든 데이터셋을 패딩\n",
        "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
        "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "  return tokenized_inputs, tokenized_outputs\n",
        "print(\"슝=3\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEvukGsDE0oC",
        "outputId": "7d35cc55-0260-4409-d415-c5684c6e881e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions, answers = tokenize_and_filter(questions, answers)\n",
        "print('단어장의 크기 :',(VOCAB_SIZE))\n",
        "print('필터링 후의 질문 샘플 개수: {}'.format(len(questions)))\n",
        "print('필터링 후의 답변 샘플 개수: {}'.format(len(answers)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxRLHOpBE6Pa",
        "outputId": "6ef41390-af77-4960-f019-7b127c19a049"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어장의 크기 : 275\n",
            "필터링 후의 질문 샘플 개수: 2\n",
            "필터링 후의 답변 샘플 개수: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. 교사 강요(Teacher Forcing) 사용하기"
      ],
      "metadata": {
        "id": "Qp6kj0JrFDDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "# 디코더는 이전의 target을 다음의 input으로 사용합니다.\n",
        "# 이에 따라 outputs에서는 START_TOKEN을 제거하겠습니다.\n",
        "dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'inputs': questions,\n",
        "        'dec_inputs': answers[:, :-1]\n",
        "    },\n",
        "    {\n",
        "        'outputs': answers[:, 1:]\n",
        "    },\n",
        "))\n",
        "\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "print(\"슝=3\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2Dlob__E_py",
        "outputId": "121ca1b3-c7bb-4b6a-f295-5f3e3b4e0d36"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## D. 추가적인 전처리들 수행하기\n",
        "\n",
        "### 1. 데이터 로드 및 전처리: 이전 코드와 동일하게 데이터를 로드하고 전처리하기\n",
        "### 2. 형태소 분석 및 불용어 제거: konlpy 라이브러리의 Okt 클래스를 사용하여 형태소 분석을 수행하고 불용어를 제거하기\n",
        "### 3. 토크나이저 정의 및 토큰화: TensorFlow Datasets의 SubwordTextEncoder를 사용하여 토크나이저를 정의하고 문장을 토큰화하기\n",
        "### 4. 데이터셋 생성: 토큰화된 데이터를 사용하여 TensorFlow 데이터셋을 생성하기\n",
        "\n"
      ],
      "metadata": {
        "id": "Qg0w_oxbFcV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "T93iX7iBHE3J",
        "outputId": "af9f4ff7-f4a0-4811-9ae4-392fc33d56ad"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (24.1)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.5.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "# 데이터 로드\n",
        "data = pd.read_csv('ChatbotData.csv')\n",
        "\n",
        "# 데이터 확인\n",
        "print(data.head())\n",
        "\n",
        "# 결측값 제거\n",
        "data = data.dropna()\n",
        "\n",
        "# 한글 외의 문자 제거\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]', '', text)\n",
        "    return text\n",
        "\n",
        "data['Q'] = data['Q'].apply(clean_text)\n",
        "data['A'] = data['A'].apply(clean_text)\n",
        "\n",
        "# 공백 제거\n",
        "data['Q'] = data['Q'].str.strip()\n",
        "data['A'] = data['A'].str.strip()\n",
        "\n",
        "# 형태소 분석 및 불용어 제거\n",
        "okt = Okt()\n",
        "stopwords = ['은', '는', '이', '가', '을', '를', '에', '의', '와', '한', '하다']\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = okt.morphs(text)\n",
        "    tokens = [word for word in tokens if word not in stopwords]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "data['Q'] = data['Q'].apply(preprocess_text)\n",
        "data['A'] = data['A'].apply(preprocess_text)\n",
        "\n",
        "# 데이터셋 분할 (훈련 데이터와 테스트 데이터)\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# 전처리된 데이터 확인\n",
        "print(train_data.head())\n",
        "print(test_data.head())\n",
        "\n",
        "# TensorFlow Datasets SubwordTextEncoder를 사용하여 토크나이저 정의\n",
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    train_data['Q'].tolist() + train_data['A'].tolist(), target_vocab_size=2**13)\n",
        "\n",
        "# 시작과 끝을 나타내는 토큰 정의\n",
        "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
        "\n",
        "# 최대 길이 정의\n",
        "MAX_LENGTH = 40\n",
        "\n",
        "# 문장을 토큰화하고 시작과 끝 토큰 추가\n",
        "def tokenize_and_filter(inputs, outputs):\n",
        "    tokenized_inputs, tokenized_outputs = [], []\n",
        "\n",
        "    for (sentence1, sentence2) in zip(inputs, outputs):\n",
        "        sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
        "        sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
        "\n",
        "        if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
        "            tokenized_inputs.append(sentence1)\n",
        "            tokenized_outputs.append(sentence2)\n",
        "\n",
        "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
        "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "    return tokenized_inputs, tokenized_outputs\n",
        "\n",
        "# 훈련 데이터 토큰화 및 필터링\n",
        "train_inputs, train_outputs = tokenize_and_filter(train_data['Q'], train_data['A'])\n",
        "\n",
        "# 테스트 데이터 토큰화 및 필터링\n",
        "test_inputs, test_outputs = tokenize_and_filter(test_data['Q'], test_data['A'])\n",
        "\n",
        "# 전처리된 데이터 확인\n",
        "print(train_inputs[:5])\n",
        "print(train_outputs[:5])\n",
        "print(test_inputs[:5])\n",
        "print(test_outputs[:5])\n",
        "\n",
        "# 데이터셋 생성\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'inputs': train_inputs,\n",
        "        'dec_inputs': train_outputs[:, :-1]\n",
        "    },\n",
        "    {\n",
        "        'outputs': train_outputs[:, 1:]\n",
        "    },\n",
        "))\n",
        "\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "print(\"전처리 및 데이터셋 생성 완료!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_N_RmtnFS1W",
        "outputId": "3c632e2a-9f0a-4240-88df-07801a30f97f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 Q            A  label\n",
            "0           12시 땡!   하루가 또 가네요.      0\n",
            "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
            "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
            "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
            "4          PPL 심하네   눈살이 찌푸려지죠.      0\n",
            "                          Q                  A  label\n",
            "10512              엄청 로맨틱 해      생각 만해 도 달콤하네요      2\n",
            "1199             대리 님 너무 갈궈      더 웃으면서 대해 보세요      0\n",
            "9841   사 내 커플 인데 비밀연애 임 답답해  비밀연애 말 도 못 하고 힘들죠      2\n",
            "5595             그 사람 참 그리워           사랑 했나 봐요      1\n",
            "7228                      왜              궁금하네요      1\n",
            "               Q             A  label\n",
            "8169      죽을거 같네  나쁜 생각 하지 마세요      1\n",
            "900     내일 시험 이야    컨디션 조절 하세요      0\n",
            "8075  정말 내 자신 싫다    자신 사랑 해주세요      1\n",
            "7625  이별 후 네 달 째  바쁘게 살면서 잊어가요      1\n",
            "2816   쌍 커 풀 해볼까        눈 기본 죠      0\n",
            "[[8553  468  805   45 8554    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8553 2009 2016   15 8250 8554    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8553  408   27 1016  105 4382 5960 1599 8554    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8553   47    2  222 2045 8554    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8553 6125 8554    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]]\n",
            "[[8553    9 1446    4 7650 8554    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8553   18 3180  640   22 8554    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8553 4382   26    4   34   19  849 8554    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8553    6  251   24 8554    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8553 1071 8554    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]]\n",
            "[[8553 5687 1077 8554    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8553  220  874  153 8554    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8553   97   27   67 1042 8554    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8553   25  112  110  201  378 8554    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8553 6477 5413 3000  238 8554    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]]\n",
            "[[8553  399    9   57   52 8554    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8553 1841 1098   48 8554    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8553   67    6  118 8554    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8553 1742 4347 5924    5 8554    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8553  193  760   50 8554    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]]\n",
            "전처리 및 데이터셋 생성 완료!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 토큰화 및 패딩 결과 분석\n",
        "\n",
        "#### 1. 토큰화된 입력 데이터 (train_inputs):\n",
        "각 문장은 SubwordTextEncoder를 사용하여 토큰화되었다.\n",
        "예시:\n",
        "[8553, 468, 805, 45, 8554, 0, 0, 0, …]\n",
        "여기서 8553은 시작 토큰, 8554는 끝 토큰을 나타낸다.\n",
        "길이가 40보다 짧은 문장들은 0으로 패딩되었다.\n",
        "\n",
        "#### 2. 토큰화된 출력 데이터 (train_outputs):\n",
        "입력 데이터와 동일한 방식으로 토큰화 및 패딩되었다.\n",
        "예시:\n",
        "[8553, 9, 1446, 4, 7650, 8554, 0, 0, …]\n",
        "\n",
        "#### 3. 테스트 데이터 (test_inputs, test_outputs):\n",
        "훈련 데이터와 동일한 방식으로 토큰화 및 패딩되었다.\n",
        "예시:\n",
        "[8553, 5687, 1077, 8554, 0, 0, 0, …]"
      ],
      "metadata": {
        "id": "3CAiu6MqIkjq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4. 모델 구성하기"
      ],
      "metadata": {
        "id": "kgm_NgWeH2FL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A. 인코더 층 함수와 디코더 층 함수를 사용하여 트랜스포머 함수를 정의하기"
      ],
      "metadata": {
        "id": "D2f3AlqkI_eF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer(vocab_size,\n",
        "                num_layers,\n",
        "                units,\n",
        "                d_model,\n",
        "                num_heads,\n",
        "                dropout,\n",
        "                name=\"transformer\"):\n",
        "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
        "\n",
        "  # 인코더에서 패딩을 위한 마스크\n",
        "  enc_padding_mask = tf.keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1, 1, None),\n",
        "      name='enc_padding_mask')(inputs)\n",
        "\n",
        "  # 디코더에서 미래의 토큰을 마스크 하기 위해서 사용합니다.\n",
        "  # 내부적으로 패딩 마스크도 포함되어져 있습니다.\n",
        "  look_ahead_mask = tf.keras.layers.Lambda(\n",
        "      create_look_ahead_mask,\n",
        "      output_shape=(1, None, None),\n",
        "      name='look_ahead_mask')(dec_inputs)\n",
        "\n",
        "  # 두 번째 어텐션 블록에서 인코더의 벡터들을 마스킹\n",
        "  # 디코더에서 패딩을 위한 마스크\n",
        "  dec_padding_mask = tf.keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1, 1, None),\n",
        "      name='dec_padding_mask')(inputs)\n",
        "\n",
        "  # 인코더\n",
        "  enc_outputs = encoder(\n",
        "      vocab_size=vocab_size,\n",
        "      num_layers=num_layers,\n",
        "      units=units,\n",
        "      d_model=d_model,\n",
        "      num_heads=num_heads,\n",
        "      dropout=dropout,\n",
        "  )(inputs=[inputs, enc_padding_mask])\n",
        "\n",
        "  # 디코더\n",
        "  dec_outputs = decoder(\n",
        "      vocab_size=vocab_size,\n",
        "      num_layers=num_layers,\n",
        "      units=units,\n",
        "      d_model=d_model,\n",
        "      num_heads=num_heads,\n",
        "      dropout=dropout,\n",
        "  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
        "\n",
        "  # 완전연결층\n",
        "  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
        "\n",
        "  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)\n",
        "print(\"슝=3\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kgDhcIeI2nm",
        "outputId": "cb9d239c-45fa-4648-f9de-fd4e2f5b4a37"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B. 모델 생성"
      ],
      "metadata": {
        "id": "Z8d45TjXJLpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 데이터 로드\n",
        "data = pd.read_csv('ChatbotData.csv')\n",
        "\n",
        "# 데이터 확인\n",
        "print(data.head())\n",
        "\n",
        "# 결측값 제거\n",
        "data = data.dropna()\n",
        "\n",
        "# 한글 외의 문자 제거\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]', '', text)\n",
        "    return text\n",
        "\n",
        "data['Q'] = data['Q'].apply(clean_text)\n",
        "data['A'] = data['A'].apply(clean_text)\n",
        "\n",
        "# 공백 제거\n",
        "data['Q'] = data['Q'].str.strip()\n",
        "data['A'] = data['A'].str.strip()\n",
        "\n",
        "# 데이터셋 분할 (훈련 데이터와 테스트 데이터)\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# 전처리된 데이터 확인\n",
        "print(train_data.head())\n",
        "print(test_data.head())\n",
        "\n",
        "# TensorFlow Datasets SubwordTextEncoder를 사용하여 토크나이저 정의\n",
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    train_data['Q'].tolist() + train_data['A'].tolist(), target_vocab_size=2**13)\n",
        "\n",
        "# 시작과 끝을 나타내는 토큰 정의\n",
        "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
        "\n",
        "# 최대 길이 정의\n",
        "MAX_LENGTH = 40\n",
        "\n",
        "# 문장을 토큰화하고 시작과 끝 토큰 추가\n",
        "def tokenize_and_filter(inputs, outputs):\n",
        "    tokenized_inputs, tokenized_outputs = [], []\n",
        "\n",
        "    for (sentence1, sentence2) in zip(inputs, outputs):\n",
        "        sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
        "        sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
        "\n",
        "        if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
        "            tokenized_inputs.append(sentence1)\n",
        "            tokenized_outputs.append(sentence2)\n",
        "\n",
        "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
        "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "    return tokenized_inputs, tokenized_outputs\n",
        "\n",
        "# 훈련 데이터 토큰화 및 필터링\n",
        "train_inputs, train_outputs = tokenize_and_filter(train_data['Q'], train_data['A'])\n",
        "\n",
        "# 테스트 데이터 토큰화 및 필터링\n",
        "test_inputs, test_outputs = tokenize_and_filter(test_data['Q'], test_data['A'])\n",
        "\n",
        "# 전처리된 데이터 확인\n",
        "print(train_inputs[:5])\n",
        "print(train_outputs[:5])\n",
        "print(test_inputs[:5])\n",
        "print(test_outputs[:5])\n",
        "\n",
        "# 패딩 마스크 생성 함수\n",
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "# look-ahead 마스크 생성 함수\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask\n",
        "\n",
        "# 인코더 레이어 정의\n",
        "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
        "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "    attention = tf.keras.layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=d_model, name=\"attention\")(inputs, inputs, inputs, attention_mask=padding_mask)\n",
        "\n",
        "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
        "    attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs + attention)\n",
        "\n",
        "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
        "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "    outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention + outputs)\n",
        "\n",
        "    return tf.keras.Model(inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
        "\n",
        "# 디코더 레이어 정의\n",
        "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
        "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
        "    look_ahead_mask = tf.keras.Input(shape=(1, None, None), name=\"look_ahead_mask\")\n",
        "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "    attention1 = tf.keras.layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=d_model, name=\"attention_1\")(inputs, inputs, inputs, attention_mask=look_ahead_mask)\n",
        "\n",
        "    attention1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention1 + inputs)\n",
        "\n",
        "    attention2 = tf.keras.layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=d_model, name=\"attention_2\")(attention1, enc_outputs, enc_outputs, attention_mask=padding_mask)\n",
        "\n",
        "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
        "    attention2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention2 + attention1)\n",
        "\n",
        "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
        "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "    outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(outputs + attention2)\n",
        "\n",
        "    return tf.keras.Model(inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask], outputs=outputs, name=name)\n",
        "\n",
        "# 인코더 정의\n",
        "def encoder(vocab_size, num_layers, units, d_model, num_heads, dropout, name=\"encoder\"):\n",
        "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "    embeddings = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "    outputs = embeddings\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        outputs = encoder_layer(units=units, d_model=d_model, num_heads=num_heads, dropout=dropout, name=f\"encoder_layer_{i}\")([outputs, padding_mask])\n",
        "\n",
        "    return tf.keras.Model(inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
        "\n",
        "# 디코더 정의\n",
        "def decoder(vocab_size, num_layers, units, d_model, num_heads, dropout, name='decoder'):\n",
        "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
        "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
        "    look_ahead_mask = tf.keras.Input(shape=(1, None, None), name='look_ahead_mask')\n",
        "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "\n",
        "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "    embeddings = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "    outputs = embeddings\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        outputs = decoder_layer(units=units, d_model=d_model, num_heads=num_heads, dropout=dropout, name=f\"decoder_layer_{i}\")([outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
        "\n",
        "    return tf.keras.Model(inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask], outputs=outputs, name=name)\n",
        "\n",
        "# 트랜스포머 모델 정의 함수\n",
        "def transformer(vocab_size,\n",
        "                num_layers,\n",
        "                units,\n",
        "                d_model,\n",
        "                num_heads,\n",
        "                dropout,\n",
        "                name=\"transformer\"):\n",
        "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
        "\n",
        "    # 인코더에서 패딩을 위한 마스크\n",
        "    enc_padding_mask = tf.keras.layers.Lambda(\n",
        "        create_padding_mask, output_shape=(1, 1, None),\n",
        "        name='enc_padding_mask')(inputs)\n",
        "\n",
        "    # 디코더에서 미래의 토큰을 마스크 하기 위해서 사용합니다.\n",
        "    # 내부적으로 패딩 마스크도 포함되어져 있습니다.\n",
        "    look_ahead_mask = tf.keras.layers.Lambda(\n",
        "        create_look_ahead_mask,\n",
        "        output_shape=(1, None, None),\n",
        "        name='look_ahead_mask')(dec_inputs)\n",
        "\n",
        "    # 두 번째 어텐션 블록에서 인코더의 벡터들을 마스킹\n",
        "    dec_padding_mask = tf.keras.layers.Lambda(\n",
        "        create_padding_mask, output_shape=(1, 1, None),\n",
        "        name='dec_padding_mask')(inputs)\n",
        "\n",
        "    enc_outputs = encoder(vocab_size=vocab_size, num_layers=num_layers, units=units,\n",
        "                          d_model=d_model, num_heads=num_heads, dropout=dropout)(inputs=[inputs, enc_padding_mask])\n",
        "\n",
        "    dec_outputs = decoder(vocab_size=vocab_size, num_layers=num_layers, units=units,\n",
        "                          d_model=d_model, num_heads=num_heads, dropout=dropout)(\n",
        "        inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
        "\n",
        "    outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
        "\n",
        "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "num_layers = 2\n",
        "d_model = 256\n",
        "num_heads = 8\n",
        "units = 512\n",
        "dropout = 0.1\n",
        "\n",
        "# vocab_size는 시작과 끝 토큰을 고려하여 +2\n",
        "vocab_size = tokenizer.vocab_size + 2\n",
        "\n",
        "# 트랜스포머 모델 생성\n",
        "model = transformer(vocab_size=vocab_size,\n",
        "                    num_layers=num_layers,\n",
        "                    units=units,\n",
        "                    d_model=d_model,\n",
        "                    num_heads=num_heads,\n",
        "                    dropout=dropout)\n",
        "\n",
        "# 모델 요약 출력\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YQOAZesBpSi4",
        "outputId": "08e0f8ff-d805-46e8-8474-b0f797737cae"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 Q            A  label\n",
            "0           12시 땡!   하루가 또 가네요.      0\n",
            "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
            "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
            "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
            "4          PPL 심하네   눈살이 찌푸려지죠.      0\n",
            "                      Q                 A  label\n",
            "10512           엄청 로맨틱해       생각만해도 달콤하네요      2\n",
            "1199         대리님이 너무 갈궈      더 웃으면서 대해보세요      0\n",
            "9841   사내커플인데 비밀연애임 답답해  비밀연애가 말도 못하고 힘들죠      2\n",
            "5595        그 사람이 참 그리워            사랑했나봐요      1\n",
            "7228                  왜             궁금하네요      1\n",
            "               Q             A  label\n",
            "8169      죽을거 같네  나쁜 생각 하지 마세요      1\n",
            "900      내일 시험이야    컨디션 조절 하세요      0\n",
            "8075  정말내 자신이 싫다    자신은 사랑해주세요      1\n",
            "7625     이별후 네달째  바쁘게 살면서 잊어가요      1\n",
            "2816     쌍커풀 해볼까       눈은 기본이죠      0\n",
            "[[8718  409 7724 8719    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718 2675 2686   13    3  581 4197 8719    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718 3086 2409  234 7322  630 8494 1531 8719    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718   53   36  165 1757 8719    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718 2947 8719    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]]\n",
            "[[8718 3063  435 8698 8651 8626 1280 8719    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718    6 4777 4084 8719    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718 7322   29 1135 1131  859 8719    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718 2222 8719    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718 1150 8719    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]]\n",
            "[[8718 4571    5 1396 8719    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718  314  213 1784  759 8719    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718 2111   51  427 1111 8719    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718 1089   37 2679 8719    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718 8698 8602 8603 1415 1588 8494  239 8719    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]]\n",
            "[[8718  418  345  140   31 8719    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718 2408 6365   77 8719    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718 1309   35  718 8719    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718 2250 7240 1188  522 8719    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718 5548 8208 8719    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"transformer\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ inputs (\u001b[38;5;33mInputLayer\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dec_inputs (\u001b[38;5;33mInputLayer\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ enc_padding_mask (\u001b[38;5;33mLambda\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ encoder (\u001b[38;5;33mFunctional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │      \u001b[38;5;34m6,967,296\u001b[0m │ inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],          │\n",
              "│                           │                        │                │ enc_padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ look_ahead_mask (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)  │              \u001b[38;5;34m0\u001b[0m │ dec_inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dec_padding_mask (\u001b[38;5;33mLambda\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder (\u001b[38;5;33mFunctional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │     \u001b[38;5;34m11,175,424\u001b[0m │ dec_inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
              "│                           │                        │                │ encoder[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],         │\n",
              "│                           │                        │                │ look_ahead_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
              "│                           │                        │                │ dec_padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ outputs (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8720\u001b[0m)     │      \u001b[38;5;34m2,241,040\u001b[0m │ decoder[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ inputs (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dec_inputs (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ enc_padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ encoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,967,296</span> │ inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],          │\n",
              "│                           │                        │                │ enc_padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ look_ahead_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)  │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dec_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dec_padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">11,175,424</span> │ dec_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
              "│                           │                        │                │ encoder[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],         │\n",
              "│                           │                        │                │ look_ahead_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
              "│                           │                        │                │ dec_padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ outputs (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8720</span>)     │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,241,040</span> │ decoder[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m20,383,760\u001b[0m (77.76 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,383,760</span> (77.76 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m20,383,760\u001b[0m (77.76 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,383,760</span> (77.76 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 구조 정리\n",
        "\n",
        "#### 위 Transformer 모델은 총 20,383,760개의 파라미터를 가지고 있으며 인코더와 디코더로 구성되어 있다. 각 레이어는 멀티-헤드 어텐션과 피드 포워드 신경망으로 이루어져 있으며 입력 시퀀스를 인코딩하고 디코딩하여 최종 출력을 생성한다.\n",
        "\n",
        "#### 입력 레이어 (inputs, dec_inputs):\n",
        "inputs: 입력 시퀀스를 받는 레이어\n",
        "dec_inputs: 디코더 입력 시퀀스를 받는 레이어\n",
        "\n",
        "#### 마스크 레이어 (enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
        "enc_padding_mask: 인코더에서 패딩을 위한 마스크를 생성\n",
        "look_ahead_mask: 디코더에서 미래의 토큰을 마스크하기 위한 마스크를 생성\n",
        "dec_padding_mask: 디코더에서 인코더의 벡터들을 마스킹하기 위한 마스크를 생성\n",
        "\n",
        "#### 인코더 (encoder):\n",
        "인코더는 입력 시퀀스와 패딩 마스크를 받아서 인코딩된 출력 벡터를 생성\n",
        "인코더는 여러 개의 인코더 레이어로 구성되어 있으며 각 레이어는 멀티-헤드 어텐션과 피드 포워드 신경망으로 구성\n",
        "파라미터 수: 6,967,296\n",
        "\n",
        "#### 디코더 (decoder):\n",
        "디코더는 디코더 입력 시퀀스, 인코더 출력, look-ahead 마스크, 패딩 마스크를 받아서 디코딩된 출력 벡터를 생성\n",
        "디코더는 여러 개의 디코더 레이어로 구성되어 있으며 각 레이어는 두 개의 멀티-헤드 어텐션과 피드 포워드 신경망으로 구성\n",
        "파라미터 수: 11,175,424\n",
        "\n",
        "#### 출력 레이어 (outputs):\n",
        "디코더의 출력을 받아서 최종 출력 시퀀스를 생성\n",
        "파라미터 수: 2,241,040\n",
        "\n",
        "#### 총 파라미터 수\n",
        "총 파라미터 수: 20,383,760 (약 77.76 MB)\n",
        "학습 가능한 파라미터 수: 20,383,760\n",
        "학습 불가능한 파라미터 수: 0\n"
      ],
      "metadata": {
        "id": "P0gjW8BaoOnW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C. 손실 함수(Loss function)"
      ],
      "metadata": {
        "id": "1_YMEwGssP0g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 레이블인 시퀀스에 패딩이 되어 있으므로, loss를 계산할 때 패딩 마스크를 적용해야 합니다."
      ],
      "metadata": {
        "id": "_wmUBlDQspdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(y_true, y_pred):\n",
        "    # Remove the last token from y_true\n",
        "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
        "\n",
        "    # Remove the predictions for the last token from y_pred\n",
        "    y_pred = y_pred[:, :-1, :]\n",
        "\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')(y_true, y_pred)\n",
        "\n",
        "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
        "    loss = tf.multiply(loss, mask)\n",
        "    return tf.reduce_mean(loss)\n",
        "\n"
      ],
      "metadata": {
        "id": "j6e1JjaEsA7k"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## D. 커스텀 된 학습률(Learning Rate)\n",
        "### 모델학습 초기에 learning rate를 급격히 높였다가, 이후 train step이 진행됨에 따라 서서히 낮추어 가면서 안정적으로 수렴하게 하는 고급 기법을 널리 사용하고 있는데 이런 방법을 커스텀 학습률 스케줄링(Custom Learning rate Scheduling)"
      ],
      "metadata": {
        "id": "yjGLtpbFszpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    step = tf.cast(step, tf.float32)  # 여기서 step을 float32로 변환\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps**-1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n"
      ],
      "metadata": {
        "id": "9QKXICW2tHPw"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sample_learning_rate = CustomSchedule(d_model=128)\n",
        "\n",
        "plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32))) # Now plt is defined and can be used\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.xlabel(\"Train Step\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        },
        "id": "xA1kebYrt-et",
        "outputId": "ca510400-1f01-496a-f9f9-8accd2bdb9b8"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Train Step')"
            ]
          },
          "metadata": {},
          "execution_count": 43
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 학습 초기에는 learning_rate가 Train Step수에 비례해서 증가하다가 이후로는 감소하는 것을 확인할 수 있다."
      ],
      "metadata": {
        "id": "f66Pwj9dEayZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E. 모델 컴파일"
      ],
      "metadata": {
        "id": "gExPsaR_uJ5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "  # y_pred의 마지막 타임스텝 제거\n",
        "  y_pred = y_pred[:, :MAX_LENGTH - 1, :]\n",
        "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
        "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
        "\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
        "print(\"슝=3\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwgjd8BvuUIA",
        "outputId": "11e9f188-23df-4636-930e-a18ba0d9c61b",
        "collapsed": true
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## F. 훈련하기"
      ],
      "metadata": {
        "id": "TM4whV38vRV7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Model Modification"
      ],
      "metadata": {
        "id": "R5Ok15c7yCjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 데이터 로드\n",
        "data = pd.read_csv('ChatbotData.csv')\n",
        "\n",
        "# 데이터 확인\n",
        "print(data.head())\n",
        "\n",
        "# 결측값 제거\n",
        "data = data.dropna()\n",
        "\n",
        "# 한글 외의 문자 제거\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]', '', text)\n",
        "    return text\n",
        "\n",
        "data['Q'] = data['Q'].apply(clean_text)\n",
        "data['A'] = data['A'].apply(clean_text)\n",
        "\n",
        "# 공백 제거\n",
        "data['Q'] = data['Q'].str.strip()\n",
        "data['A'] = data['A'].str.strip()\n",
        "\n",
        "# 데이터셋 분할 (훈련 데이터와 테스트 데이터)\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# 전처리된 데이터 확인\n",
        "print(train_data.head())\n",
        "print(test_data.head())\n",
        "\n",
        "# TensorFlow Datasets SubwordTextEncoder를 사용하여 토크나이저 정의\n",
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    train_data['Q'].tolist() + train_data['A'].tolist(), target_vocab_size=2**13)\n",
        "\n",
        "# 시작과 끝을 나타내는 토큰 정의\n",
        "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
        "\n",
        "# 최대 길이 정의\n",
        "MAX_LENGTH = 40\n",
        "\n",
        "# 문장을 토큰화하고 시작과 끝 토큰 추가\n",
        "def tokenize_and_filter(inputs, outputs):\n",
        "    tokenized_inputs, tokenized_outputs = [], []\n",
        "\n",
        "    for (sentence1, sentence2) in zip(inputs, outputs):\n",
        "        sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
        "        sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
        "\n",
        "        if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
        "            tokenized_inputs.append(sentence1)\n",
        "            tokenized_outputs.append(sentence2)\n",
        "\n",
        "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
        "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "    return tokenized_inputs, tokenized_outputs\n",
        "\n",
        "# 훈련 데이터 토큰화 및 필터링\n",
        "train_inputs, train_outputs = tokenize_and_filter(train_data['Q'], train_data['A'])\n",
        "\n",
        "# 테스트 데이터 토큰화 및 필터링\n",
        "test_inputs, test_outputs = tokenize_and_filter(test_data['Q'], test_data['A'])\n",
        "\n",
        "# 전처리된 데이터 확인\n",
        "print(train_inputs[:5])\n",
        "print(train_outputs[:5])\n",
        "print(test_inputs[:5])\n",
        "print(test_outputs[:5])\n",
        "\n",
        "# 패딩 마스크 생성 함수\n",
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "# look-ahead 마스크 생성 함수 수정\n",
        "def create_look_ahead_mask(seq):\n",
        "    seq_len = tf.shape(seq)[1]\n",
        "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "    padding_mask = create_padding_mask(seq)\n",
        "    return tf.maximum(look_ahead_mask, padding_mask)\n",
        "\n",
        "# 인코더 레이어 정의\n",
        "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
        "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "    attention = tf.keras.layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=d_model, name=\"attention\")(inputs, inputs, inputs, attention_mask=padding_mask)\n",
        "\n",
        "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
        "    attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs + attention)\n",
        "\n",
        "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
        "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "    outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention + outputs)\n",
        "\n",
        "    return tf.keras.Model(inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
        "\n",
        "# 디코더 레이어 정의\n",
        "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
        "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
        "    look_ahead_mask = tf.keras.Input(shape=(1, None, None), name=\"look_ahead_mask\")\n",
        "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "    attention1 = tf.keras.layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=d_model, name=\"attention_1\")(inputs, inputs, inputs, attention_mask=look_ahead_mask)\n",
        "\n",
        "    attention1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention1 + inputs)\n",
        "\n",
        "    attention2 = tf.keras.layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=d_model, name=\"attention_2\")(attention1, enc_outputs, enc_outputs, attention_mask=padding_mask)\n",
        "\n",
        "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
        "    attention2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention2 + attention1)\n",
        "\n",
        "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
        "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "    outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(outputs + attention2)\n",
        "\n",
        "    return tf.keras.Model(inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask], outputs=outputs, name=name)\n",
        "\n",
        "# 디코더 정의\n",
        "def decoder(vocab_size, num_layers, units, d_model, num_heads, dropout, name='decoder'):\n",
        "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
        "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
        "    look_ahead_mask = tf.keras.Input(shape=(1, None, None), name='look_ahead_mask')\n",
        "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "\n",
        "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "    embeddings = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "    outputs = embeddings\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        outputs = decoder_layer(units=units, d_model=d_model, num_heads=num_heads, dropout=dropout, name=f\"decoder_layer_{i}\")([outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
        "\n",
        "    return tf.keras.Model(inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask], outputs=outputs, name=name)\n",
        "\n",
        "# 트랜스포머 모델 정의 함수에서 수정된 부분\n",
        "def transformer(vocab_size,\n",
        "                num_layers,\n",
        "                units,\n",
        "                d_model,\n",
        "                num_heads,\n",
        "                dropout,\n",
        "                name=\"transformer\"):\n",
        "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
        "\n",
        "    # 인코더에서 패딩을 위한 마스크\n",
        "    enc_padding_mask = tf.keras.layers.Lambda(\n",
        "        create_padding_mask, output_shape=(1, 1, None),\n",
        "        name='enc_padding_mask')(inputs)\n",
        "\n",
        "    # 디코더에서 미래의 토큰을 마스크 하기 위해 사용합니다.\n",
        "    # 내부적으로 패딩 마스크도 포함되어 있습니다.\n",
        "    look_ahead_mask = tf.keras.layers.Lambda(\n",
        "        create_look_ahead_mask, output_shape=(1, None, None),\n",
        "        name='look_ahead_mask')(dec_inputs)\n",
        "\n",
        "    # 두 번째 어텐션 블록에서 인코더의 벡터들을 마스킹\n",
        "    dec_padding_mask = tf.keras.layers.Lambda(\n",
        "        create_padding_mask, output_shape=(1, 1, None),\n",
        "        name='dec_padding_mask')(inputs)\n",
        "\n",
        "    enc_outputs = encoder(vocab_size=vocab_size, num_layers=num_layers, units=units,\n",
        "                          d_model=d_model, num_heads=num_heads, dropout=dropout)(inputs=[inputs, enc_padding_mask])\n",
        "\n",
        "    dec_outputs = decoder(vocab_size=vocab_size, num_layers=num_layers, units=units,\n",
        "                          d_model=d_model, num_heads=num_heads, dropout=dropout)(\n",
        "        inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
        "\n",
        "    outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
        "\n",
        "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)\n",
        "\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "num_layers = 2\n",
        "d_model = 256\n",
        "num_heads = 8\n",
        "units = 512\n",
        "dropout = 0.1\n",
        "\n",
        "# vocab_size는 시작과 끝 토큰을 고려하여 +2\n",
        "vocab_size = tokenizer.vocab_size + 2\n",
        "\n",
        "# 트랜스포머 모델 생성 및 훈련\n",
        "model = transformer(vocab_size=vocab_size,\n",
        "                    num_layers=num_layers,\n",
        "                    units=units,\n",
        "                    d_model=d_model,\n",
        "                    num_heads=num_heads,\n",
        "                    dropout=dropout)\n",
        "\n",
        "# 모델 컴파일 (이미 설정됨)\n",
        "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
        "\n",
        "# 모델 훈련\n",
        "EPOCHS = 10\n",
        "\n",
        "model.fit(train_dataset, epochs=EPOCHS, validation_data=test_dataset)\n",
        "\n",
        "# 모델 저장\n",
        "model.save('chatbot_transformer_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nQXtzEgvyKag",
        "outputId": "cd09fd3e-b306-4593-9234-88500887fcc2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 Q            A  label\n",
            "0           12시 땡!   하루가 또 가네요.      0\n",
            "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
            "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
            "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
            "4          PPL 심하네   눈살이 찌푸려지죠.      0\n",
            "                      Q                 A  label\n",
            "10512           엄청 로맨틱해       생각만해도 달콤하네요      2\n",
            "1199         대리님이 너무 갈궈      더 웃으면서 대해보세요      0\n",
            "9841   사내커플인데 비밀연애임 답답해  비밀연애가 말도 못하고 힘들죠      2\n",
            "5595        그 사람이 참 그리워            사랑했나봐요      1\n",
            "7228                  왜             궁금하네요      1\n",
            "               Q             A  label\n",
            "8169      죽을거 같네  나쁜 생각 하지 마세요      1\n",
            "900      내일 시험이야    컨디션 조절 하세요      0\n",
            "8075  정말내 자신이 싫다    자신은 사랑해주세요      1\n",
            "7625     이별후 네달째  바쁘게 살면서 잊어가요      1\n",
            "2816     쌍커풀 해볼까       눈은 기본이죠      0\n",
            "[[8718  409 7724 8719    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718 2675 2686   13    3  581 4197 8719    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718 3086 2409  234 7322  630 8494 1531 8719    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718   53   36  165 1757 8719    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718 2947 8719    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]]\n",
            "[[8718 3063  435 8698 8651 8626 1280 8719    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718    6 4777 4084 8719    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718 7322   29 1135 1131  859 8719    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718 2222 8719    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718 1150 8719    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]]\n",
            "[[8718 4571    5 1396 8719    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718  314  213 1784  759 8719    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718 2111   51  427 1111 8719    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718 1089   37 2679 8719    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718 8698 8602 8603 1415 1588 8494  239 8719    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]]\n",
            "[[8718  418  345  140   31 8719    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718 2408 6365   77 8719    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718 1309   35  718 8719    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718 2250 7240 1188  522 8719    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [8718 5548 8208 8719    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]]\n",
            "Epoch 1/10\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1917s\u001b[0m 13s/step - accuracy: 0.0116 - loss: 1.2847 - val_accuracy: 0.0256 - val_loss: 1.2631\n",
            "Epoch 2/10\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1910s\u001b[0m 13s/step - accuracy: 0.0256 - loss: 1.1343 - val_accuracy: 0.0256 - val_loss: 1.1293\n",
            "Epoch 3/10\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1898s\u001b[0m 13s/step - accuracy: 0.0256 - loss: 1.0183 - val_accuracy: 0.0256 - val_loss: 1.0815\n",
            "Epoch 4/10\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1813s\u001b[0m 12s/step - accuracy: 0.0258 - loss: 0.9808 - val_accuracy: 0.0261 - val_loss: 1.0676\n",
            "Epoch 5/10\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1849s\u001b[0m 13s/step - accuracy: 0.0261 - loss: 0.9508 - val_accuracy: 0.0261 - val_loss: 1.0621\n",
            "Epoch 6/10\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1847s\u001b[0m 12s/step - accuracy: 0.0261 - loss: 0.9311 - val_accuracy: 0.0261 - val_loss: 1.0611\n",
            "Epoch 7/10\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1823s\u001b[0m 12s/step - accuracy: 0.0263 - loss: 0.9039 - val_accuracy: 0.0260 - val_loss: 1.0599\n",
            "Epoch 8/10\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1824s\u001b[0m 12s/step - accuracy: 0.0270 - loss: 0.8736 - val_accuracy: 0.0258 - val_loss: 1.0600\n",
            "Epoch 9/10\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1824s\u001b[0m 12s/step - accuracy: 0.0286 - loss: 0.8265 - val_accuracy: 0.0251 - val_loss: 1.0620\n",
            "Epoch 10/10\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1815s\u001b[0m 12s/step - accuracy: 0.0322 - loss: 0.7769 - val_accuracy: 0.0253 - val_loss: 1.0649\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Learning rate schedule 'CustomSchedule' must override `get_config()` in order to be serializable.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-9e5321b0a611>\u001b[0m in \u001b[0;36m<cell line: 216>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;31m# 모델 저장\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'chatbot_transformer_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/schedules/learning_rate_schedule.py\u001b[0m in \u001b[0;36mget_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         raise NotImplementedError(\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0;34mf\"Learning rate schedule '{self.__class__.__name__}' \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;34m\"must override `get_config()` in order to be serializable.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Learning rate schedule 'CustomSchedule' must override `get_config()` in order to be serializable."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.  CustomSchedule 클래스에 get_config() 메서드 추가 및 모델 훈련의 출력값 그래프로 표현하기"
      ],
      "metadata": {
        "id": "udKsmTk5A0LV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import os  # Import the 'os' module\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "    def get_config(self): # Add this method to make the schedule serializable\n",
        "        return {\n",
        "            'd_model': self.d_model,\n",
        "            'warmup_steps': self.warmup_steps,\n",
        "        }\n",
        "\n",
        "# Assuming 'model' is defined and compiled with the CustomSchedule somewhere above\n",
        "# Assuming necessary imports and CustomSchedule definition are present\n",
        "\n",
        "learning_rate = CustomSchedule(d_model=d_model) # Assuming d_model is defined\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "model.compile(optimizer=optimizer)\n",
        "if os.path.exists('chatbot_transformer_model.h5'):  # Now 'os' is defined\n",
        "    os.remove('chatbot_transformer_model.h5')\n",
        "    print(\"Existing model file removed.\")\n",
        "\n",
        "\n",
        "# Now save the model\n",
        "# 새로운 Keras 포맷으로 모델 저장\n",
        "model.save('chatbot_transformer_model.keras')\n",
        "print(\"Model saved in new Keras format.\")\n",
        "\n",
        "\n",
        "\n",
        "# 학습 결과 출력값\n",
        "history = {\n",
        "    'accuracy': [0.0116, 0.0256, 0.0256, 0.0258, 0.0261, 0.0261, 0.0263, 0.0270, 0.0286],\n",
        "    'loss': [1.2847, 1.1343, 1.0183, 0.9808, 0.9508, 0.9311, 0.9039, 0.8736, 0.8265],\n",
        "    'val_accuracy': [0.0256, 0.0256, 0.0256, 0.0261, 0.0261, 0.0261, 0.0260, 0.0258, 0.0251],\n",
        "    'val_loss': [1.2631, 1.1293, 1.0815, 1.0676, 1.0621, 1.0611, 1.0599, 1.0600, 1.0620]\n",
        "}\n",
        "\n",
        "# 그래프 그리기\n",
        "epochs = range(1, 10)\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, history['accuracy'], 'bo-', label='Training accuracy')\n",
        "plt.plot(epochs, history['val_accuracy'], 'ro-', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, history['loss'], 'bo-', label='Training loss')\n",
        "plt.plot(epochs, history['val_loss'], 'ro-', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "AcZRjs-EAqj-",
        "outputId": "12a535b9-1930-43fe-91b1-4a396b368d2a",
        "collapsed": true
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Existing model file removed.\n",
            "Model saved in new Keras format.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABAMAAAGJCAYAAAD/rfo3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACvg0lEQVR4nOzdd3hT1RvA8W9a6KJ0sFqghbJnKRsZZQhaEBmy8adMQUGWoALKFGVvRFCUobIECoIgUpZsWS1LNi2zZdMy25Le3x/HBkJXCm2TtO/nee6T5Obk3je3ae65b87QaZqmIYQQQgghhBBCiCzDxtwBCCGEEEIIIYQQImNJMkAIIYQQQgghhMhiJBkghBBCCCGEEEJkMZIMEEIIIYQQQgghshhJBgghhBBCCCGEEFmMJAOEEEIIIYQQQogsRpIBQgghhBBCCCFEFiPJACGEEEIIIYQQIouRZIAQQgghhBBCCJHFSDJAZEldunTBx8fnpV47atQodDpd2gZkYcLCwtDpdCxcuDBD97t9+3Z0Oh3bt283rDP1b5VeMfv4+NClS5c03aYQQgjrJXWI5Ekd4hlz1SEWLlyITqcjLCwsw/ctrIskA4RF0el0Ji3Pf9EL8ar27NnDqFGjuHfvnrlDEUII8ZKkDiHMQeoQwpplM3cAQjzvl19+MXr8888/ExQUlGB9mTJlXmk/8+bNIy4u7qVeO2zYMIYMGfJK+xeme5W/lan27NnD6NGj6dKlC25ubkbPnT59GhsbyZsKIYSlkzqEeJHUIYRIniQDhEV57733jB7v27ePoKCgBOtf9OjRI5ycnEzeT/bs2V8qPoBs2bKRLZv862SUV/lbpQV7e3uz7t9aPHz4kBw5cpg7DCFEFiZ1CPEiqUMIkTxJVQmrU79+fcqXL8+hQ4eoW7cuTk5OfPHFFwD8/vvvNG3alAIFCmBvb0+xYsUYM2YMer3eaBsv9iGL7ys2efJkfvjhB4oVK4a9vT3VqlXjwIEDRq9NrL+fTqejT58+rFmzhvLly2Nvb0+5cuXYuHFjgvi3b99O1apVcXBwoFixYnz//fcm9yHcuXMnbdu2pVChQtjb2+Pt7c0nn3zC48ePE7w/Z2dnrl69SsuWLXF2diZv3rx8+umnCY7FvXv36NKlC66urri5udG5c2eTmrodPHgQnU7HokWLEjz3119/odPp+OOPPwC4ePEivXv3plSpUjg6OpI7d27atm1rUl+2xPr7mRrz0aNH6dKlC0WLFsXBwQFPT0+6devG7du3DWVGjRrFZ599BkCRIkUMzUjjY0usv9+FCxdo27YtuXLlwsnJiddee43169cblYnvu/jbb7/xzTff4OXlhYODAw0bNuTcuXMpvu/UHLN79+7xySef4OPjg729PV5eXnTq1Ilbt24Zyjx58oRRo0ZRsmRJHBwcyJ8/P61ateL8+fNG8b7YfDaxfpTxn6/z58/z1ltvkTNnTv73v/8Bpn9GAU6dOkW7du3Imzcvjo6OlCpVii+//BKAbdu2odPpWL16dYLXLVmyBJ1Ox969e1M8jkII8TypQ0gdIivUIZLy3XffUa5cOezt7SlQoAAff/xxgvd+9uxZWrdujaenJw4ODnh5edGhQwciIyMNZYKCgqhTpw5ubm44OztTqlQpw/+RsC6SmhRW6fbt2zRp0oQOHTrw3nvv4eHhAagBU5ydnRk4cCDOzs5s3bqVESNGEBUVxaRJk1Lc7pIlS7h//z4ffvghOp2OiRMn0qpVKy5cuJBidnnXrl0EBgbSu3dvcubMycyZM2ndujWXLl0id+7cAAQHB9O4cWPy58/P6NGj0ev1fPXVV+TNm9ek971ixQoePXpEr169yJ07N/v372fWrFlcuXKFFStWGJXV6/UEBARQo0YNJk+ezObNm5kyZQrFihWjV69eAGiaRosWLdi1axcfffQRZcqUYfXq1XTu3DnFWKpWrUrRokX57bffEpRfvnw57u7uBAQEAHDgwAH27NlDhw4d8PLyIiwsjDlz5lC/fn3+/fffVP0ik5qYg4KCuHDhAl27dsXT05MTJ07www8/cOLECfbt24dOp6NVq1acOXOGpUuXMm3aNPLkyQOQ5N/k+vXr1KpVi0ePHtGvXz9y587NokWLaN68OStXruSdd94xKj9+/HhsbGz49NNPiYyMZOLEifzvf//jn3/+SfZ9mnrMHjx4gL+/PydPnqRbt25UrlyZW7dusXbtWq5cuUKePHnQ6/W8/fbbbNmyhQ4dOtC/f3/u379PUFAQx48fp1ixYiYf/3hPnz4lICCAOnXqMHnyZEM8pn5Gjx49ir+/P9mzZ6dnz574+Phw/vx51q1bxzfffEP9+vXx9vZm8eLFCY7p4sWLKVasGDVr1kx13EIIIXUIqUNk9jpEYkaNGsXo0aNp1KgRvXr14vTp08yZM4cDBw6we/dusmfPTkxMDAEBAURHR9O3b188PT25evUqf/zxB/fu3cPV1ZUTJ07w9ttvU6FCBb766ivs7e05d+4cu3fvTnVMwgJoQliwjz/+WHvxY1qvXj0N0ObOnZug/KNHjxKs+/DDDzUnJyftyZMnhnWdO3fWChcubHgcGhqqAVru3Lm1O3fuGNb//vvvGqCtW7fOsG7kyJEJYgI0Ozs77dy5c4Z1R44c0QBt1qxZhnXNmjXTnJyctKtXrxrWnT17VsuWLVuCbSYmsfc3btw4TafTaRcvXjR6f4D21VdfGZWtVKmSVqVKFcPjNWvWaIA2ceJEw7qnT59q/v7+GqAtWLAg2XiGDh2qZc+e3eiYRUdHa25ublq3bt2SjXvv3r0aoP3888+Gddu2bdMAbdu2bUbv5fm/VWpiTmy/S5cu1QBtx44dhnWTJk3SAC00NDRB+cKFC2udO3c2PB4wYIAGaDt37jSsu3//vlakSBHNx8dH0+v1Ru+lTJkyWnR0tKHsjBkzNEA7duxYgn09z9RjNmLECA3QAgMDE5SPi4vTNE3T5s+frwHa1KlTkyyT2LHXtGf/G88f1/jP15AhQ0yKO7HPaN26dbWcOXMarXs+Hk1Tny97e3vt3r17hnU3btzQsmXLpo0cOTLBfoQQ4nlSh0j5/UkdInPWIRYsWGAU040bNzQ7OzvtzTffNOxD0zTt22+/1QBt/vz5mqZpWnBwsAZoK1asSHLb06ZN0wDt5s2bycYgrIN0ExBWyd7enq5duyZY7+joaLh///59bt26hb+/P48ePeLUqVMpbrd9+/a4u7sbHvv7+wOqSVdKGjVqZPQLa4UKFXBxcTG8Vq/Xs3nzZlq2bEmBAgUM5YoXL06TJk1S3D4Yv7+HDx9y69YtatWqhaZpBAcHJyj/0UcfGT329/c3ei8bNmwgW7Zshiw/gK2tLX379jUpnvbt2xMbG0tgYKBh3aZNm7h37x7t27dPNO7Y2Fhu375N8eLFcXNz4/Dhwybt62Vifn6/T5484datW7z22msAqd7v8/uvXr06derUMaxzdnamZ8+ehIWF8e+//xqV79q1K3Z2dobHpn6mTD1mq1atws/PL8GvCYCh2eiqVavIkydPosfoVaa4ev5vkFjcSX1Gb968yY4dO+jWrRuFChVKMp5OnToRHR3NypUrDeuWL1/O06dPU+wDLIQQSZE6hNQhMnsd4kWbN28mJiaGAQMGGA1o2KNHD1xcXAzdFFxdXQHVVePRo0eJbit+kMTff/893QdnFOlPkgHCKhUsWNDoyzHeiRMneOedd3B1dcXFxYW8efMaLhqe7+uUlBcvTOJP6nfv3k31a+NfH//aGzdu8PjxY4oXL56gXGLrEnPp0iW6dOlCrly5DH346tWrByR8fw4ODgmaqT0fD6h+ePnz58fZ2dmoXKlSpUyKx8/Pj9KlS7N8+XLDuuXLl5MnTx5ef/11w7rHjx8zYsQIvL29sbe3J0+ePOTNm5d79+6Z9Hd5XmpivnPnDv3798fDwwNHR0fy5s1LkSJFANM+D0ntP7F9xY9OffHiRaP1L/uZMvWYnT9/nvLlyye7rfPnz1OqVKk0HbQqW7ZseHl5JVhvymc0vhKTUtylS5emWrVqLF682LBu8eLFvPbaayb/zwghxIukDiF1iMxeh0hsv5DwfdrZ2VG0aFHD80WKFGHgwIH8+OOP5MmTh4CAAGbPnm30ftu3b0/t2rX54IMP8PDwoEOHDvz222+SGLBSMmaAsErPZ2vj3bt3j3r16uHi4sJXX31FsWLFcHBw4PDhwwwePNikLylbW9tE12ualq6vNYVer+eNN97gzp07DB48mNKlS5MjRw6uXr1Kly5dEry/pOJJa+3bt+ebb77h1q1b5MyZk7Vr19KxY0ejC8++ffuyYMECBgwYQM2aNXF1dUWn09GhQ4d0PXm0a9eOPXv28Nlnn1GxYkWcnZ2Ji4ujcePGGXbSetnPRUYfs6RaCLw4WFQ8e3v7BNMlpfYzaopOnTrRv39/rly5QnR0NPv27ePbb79N9XaEECKe1CGkDmEKa65DvIopU6bQpUsXfv/9dzZt2kS/fv0YN24c+/btw8vLC0dHR3bs2MG2bdtYv349GzduZPny5bz++uts2rQpwz47Im1IMkBkGtu3b+f27dsEBgZSt25dw/rQ0FAzRvVMvnz5cHBwSHQUWFNGhj127Bhnzpxh0aJFdOrUybA+KCjopWMqXLgwW7Zs4cGDB0ZZ8tOnT5u8jfbt2zN69GhWrVqFh4cHUVFRdOjQwajMypUr6dy5M1OmTDGse/LkiUkjDr9szHfv3mXLli2MHj2aESNGGNafPXs2wTZT01S+cOHCiR6f+CakhQsXNnlbyTH1mBUrVozjx48nu61ixYrxzz//EBsbm+QgVvG/Nry4/Rd/pUiOqZ/RokWLAqQYN0CHDh0YOHAgS5cu5fHjx2TPnt2o+agQQqQFqUOkntQhFEusQyS2X1DvM/4cDBATE0NoaCiNGjUyKu/r64uvry/Dhg1jz5491K5dm7lz5/L1118DYGNjQ8OGDWnYsCFTp05l7NixfPnll2zbti3BtoRlk24CItOIz0Q+ny2NiYnhu+++M1dIRmxtbWnUqBFr1qzh2rVrhvXnzp3jzz//NOn1YPz+NE1jxowZLx3TW2+9xdOnT5kzZ45hnV6vZ9asWSZvo0yZMvj6+rJ8+XKWL19O/vz5jSpS8bG/mMWeNWtWkr86p0XMiR0vgOnTpyfYZo4cOYCEF8JJ7X///v1G09o9fPiQH374AR8fH8qWLWvqW0mWqcesdevWHDlyJNEp+OJf37p1a27dupXoL+rxZQoXLoytrS07duwwej41/z+mfkbz5s1L3bp1mT9/PpcuXUo0nnh58uShSZMm/PrrryxevJjGjRsbRmsWQoi0InWI1JM6hGKJdYgXNWrUCDs7O2bOnGn0nn766SciIyNp2rQpAFFRUTx9+tTotb6+vtjY2BAdHQ2o7hMvqlixIoChjLAe0jJAZBq1atXC3d2dzp07069fP3Q6Hb/88ku6NqVKrVGjRrFp0yZq165Nr1690Ov1fPvtt5QvX56QkJBkX1u6dGmKFSvGp59+ytWrV3FxcWHVqlWp7jf2vGbNmlG7dm2GDBlCWFgYZcuWJTAwMNV94dq3b8+IESNwcHCge/fuCZqPv/322/zyyy+4urpStmxZ9u7dy+bNmw3TJaVHzC4uLtStW5eJEycSGxtLwYIF2bRpU6K/8lSpUgWAL7/8kg4dOpA9e3aaNWtmOME/b8iQISxdupQmTZrQr18/cuXKxaJFiwgNDWXVqlUJ3vvLMvWYffbZZ6xcuZK2bdvSrVs3qlSpwp07d1i7di1z587Fz8+PTp068fPPPzNw4ED279+Pv78/Dx8+ZPPmzfTu3ZsWLVrg6upK27ZtmTVrFjqdjmLFivHHH39w48YNk2NOzWd05syZ1KlTh8qVK9OzZ0+KFClCWFgY69evT/C/0KlTJ9q0aQPAmDFjUn8whRAiBVKHSD2pQyiWWId4Ud68eRk6dCijR4+mcePGNG/enNOnT/Pdd99RrVo1w9gYW7dupU+fPrRt25aSJUvy9OlTfvnlF2xtbWndujUAX331FTt27KBp06YULlyYGzdu8N133+Hl5WU0MKKwDpIMEJlG7ty5+eOPPxg0aBDDhg3D3d2d9957j4YNGxrmqjW3KlWq8Oeff/Lpp58yfPhwvL29+eqrrzh58mSKIxVnz56ddevWGfpuOTg48M4779CnTx/8/PxeKh4bGxvWrl3LgAED+PXXX9HpdDRv3pwpU6ZQqVIlk7fTvn17hg0bxqNHjxJtwj1jxgxsbW1ZvHgxT548oXbt2mzevPml/i6piXnJkiX07duX2bNno2kab775Jn/++afRSMwA1apVY8yYMcydO5eNGzcSFxdHaGhooidyDw8P9uzZw+DBg5k1axZPnjyhQoUKrFu3zpBZTwumHjNnZ2d27tzJyJEjWb16NYsWLSJfvnw0bNjQMMCfra0tGzZs4JtvvmHJkiWsWrWK3LlzU6dOHXx9fQ3bmjVrFrGxscydOxd7e3vatWvHpEmTUhzoL15qPqN+fn7s27eP4cOHM2fOHJ48eULhwoVp165dgu02a9YMd3d34uLiaN68eWoPpRBCpEjqEKkndQjFEusQiRk1ahR58+bl22+/5ZNPPiFXrlz07NmTsWPHGroQ+vn5ERAQwLp167h69SpOTk74+fnx559/GmZSaN68OWFhYcyfP59bt26RJ08e6tWrx+jRow2zEQjrodMsKeUpRBbVsmVLTpw4kWhfNCGyuqdPn1KgQAGaNWvGTz/9ZO5whBDCokgdQgjxsmTMACEy2OPHj40enz17lg0bNlC/fn3zBCSEhVuzZg03b940GvRKCCGyIqlDCCHSkrQMECKD5c+fny5duhjmdZ0zZw7R0dEEBwdTokQJc4cnhMX4559/OHr0KGPGjCFPnjwcPnzY3CEJIYRZSR1CCJGWZMwAITJY48aNWbp0KREREdjb21OzZk3Gjh0rJ3EhXjBnzhx+/fVXKlasyMKFC80djhBCmJ3UIYQQaUlaBgghhBBCCCGEEFmMjBkghBBCCCGEEEJkMZIMEEIIIYQQQgghshgZMyAdxcXFce3aNXLmzIlOpzN3OEIIIbI4TdO4f/8+BQoUwMZGfg9IC3KuF0IIYWlMPd9LMiAdXbt2DW9vb3OHIYQQQhi5fPkyXl5e5g4jze3YsYNJkyZx6NAhwsPDWb16NS1btkyy/K5duxg8eDCnTp3i0aNHFC5cmA8//JBPPvnE5H3KuV4IIYSlSul8L8mAdJQzZ05A/RFcXFzMHI0QQoisLioqCm9vb8P5KbN5+PAhfn5+dOvWjVatWqVYPkeOHPTp04cKFSqQI0cOdu3axYcffkiOHDno2bOnSfuUc70QQghLY+r5XpIB6Si+uaCLi4tUEIQQQliMzNqcvUmTJjRp0sTk8pUqVaJSpUqGxz4+PgQGBrJz506TkwFyrhdCCGGpUjrfS4dBIYQQQgggODiYPXv2UK9evSTLREdHExUVZbQIIYQQ1kiSAUIIIYTI0ry8vLC3t6dq1ap8/PHHfPDBB0mWHTduHK6uroZFxgsQQghhrSQZIIQQQogsbefOnRw8eJC5c+cyffp0li5dmmTZoUOHEhkZaVguX76cgZEKIYQQaUfGDDAjTdN4+vQper3e3KEIkeZsbW3Jli1bpu2bLITIPIoUKQKAr68v169fZ9SoUXTs2DHRsvb29tjb22dkeEKITEKv1xMbG2vuMEQmkFb1bEkGmElMTAzh4eE8evTI3KEIkW6cnJzInz8/dnZ25g5FCCFMEhcXR3R0tLnDEEJkMg8ePODKlStommbuUEQmkRb1bEkGmEFcXByhoaHY2tpSoEAB7Ozs5NdTkalomkZMTAw3b94kNDSUEiVKYGMjvZKEEOnrwYMHnDt3zvA4NDSUkJAQcuXKRaFChRg6dChXr17l559/BmD27NkUKlSI0qVLA7Bjxw4mT55Mv379zBK/ECJz0uv1XLlyBScnJ/LmzSv1fvFK0rKeLckAM4iJiSEuLg5vb2+cnJzMHY4Q6cLR0ZHs2bNz8eJFYmJicHBwMHdIQlgdvR527oTwcMifH/z9wdbW3FFZroMHD9KgQQPD44EDBwLQuXNnFi5cSHh4OJcuXTI8HxcXx9ChQwkNDSVbtmwUK1aMCRMm8OGHH2Z47PK3FiLzio2NRdM08ubNi6Ojo7nDEZlAWtWzJRlgRvJLqcjs5DMuxMsLDIT+/eHKlWfrvLxgxgxo1cp8cVmy+vXrJ9sEd+HChUaP+/btS9++fdM5qpTJ31qIrEFaBIi0lBb1bKmpCyGEEBYmMBDatDG+OAS4elWtDww0T1wi7cnfWgghhLlIMkAIIYSwIHq9+pU4sR+449cNGKDKCesmf2shhBDmJMkAK6fXw/btsHSpurXGCoOPjw/Tp083ufz27dvR6XTcu3cv3WISQghz2bkz4a/Ez9M0uHxZlRPWTf7WQojUkHp/+lm4cCFubm7pug9LJMkAKxYYCD4+0KABvPuuuvXxSb8mhTqdLtll1KhRL7XdAwcO0LNnT5PL16pVi/DwcFxdXV9qf0IIYcnOnzetXHh4+sYh0p+pf0P5WwshpN4v9f70IAMIWqn4PoYvNi2M72O4cmXaDzoU/lxtZPny5YwYMYLTp08b1jk7Oxvua5qGXq8nW7aUP2J58+ZNVRx2dnZ4enqm6jWZRUxMzCvNJSqEsFwxMTB3Lgwfblr5/PnTNx6R/kz9G8rfWoisTer9WbPenxGkZYCF0DR4+NC0JSoK+vVLvo9h//6qnCnbS2bgZSOenp6GxdXVFZ1OZ3h86tQpcubMyZ9//kmVKlWwt7dn165dnD9/nhYtWuDh4YGzszPVqlVj8+bNRtt9sbmQTqfjxx9/5J133sHJyYkSJUqwdu1aw/MvNheKb9bz119/UaZMGZydnWncuLHRl9jTp0/p168fbm5u5M6dm8GDB9O5c2datmyZ5Pu9ffs2HTt2pGDBgjg5OeHr68vSpUuNysTFxTFx4kSKFy+Ovb09hQoV4ptvvjE8f+XKFTp27EiuXLnIkSMHVatW5Z9//gGgS5cuCfY/YMAA6tevb3hcv359+vTpw4ABA8iTJw8BAQEATJ06FV9fX3LkyIG3tze9e/fmwYMHRtvavXs39evXx8nJCXd3dwICArh79y4///wzuXPnJjo62qh8y5Ytef/995M8HkKI9KFpsGIFlC377Ls7ufqUTgfe3mrqOWHd/P3VrAFJDTAuf2shMiep9083PLaUen9i5syZQ7FixbCzs6NUqVL88ssvzx17jVGjRlGoUCHs7e0pUKAA/fr1Mzz/3XffUaJECRwcHPDw8KBNmzap2ndGkWSAhXj0CJydTVtcXVUmMCmapvogurqatr1Hj9LufQwZMoTx48dz8uRJKlSowIMHD3jrrbfYsmULwcHBNG7cmGbNmhnN85yY0aNH065dO44ePcpbb73F//73P+7cuZNk+UePHjF58mR++eUXduzYwaVLl/j0008Nz0+YMIHFixezYMECdu/eTVRUFGvWrEk2hidPnlClShXWr1/P8ePH6dmzJ++//z779+83lBk6dCjjx49n+PDh/PvvvyxZsgQPDw8AHjx4QL169bh69Spr167lyJEjfP7558TFxZlwJJ9ZtGgRdnZ27N69m7lz5wJqKpGZM2dy4sQJFi1axNatW/n8888NrwkJCaFhw4aULVuWvXv3smvXLpo1a4Zer6dt27bo9XqjL9obN26wfv16unXrlqrYhBCvZudOqFkT2rVT3QM8PeH772HJEnUh+OJFYvzj6dNlDvrMwNZWTR8ISScE5G8tROYj9X5jllDvf9Hq1avp378/gwYN4vjx43z44Yd07dqVbdu2AbBq1SqmTZvG999/z9mzZ1mzZg2+vr4AHDx4kH79+vHVV19x+vRpNm7cSN26dVO1/wyjiXQTGRmpAVpkZKTR+sePH2v//vuv9vjxY8O6Bw80Tf07Z/zy4EHq39uCBQs0V1dXw+Nt27ZpgLZmzZoUX1uuXDlt1qxZhseFCxfWpk2bZngMaMOGDXvu2DzQAO3PP/802tfdu3cNsQDauXPnDK+ZPXu25uHhYXjs4eGhTZo0yfD46dOnWqFChbQWLVqY+pY1TdO0pk2baoMGDdI0TdOioqI0e3t7bd68eYmW/f7777WcOXNqt2/fTvT5zp07J9h///79tXr16hke16tXT6tUqVKKca1YsULLnTu34XHHjh212rVrJ1m+V69eWpMmTQyPp0yZohUtWlSLi4tLcV+pkdhnXQihaSdPalqLFs++h3Pk0LTRozXt/v1nZVat0jQvL+Pva29vtf5lJXVeEi8vLY5pYn9r0LRq1dIwUCGE2bxYH5J6/zTDY0up97/4HmvVqqX16NHDqEzbtm21t956S9M0VXcuWbKkFhMTk2Bbq1at0lxcXLSoqKgk95cWkqtnm3pukpYBFsLJCR48MG3ZsMG0bW7YYNr2nJzS7n1UrVrV6PGDBw/49NNPKVOmDG5ubjg7O3Py5MkUM4QVKlQw3M+RIwcuLi7cuHEjyfJOTk4UK1bM8Dh//vyG8pGRkVy/fp3q1asbnre1taVKlSrJxqDX6xkzZgy+vr7kypULZ2dn/vrrL0PsJ0+eJDo6moYNGyb6+pCQECpVqkSuXLmS3U9KEotz8+bNNGzYkIIFC5IzZ07ef/99bt++zaP/0r3xLQOS0qNHDzZt2sTV/1LNCxcupEuXLuiS+mlKCJEmIiKgVy8oXx5+/1394vvRR3DuHIwYoX61ideqFYSFwbZtqqXAtm0QGpr2/UKF+b34t160SLUUOHAA/utZJoTIRKTeb8wS6v0vOnnyJLVr1zZaV7t2bU6ePAlA27Ztefz4MUWLFqVHjx6sXr2ap0+fAvDGG29QuHBhihYtyvvvv8/ixYsNdXRLIwMIWgidDnLkMK3sm2+qPoZXrybe70enU8+/+WbGNy3M8cKb+PTTTwkKCmLy5MkUL14cR0dH2rRpQ0xMTLLbyZ49u9FjnU6XbPP6xMprpnaKSsKkSZOYMWMG06dPN/TPHzBggCF2R0fHZF+f0vM2NjYJYoyNjU1Q7sVjGhYWxttvv02vXr345ptvyJUrF7t27aJ79+7ExMTg5OSU4r4rVaqEn58fP//8M2+++SYnTpxg/fr1yb5GCPHyHjyAqVNh4kTVZxOgRQsYNw7KlEn6dba28NwwIiITe/FvvW0bLFwIX34JL3S5FUJYOan3G7OEen9qeXt7c/r0aTZv3kxQUBC9e/dm0qRJ/P333+TMmZPDhw+zfft2Nm3axIgRIxg1ahQHDhywuOkLpWWAFUquj6Gl9SfdvXs3Xbp04Z133sHX1xdPT0/CwsIyNAZXV1c8PDw4cOCAYZ1er+fw4cPJvm737t20aNGC9957Dz8/P4oWLcqZM2cMz5coUQJHR0e2bNmS6OsrVKhASEhIkn2e8ubNazTYCahf9FNy6NAh4uLimDJlCq+99holS5bk2rVrCfadVFzxPvjgAxYuXMiCBQto1KgR3t7eKe5bCJE6T5/CvHlQogSMHKkSAdWrw99/w5o1yScCRNY2ciRkzw5btsDWreaORghhLlLvT52Xrfe/qEyZMuzevdto3e7duylbtqzhsaOjI82aNWPmzJls376dvXv3cuzYMQCyZctGo0aNmDhxIkePHiUsLIytFvhlLskAK9WqlZpGpGBB4/VeXukzvcjLKlGiBIGBgYSEhHDkyBHefffdVA+glxb69u3LuHHj+P333zl9+jT9+/fn7t27yTaLL1GiBEFBQezZs4eTJ0/y4Ycfcv36dcPzDg4ODB48mM8//5yff/6Z8+fPs2/fPn766ScAOnbsiKenJy1btmT37t1cuHCBVatWsXfvXgBef/11Dh48yM8//8zZs2cZOXIkx48fT/G9FC9enNjYWGbNmsWFCxf45ZdfDAMLxhs6dCgHDhygd+/eHD16lFOnTjFnzhxu3bplKPPuu+9y5coV5s2bJwMHCpHGNA3++AP8/KBnT9U9oGhRWL4c9u0DSx1HSJiRXg/bt8PSpbB9Oz7eej78UD315ZemjwAuhMh8pN6fOi9T73/RZ599xsKFC5kzZw5nz55l6tSpBAYGGgYqXLhwIT/99BPHjx/nwoUL/Prrrzg6OlK4cGH++OMPZs6cSUhICBcvXuTnn38mLi6OUqVKpddbfmmSDLBi1tCfdOrUqbi7u1OrVi2aNWtGQEAAlStXzvA4Bg8eTMeOHenUqRM1a9bE2dmZgIAAHBwcknzNsGHDqFy5MgEBAdSvX99wYf+84cOHM2jQIEaMGEGZMmVo3769oc+SnZ0dmzZtIl++fLz11lv4+voyfvx4bP9L3QYEBDB8+HA+//xzqlWrxv379+nUqVOK78XPz4+pU6cyYcIEypcvz+LFixk3bpxRmZIlS7Jp0yaOHDlC9erVqVmzJr///rvR/K+urq60bt0aZ2fnVE+1IoRI2oED0KABNGsG//4LuXOrX3VOnlSzBsjQHCKBwEDw8VEfnHffVbc+PnxVMRBHR5VA+uMPcwcphDAnqfeb7mXq/S9q2bIlM2bMYPLkyZQrV47vv/+eBQsWGKYAd3NzY968edSuXZsKFSqwefNm1q1bR+7cuXFzcyMwMJDXX3+dMmXKMHfuXJYuXUq5cuXS6R2/PJ2W0R0sspCoqChcXV2JjIzExcXFsP7JkyeEhoZSpEiRVH0oRdqJi4ujTJkytGvXjjFjxpg7HLNp2LAh5cqVY+bMmemyffmsi6zkwgX1C+6yZeqxvT0MGABDhoCldBFM6rwkXt4rH9PAQGjTJuFP//9ljX5psZJOa1rh6wshIWAjP+MIYXWkPmRembXen9znytRzk5xSRJZw8eJF5s2bx5kzZzh27Bi9evUiNDSUd99919yhmcXdu3dZvXo127dv5+OPPzZ3OEJYtdu3YeBAKF1aJQJ0OujcGc6cgfHjLScRICyQXg/9+yfeB+C/de8eGIBbTj3HjqluJkIIIZIn9X7TSTJAZAk2NjYsXLiQatWqUbt2bY4dO8bmzZspk0VH76pUqRJdunRhwoQJFtl/SQhr8OQJTJoExYrBtGkQGwtvvAGHD6tR4AsVMneEwuLt3AlXriT9vKZhe/UyM9vuBNT0k4lMOiOEEOI5Uu83nUwtKLIEb2/vBCOCZmUZPbKrEJlJXBwsXgzDhkH81MkVKqjEwJtvmjc2YWVemFEmKW1qhzNoHZw7B4sWwQcfpHNcQghhxaTebzqLaBkwe/ZsfHx8cHBwoEaNGuzfvz/Z8itWrKB06dI4ODjg6+vLhg0bDM/FxsYyePBgw7zwBQoUoFOnTkZTr23fvh2dTpfoEj8NRVhYWKLP79u3L30OghBCCIu3eTNUrQqdOqlEgJeXagVw+LAkAsRLyJ/fpGKORfPzxRfq/ujRqlWKEEII8arMngxYvnw5AwcOZOTIkRw+fBg/Pz8CAgIMI7K/aM+ePXTs2JHu3bsTHBxMy5YtadmypWFKtkePHnH48GGGDx/O4cOHCQwM5PTp0zRv3tywjVq1ahEeHm60fPDBBxQpUoSqVasa7W/z5s1G5apUqZJ+B0MIIYRFOnoUmjRR3QCCg8HFBcaNU+MCdO5sGfM7Cyvk768ySklNMaHTgbc3+Pvz0Ueq6JUr8MJsskIIIcRLMXsyYOrUqfTo0YOuXbtStmxZ5s6di5OTE/Pnz0+0/IwZM2jcuDGfffYZZcqUYcyYMVSuXJlvv/0WUNOlBQUF0a5dO0qVKsVrr73Gt99+y6FDh7j0X3tOOzs7PD09DUvu3Ln5/fff6dq1a4L5J3Pnzm1UNnv27Ol7QIQQQliMK1egWzeoWBE2boTs2dV4b+fPq1kCHB3NHaGwara2at5JSDwhoGkwfTrY2uLgoMYMABg7Fh48yLAohRBCZFJmTQbExMRw6NAhGjVqZFhnY2NDo0aN2Lt3b6Kv2bt3r1F5UPO1J1UeIDIyEp1Oh1sSQzqvXbuW27dv07Vr1wTPNW/enHz58lGnTh3Wrl2b7PuJjo4mKirKaBFCCGF9IiPVNIElS8KCBeqarG1bOHlSXZvlyWPuCEWm0aoVrFwJBQsmfC5PHmjc2PCwSxcoXhxu3nyWQxBCCCFellmTAbdu3UKv1+Ph4WG03sPDg4iIiERfExERkaryT548YfDgwXTs2DHJORZ/+uknAgIC8PLyMqxzdnZmypQprFixgvXr11OnTh1atmyZbEJg3LhxuLq6GhZvb+8kywohhLA8MTEwa5a64Bo7Fh4/hjp1YN8++O03NXOAEGmuVSsIC4Nt22DJEvjzT9U94NYtmDjRUCx7dvjqK3V/0iS4c8c84QohhMgczN5NID3FxsbSrl07NE1jzpw5iZa5cuUKf/31F927dzdanydPHgYOHEiNGjWoVq0a48eP57333mPSpElJ7m/o0KFERkYalsuXL6fp+xFCCJE+NE39OFuuHPTrp67BSpWCNWtgxw6oUcPcEYpMz9YW6teHjh1Va4CpU9X6CRNUouA/7duDr69qvZJMlUQIIYRIkVmTAXny5MHW1pbr168brb9+/Tqenp6JvsbT09Ok8vGJgIsXLxIUFJRkq4AFCxaQO3duowEGk1KjRg3OnTuX5PP29va4uLgYLelOr4ft22HpUnWr16f/Pl9R/fr1GTBggOGxj48P06dPT/Y1Op2ONWvWvPK+02o7QojMY/duqFVLdQM4dw7y5YM5c+D4cWjRIumx3YRIV61bQ4MGauqAQYMMq21s4Ouv1f0ZMyCJhpFCiMxI6v2pkhH1/lGjRlGxYsV03Ud6MmsywM7OjipVqrBlyxbDuri4OLZs2ULNmjUTfU3NmjWNygMEBQUZlY9PBJw9e5bNmzeTO3fuRLelaRoLFiygU6dOJg0MGBISQn4TpwHKEIGB4OOjKgvvvqtufXzU+nTQrFkzGj/Xd/F5O3fuRKfTcfTo0VRv98CBA/Ts2fNVwzOS1D9meHg4TZo0SdN9CSEsV3L1ptOnVevs+G4ATk4wcqRKCHz0EWTLZq6ohUBloWbOVC0GAgPVvJb/adZMtVZ5/Bi++caMMQohMo7U+5Mk9f6XZ/ZuAgMHDmTevHksWrSIkydP0qtXLx4+fGgYzK9Tp04MHTrUUL5///5s3LiRKVOmcOrUKUaNGsXBgwfp06cPoBIBbdq04eDBgyxevBi9Xk9ERAQRERHExMQY7Xvr1q2EhobywQcfJIhr0aJFLF26lFOnTnHq1CnGjh3L/Pnz6du3bzoejVQIDIQ2bdRQ18+7elWtT4cvhu7duxMUFMSVF/eJamFRtWpVKlSokOrt5s2bFycnp7QIMUWenp7Y29tnyL4syYuffSGygqTqTfPnQ+/eqkvA6tXql9aePVUSYNQoyJnTzIELEa98efj4Y3W/Xz+IjQVUnmDsWLX6++/h4kUzxSeEyBhS738pWbXenyqaBZg1a5ZWqFAhzc7OTqtevbq2b98+w3P16tXTOnfubFT+t99+00qWLKnZ2dlp5cqV09avX294LjQ0VAMSXbZt22a0nY4dO2q1atVKNKaFCxdqZcqU0ZycnDQXFxetevXq2ooVK1L1viIjIzVAi4yMNFr/+PFj7d9//9UeP378bGVcnKY9eGDaEhmpaQULaprq5ppw0ek0zctLlTNle3FxJr2f2NhYzcPDQxszZozR+vv372vOzs7anDlztFu3bmkdOnTQChQooDk6Omrly5fXlixZYlS+Xr16Wv/+/Q2PCxcurE2bNs3w+MyZM5q/v79mb2+vlSlTRtu0aZMGaKtXrzaU+fzzz7USJUpojo6OWpEiRbRhw4ZpMTExmqZp2oIFCxL87RcsWKBpmpZgO0ePHtUaNGigOTg4aLly5dJ69Oih3b9/3/B8586dtRYtWmiTJk3SPD09tVy5cmm9e/c27Csx586d05o3b67ly5dPy5Ejh1a1alUtKCjIqMyTJ0+0zz//XPPy8tLs7Oy0YsWKaT/++KPh+ePHj2tNmzbVcubMqTk7O2t16tTRzp07l+jx0zRNa9GihdH/SeHChbWvvvpKe//997WcOXMankvuuMVbu3atVrVqVc3e3l7LnTu31rJlS03TNG306NFauXLlErxfPz8/bdiwYYkei0Q/60JkgFWr1FdhUl+T8UuzZpp24oS5o804SZ2XxMtL92N6546m5cmjPrDPnSs1TdMaNlSru3ZNn10LIdJGgvqQ1PsNj6293j9y5EjNz8/P8Fiv12ujR4/WChYsqNnZ2Wl+fn7an3/+aXg+Ojpa+/jjjzVPT0/N3t5eK1SokDZ27FhN0zQtLi5OGzlypObt7a3Z2dlp+fPn1/r27ZvkvpOrZ5t6brKIZEBmlapkwIMHKdda02t58MDk9/TZZ59pxYoV0+Ke+yKZP3++5ujoqN27d0+7cuWKNmnSJC04OFg7f/68NnPmTM3W1lb7559/DOWT+1LQ6/Va+fLltYYNG2ohISHa33//rVWqVCnBP/OYMWO03bt3a6GhodratWs1Dw8PbcKECZqmadqjR4+0QYMGaeXKldPCw8O18PBw7dGjR5qmGX8pPHjwQMufP7/WqlUr7dixY9qWLVu0IkWKGF1Ud+7cWXNxcdE++ugj7eTJk9q6des0Jycn7YcffkjyGIWEhGhz587Vjh07pp05c0YbNmyY5uDgoF28eNFQpl27dpq3t7cWGBionT9/Xtu8ebO2bNkyTdM07cqVK1quXLm0Vq1aaQcOHNBOnz6tzZ8/Xzt16lSix0/TEk8GuLi4aJMnT9bOnTtnSCQkd9w0TdP++OMPzdbWVhsxYoT277//aiEhIYYvqMuXL2s2Njba/v37DeUPHz6s6XQ67fz584keC0kGCHN4+lTVi5L72sueXdM2bzZ3pBlPkgFpL0OO6bx56oPr4qJp168bVu/bp1bb2GjayZPpt3shxKtJUB+Ser+maZmj3v9iMmDq1Kmai4uLtnTpUu3UqVPa559/rmXPnl07c+aMpmmaNmnSJM3b21vbsWOHFhYWpu3cudOQQFmxYoXm4uKibdiwQbt48aL2zz//JLtvSQZYuMyYDDh58qT2YisLf39/7b333kvyNU2bNtUGDRpkeJzcl8Jff/2lZcuWTbt69arh+T///DPBl8KLJk2apFWpUsXw+MV/zHjPb+eHH37Q3N3dtQfPvf/169drNjY2WkREhKZp6kuhcOHC2tOnTw1l2rZtq7Vv3z7JWBJTrlw5bdasWZqmadrp06c1IEFrgXhDhw7VihQpkmQW0tRkQPwv+sl58bjVrFlT+9///pdk+SZNmmi9evUyPO7bt69Wv379JMtLMkCYw9atpn31vdBYLEuQZEDay5Bj+vSpplWpoj643boZPdW8uVrdtm367V4I8WqsNRkg9f6U6/0v7rtAgQLaN998Y1SmWrVqWu/evTVNU3Xn119/3SjBEm/KlClayZIlk22J8Ly0SAaYfcwA8R8nJ3jwwLRlwwbTtrlhg2nbS0W/ndKlS1OrVi3mz58PwLlz59i5c6dhaka9Xs+YMWPw9fUlV65cODs789dff3Hp0iWTtn/y5Em8vb0pUKCAYV1ig0kuX76c2rVr4+npibOzM8OGDTN5H8/vy8/Pjxw5chjW1a5dm7i4OE6fPm1YV65cOWxtbQ2P8+fPz40bN5Lc7oMHD/j0008pU6YMbm5uODs7c/LkSUN8ISEh2NraUq9evURfHxISgr+/v0mDWianatWqCdaldNxCQkJo2LBhktvs0aMHS5cu5cmTJ8TExLBkyRK6dev2SnEK8aqePoVDh2DaNDUgYIsWpr0uPDx94xIizdjawqxZ6v78+bB/v+GpMWPUGAIrVkBwsJniE0KkjtT7gcxR739eVFQU165do3bt2kbra9euzcmTJwHo0qULISEhlCpVin79+rFp0yZDubZt2/L48WOKFi1Kjx49WL16NU+fPk3V+0wtSQZYCp0OcuQwbXnzTfDySnq+K50OvL1VOVO2l8p5s7p3786qVau4f/8+CxYsoFixYoYL20mTJjFjxgwGDx7Mtm3bCAkJISAgIE0HsNu7dy//+9//eOutt/jjjz8IDg7myy+/TLdB8l68KNfpdMTFxSVZ/tNPP2X16tWMHTuWnTt3EhISgq+vryE+R0fHZPeX0vM2NjZomma0Lva/QaWe9/yXHZh23FLad7NmzbC3t2f16tWsW7fOMGCnEBnpyRPYuVONot64Mbi7Q9WqMHCgGhDw/n3TtmNJk8MIkaKaNaFTJ3W/b1/47zxUoQJ07KhWDxtmptiEEKkj9X6TWXq9P7UqV65MaGgoY8aM4fHjx7Rr185Ql/b29ub06dN89913ODo60rt3b+rWrZtoPT+tSDLAGtnaqsmFIeE/dPzj6dNVuXTQrl07bGxsWLJkCT///DPdunVD999+d+/eTYsWLXjvvffw8/OjaNGinDlzxuRtlylThsuXLxP+3E92+/btMyqzZ88eChcuzJdffknVqlUpUaIEF18YStnOzg59CnOvlilThiNHjvDw4UPDut27d2NjY0OpUqVMjvlFu3fvpkuXLrzzzjv4+vri6elJWFiY4XlfX1/i4uL4+++/E319hQoV2LlzZ5L/+Hnz5jU6Pnq9nuPHj6cYlynHrUKFCgmm7nxetmzZ6Ny5MwsWLGDBggV06NAhxQSCEK8qKgo2boQvv4S6dcHVVd0OGwZ//aV+6HBxgbfegnHjYMcOKFgw5XqTv3/Gvg8hXtn48Wq6i/374eefDatHj1an/A0bYNcuM8YnhEh7Uu+36Hr/81xcXChQoAC7d+82Wr97927Kli1rVK59+/bMmzeP5cuXs2rVKu7cuQOoH+aaNWvGzJkz2b59O3v37uXYsWNpEl9iJBlgrVq1gpUrVY33eV5ean2rVum2a2dnZ9q3b8/QoUMJDw+nS5cuhudKlChBUFAQe/bs4eTJk3z44Ydcv37d5G03atSIkiVL0rlzZ44cOcLOnTv58ssvjcqUKFGCS5cusWzZMs6fP8/MmTNZvXq1URkfHx9CQ0MJCQnh1q1bREdHJ9jX//73PxwcHOjcuTPHjx9n27Zt9O3bl/fffx8PD4/UHZQX4gsMDCQkJIQjR47w7rvvGmUUfXx86Ny5M926dWPNmjWEhoayfft2fvvtNwD69OlDVFQUHTp04ODBg5w9e5ZffvnF0ITp9ddfZ/369axfv55Tp07Rq1cv7t27Z1JcKR23kSNHsnTpUkaOHMnJkyc5duwYEyZMMCrzwQcfsHXrVjZu3ChdBES6uHlTzZL0ySdQpYr65b9JEzWV2s6dEBMDHh5qNqWZM1XT6Dt3YP16GDJEXeTPnKm2ZYZ6kxDpJ39+GDFC3R8yBCIjASheHP5rtcsXX6iOwUKITETq/RZb73/RZ599xoQJE1i+fDmnT59myJAhhISE0L9/fwCmTp1qmL7+zJkzrFixAk9PT9zc3Fi4cCE//fQTx48f58KFC/z66684OjpSuHDhNIvvRZIMsGatWkFYGGzbBkuWqNvQ0HT9QojXvXt37t69S0BAgFE/n2HDhlG5cmUCAgKoX78+np6etGzZ0uTt2tjYsHr1ah4/fkz16tX54IMP+Oabb4zKNG/enE8++YQ+ffpQsWJF9uzZw/Dhw43KtG7dmsaNG9OgQQPy5s3L0qVLE+zLycmJv/76izt37lCtWjXatGlDw4YN+fbbb1N3MF4wdepU3N3dqVWrFs2aNSMgIIDKlSsblZkzZw5t2rShd+/elC5dmh49ehgylblz52br1q08ePCAevXqUaVKFebNm2dottStWzc6d+5Mp06dqFevHkWLFqVBgwYpxmXKcatfvz4rVqxg7dq1VKxYkddff539z/VNBfWlXKtWLUqXLk2NGjVe5VAJAag50n/9FT78EMqUgXz5oHVrdcF++LBqDV2kiGoh/eOPcPq06u+/YoVqLV2xYsILezPWm4RIX/36QcmScP06fPWVYfXw4WBvrxJmz3VBFUJkFlLvt8h6/4v69evHwIEDGTRoEL6+vmzcuJG1a9dSokQJAHLmzMnEiROpWrUq1apVIywsjA0bNmBjY4Obmxvz5s2jdu3aVKhQgc2bN7Nu3Tpy586dpjE+T6e92PlYpJmoqChcXV2JjIzExcXFsP7JkyeEhoZSpEgRHBwczBihEKmnaRolSpSgd+/eDBw4MNmy8lkXL9I0OHVKXbDs2KFuExsDqFw51RXA318tXl4vtz+9Xu0jPFz9qOrvn7VbBCR1XhIvzyzHdONG1VwmWzY4elRl0VDjZkybBpUrw8GDqe4aLIRIJ1IfEukhuc+VqeembOkdpBAi87h58ybLli0jIiKCrl27mjscYQWePoUjR55d/O/apboBPM/WVnUH8PdXCYDatSGtkuC2tlC/ftpsSwiL0bgxNG8Oa9dC//5q8AydjqFDYd481aImMFC1sBFCCCGSIskAIYTJ8uXLR548efjhhx9wd3c3dzgiA6T2l/UnT+DAgWe/+u/Zk3B0fwcHeO21Zxf/r70Gzs7p+z6EyHSmTlVJgKAg+P13aNmSvHnVWBtjxqhuAy1bZu2WMEIIIZInyQAhhMmkV1HWEhiofnS8cuXZOi8vNahxfBfFqCh1wb9zp1r++UcN8Pc8V1f1a398s/8qVVTfZiHEKyhWDD79VM2x+cknEBAAjo4MGgTffgsnT8Lixc9mIxRCCCFeJMkAIYQQCQQGqtH6X8z/XL2qmh6//TZcuwYhIYbpzg08PJ796u/vD76+8uukEOli6FBYtEgNKjZ5MgwfjqsrDB6sJhsYORI6dAA7O3MHKoQQwhJJMsCM5FdWkSE0TU3EHhOjaoTOzhk2qlRafMb1MXqOfbeTR+fDcSqWH9/e/tjaWc+VpTXFr9ermcpu3YJevRKfnix+3R9/PFtXpMizC/+6ddU0ZzJwmRAZIEcOlQTo0AHGjYPOnaFQIfr0UbNxhIWpGTh69zZ3oEIIkLq/SFtp8XmSZIAZxE8R9+jRIxwdHc0cjcjU7t6Fy5eN223b2YG3t5q8PZ09evQIePaZT619nwdSaGp/KuqftVO/9qkXlwbO4LWJlj83nDni1zR4/Bju3FF//jt3jO8nd3vvXurmJx82TE0H+LIj/Qsh0kC7dvDdd2qgjk8/hd9+I0cO9f/Zp48aP6BLF3ByMnegQmRdtv81j4uJiZG6v0gzr1rPBkkGmIWtrS1ubm7cuHEDUPNe6uRnNJHWIiNVIuBFMTFw/rxKCLi6psuuNU3j0aNH3LhxAzc3N8NJMDX2fR5I9UltAOOrU0/9VTwntWEfKy06IfCq8T99qi7QU7qIT2zdi332U8veHqKj1X0b9Pizk/yEE05+duJPHOrvWbasJAKEMDudDmbOVPMJrlih5h5v0IAePVSjgbAwmD0bPvvM3IEKkXVly5YNJycnbt68Sfbs2bGxsTF3SMKKpUU9O55Ok/Yq6Sa5+R01TSMiIoJ79+6ZJziRuWma6tyt1yddxtZWde5OxxjcbG3xtLNLdbJLH6PndoUG5ImLILHTZRw6btp48mDtVotscq+P0ePcvAF5k4n/ms6LWQNDuRtlm+iFflTUq8Vgawu5cqkGILlyGd9P6jb+/p490KABvEMgM+iPN89aNlzGi/7MYDWt2LZNpu2zNqbOOyxMZzHHtE8fddVfvjwEB0O2bCxapFoF5MoFFy6kW/5XCGGCmJgYQkNDiXtxoB0hXpKbmxuenp6J1rNNPTdJMiAdmfJH0Ov1xMbGZnBkItO6exfOnVNTTf38s/ni0DSy37qF7X/Nl0TiHmPPQ5x5ggOPcTRa4tc9zeZInL0jOkcHcHTEJocjts6OZHdxJLuLA/Zujji6O+KYy5EcuR3IkceRnPkcccrtiM7JUc3j5+ioFnt7kzrz6/XwkUcg399WLRueT2jEoV7/Ue6VzLneSgYGtDIWc+GaiVjMMb1zB0qWhNu3VUuBvn3R69UAnidPwogRMHq0+cITQkBcXBwxr9p8TwhU14DkWgRIMsACWEwFQWQ+9+/Dv//C8ePPlhMn1GTwqeHoCK/QzygpGmqEeU377zYO4rQX1iXzzZOdWJx4nOJ+HuFILGkf/6syNX6zeD45EL+8uM7Ojqdr12Mb/YjEUgdxQIybBw6H9kCePJAzp+WOGKjXqzkPw8Mhf341ymEWzmDIeSntWdQx/f57+OgjcHODM2cgb15WroS2bdXYsRcuQN685g1RCCFE+pNkgAWwqAqCsE5PnsCpU+pC//kL/7CwpF/j4wOenrBvX8rbT2U77ydP1DXVtWtquXo18fsPHpi8SfLlgwIF1FKwoLrN9+92eq9okOJrQ6Zto+IA0+PPKCHTt1Pxk5Tj//fLxZTt6KcO7OPHxsuL61J6nFyZ9G6SaGubdJ+DlG7t7dMvrsBA6N8frjzr5oCXF8yYAa0sd7yJ9CTnpbRnUcdUr4eqVdWcnz16wA8/EBcH1arB4cMwcCBMmWLeEIUQQqQ/SQZYAIuqIAjL9vSpat7//AX/8eNw9mzSF3Kenqpv6PNL2bLqV1q9nkcePjjcvooNCf/F49DxJLcXTtdDwdYWvR6uX0/5Iv/2bdPfkovLs4v75y/0n7/v6Zn4/Nf6GD3XnXzw1Ccdf7itF56PQi12zACLiV/TIDY2dQmEXbvgl19S3na2bOqz+yqcnJIfwCCp51xcILkBmAIDoU2bhE1Q4lswrFyZJRMCcl5KexZ3THftUi1gdDo4cACqVGHjRmjSROXezp2TgT+FECKzM/XcJLMJWAMrb+ZqTfOsJyZN44+Lg4sXjZv2Hz+uOnQm1YfM3T3hRX+5cpA7d9IxY0t/ZvA9bYhDZ3RBGt/nu/v96ZytYUt4OEREmP7jsb190hf38ffz51dNUl+WrZ0tlwbOwHNS0vFfHjidghb6ObKo+HU6lXGxszN99LCSJU1LBgQFQY0aKc9fmNi6+HkMHz1Sy/O/3pvCxkY1hU4sYeDmpqZaSyzXrWnqmAwYAC1aWNV3qRAmqVMH/vc/WLwY+vaF3bsJCNDh76+qEl9/DXPnmjtIIYQQlkBaBqSjNPm1wMqbucbPs17g+XnWba1vnvhUx69p6gr7xV/6T5yAhw8Tf02OHOoi//kL/vLlic2Tn7v3dKm61oqIUNdaiY0GfwlvBjCd1RjHb2OjfqlP6ULf3T3juocndvyv2npzeeB0q/38WEX8er3qbnL1auIX1Dqd+h4KDX35i+m4ODX9ZWqTCHfvquRBWti6VU2bkIVY3K/YmYBFHtOrV6FUKXW++flneP99du6EunVVg56TJ6F4cXMHKYQQIr1INwEL8MoVBCtv5vr8POuJjUa+/zPrmSc+2fjv3El40X/8uLpoSURcdjseFirDnfzluJ6nPJdcynPOoTwX9IW5fdcmwbVPavrfJya5eeIHDYIOHdSFvoeHZf5IKi1LzCT++weMv4Ms4fsnOjr5hME//8CmTSlvx8UFGjVSV0j+/uDnZ5n/BGnIIi9crZzFHtMJE2DIEJXlPXMGcuakSRPYuFE1HPj1V3MHKIQQIr1IMsACvFIFIf6XuaSazqbFL3Pp6Fmf6StJzrNuHX2+E49fA6KxR3Nzx/FeROLbwIaL2UtwQlee4NjyHNXKc5zynKM4+pfooePqanqX6nPn1NhRKZF54kWyEmuZ5O0N06dbdCKS7dtf7hd/FxeoVetZcqBatfQd4NAMLPbC1YpZ7DGNjlatzM6dg88+g4kTOXRIjS+o08HRo+ppIYQQmY8kAyzAK1UQTK3MursnPgKbmcU+jCH7g8R/GX/eHdx5amN58WeLiyEXKccfLxQfjlPesJygHKcoTTQORuUcHEwbZD2xLtCpyflkRCtvkUVY45glpvwDFCyo+lTv3q3e365dasrO59nbqzER/P1VgqBmTTVApxWz2AvXNLJjxw4mTZrEoUOHCA8PZ/Xq1bRs2TLJ8oGBgcyZM4eQkBCio6MpV64co0aNIiAgwOR9WvQxXb8e3n5bTSF77BiUKkXbtqphT8uWsHq1uQMUQgiRHiQZYAFeqYKwdCm8+276BCbSzFS3r9hZZQCO+XKaNIuao2PGxWbJrbyFSHep/QfQ69VPpTt2qOTAzp1w44bxNm1toVKlZ8mBOnUgT570fR9pzKIvXNPAn3/+ye7du6lSpQqtWrVKMRkwYMAAChQoQIMGDXBzc2PBggVMnjyZf/75h0qVKpm0T4s/pm+/rZICjRvDhg2cPKWjfHk1bMc//0D16uYOUAghRFqTZIAFyJCWAT/9pJqyWphTvxyg9KTuKZbb2eUnPN62vPiv/3EA/4Upx2+p89zHs9ZW3kKkiVf5B9A01c96585nCYKwsITlypR51q2gbl21fQtm8ReuaUin06WYDEhMuXLlaN++PSNGjEj0+ejoaKKjow2Po6Ki8Pb2ttxjevas6g8QEwNr10KzZnTtCgsXqiEzgoLMHaAQQoi0JskAC5AmYwZYaTtvi5pn/SVYe/zPs8ZW3kKkmbT8B7h8+VmrgR074N9/E5YpXPhZcsDfX43onlHTb5hAkgHJi4uLw8fHh88//5w+ffokWmbUqFGMHj06wXqLPqZDh8L48VC0KJw4QViEAyVLQmwsbNkCr79u7gCFEEKkJVPP94mNjSYsga2tmj4QElYk4x9Pn26xV3Xx86zDs9H34z0/z7qlXkhbe/zPs7VVgwR27KhuLfQjI0T6SMt/AG9v1X1rzhw1TejNm6rT9cCBalQ2W1u4eBF++QV69lStBjw8oHVr9X1++LBKTqSGXq9aii1dqm5T+3qRKpMnT+bBgwe0a9cuyTJDhw4lMjLSsFy+fDkDI3xJX36ppo25cAGmTsXHR31E45+Sn4WEECJrkpYB6ShNfoGx8nbev3cOpPLPxvPcW8U86/+x2nnihRAZ7/592LfvWbeCffvUiO7Py5kTatd+1q0guRkLEvv+9/JSiYWX/P6XlgFJW7JkCT169OD333+nUaNGJu/Hao7pkiVqTkEnJzh9mnBbL4oVg8ePYd06NbSAEEKIzEG6CViANKsgWHE77++/h94f6fmg5E46v2ll86z/x2rniRdCmFd0NBw8+Cw5sHs3REUZl3l+xgJ/fzW1Yc6czwZAfPEU/YojgFrNhWsaSE0yYNmyZXTr1o0VK1bQtGnTVO3Hao6ppqnP2O7dqqXMkiUMGQITJkCFChAcDDbSXlQIITIFq+omMHv2bHx8fHBwcKBGjRrs378/2fIrVqygdOnSODg44Ovry4YNGwzPxcbGMnjwYHx9fcmRIwcFChSgU6dOXLt2zWgbPj4+6HQ6o2X8+PFGZY4ePYq/vz8ODg54e3szceLEtHvTqWHF7bz374c4bMnbtj61ZnWk4oD6VnchbWtnS8UB1hu/EMJM7O1VK4ChQ2HDBrhzR3UVmDFDdR3Il08lDHbsgG++UaO9u7lBlSrQuXPibbfj1w0YIF0G0sjSpUvp2rUrS5cuTXUiwKrodDBrlrpduhR27ODzz8HFRU2k8dtv5g5QCCFERjN7MmD58uUMHDiQkSNHcvjwYfz8/AgICODGi1M6/WfPnj107NiR7t27ExwcTMuWLWnZsiXHjx8H4NGjRxw+fJjhw4dz+PBhAgMDOX36NM2bN0+wra+++orw8HDD0rdvX8NzUVFRvPnmmxQuXJhDhw4xadIkRo0axQ8//JA+ByKTOnBA3VrghAdCCJGx4qcm7NdP/bIfEQGnTsG8edCpkxo0Ni5OJQwePEh6O5r2bDBDYeTBgweEhIQQEhICQGhoKCEhIVy6dAlQ/f07depkKL9kyRI6derElClTqFGjBhEREURERBAZGWmO8NNfpUrw4Yfqft++5HJ5yqefqofDh6sBBYUQQmQhmplVr15d+/jjjw2P9Xq9VqBAAW3cuHGJlm/Xrp3WtGlTo3U1atTQPvzwwyT3sX//fg3QLl68aFhXuHBhbdq0aUm+5rvvvtPc3d216Ohow7rBgwdrpUqVSuktGURGRmqAFhkZafJrMpMHDzTNxkbTQNOuXTN3NEIIYQUuXdK0jz9WX5wpLUuWpHrzmf28tG3bNg1IsHTu3FnTNE3r3LmzVq9ePUP5evXqJVveFFZ3TG/d0jR3d/UZmj1bi4rStDx51MN588wdnBBCiLRg6rnJrC0DYmJiOHTokNFAPTY2NjRq1Ii9e/cm+pq9e/cmGNgnICAgyfKgpvvR6XS4ubkZrR8/fjy5c+emUqVKTJo0iadPnxrtp27dutjZ2Rnt5/Tp09y9ezfR/URHRxMVFWW0ZGWHD6sfuQoWVEMdCCGESIG3txorwBTyxZpA/fr10TQtwbJw4UIAFi5cyPbt2w3lt2/fnmz5TCl3bvj6a3V/2DByxtzmiy/Uw6++gidPzBeaEEKIjGXWZMCtW7fQ6/V4eHgYrffw8CAiIiLR10RERKSq/JMnTxg8eDAdO3Y0GjyhX79+LFu2jG3btvHhhx8yduxYPv/88xT3E/9cYsaNG4erq6th8fb2TuKdZw3SRUAIIV6Cv7+aNeDFaWXj6XQqaeDvn7FxicyjZ081auDduzB8OL16qY/c5ctq4F8hhBBZg9nHDEhPsbGxtGvXDk3TmDNnjtFzAwcOpH79+lSoUIGPPvqIKVOmMGvWLKJfnAYqFaxy7uF0FJ8MqF7dvHEIIYRVsbVVgwxCwoRA/OPp061qMFlhYbJlg5kz1f3vv8fhVAgjRqiH33yT/JAVQgghMg+zJgPy5MmDra0t169fN1p//fp1PD09E32Np6enSeXjEwEXL14kKCgoxel+atSowdOnTwkLC0t2P/HPJcbe3h4XFxejJSuLnxRCWgYIIUQqtWqlBhksWNB4vZfXS08rKISRevWgQwfVn69vX7p01ihWDG7efJaLEkIIkbmZNRlgZ2dHlSpV2LJli2FdXFwcW7ZsoWbNmom+pmbNmkblAYKCgozKxycCzp49y+bNm8mdO3eKsYSEhGBjY0O+fPkM+9mxYwexzw2tGxQURKlSpXB3d0/V+8yKbt+GCxfU/apVzRuLEEJYpVatICwMtm2DJUvUbWioJAJE2pk0CZycYNcusq9cyldfPVudxPBIQgghMhGzdxMYOHAg8+bNY9GiRZw8eZJevXrx8OFDunbtCkCnTp0YOnSooXz//v3ZuHEjU6ZM4dSpU4waNYqDBw/Sp08fQCUC2rRpw8GDB1m8eDF6vd4wVVBMTAygBgecPn06R44c4cKFCyxevJhPPvmE9957z3Ch/+6772JnZ0f37t05ceIEy5cvZ8aMGQwcODCDj5B1OnhQ3ZYsqabNFkII8RJsbaF+fejYUd1K1wCRlry84Msv1f3PPqPD2w/w9YXISJUQEEIIkbmZPRnQvn17Jk+ezIgRI6hYsSIhISFs3LjRMFjfpUuXCA8PN5SvVasWS5Ys4YcffsDPz4+VK1eyZs0aypcvD8DVq1dZu3YtV65coWLFiuTPn9+w7NmzB1DN+ZctW0a9evUoV64c33zzDZ988gk//PCDYT+urq5s2rSJ0NBQqlSpwqBBgxgxYgQ9e/bMwKNjvaSLgBBCCGEFBg6EokXh2jVsxo81TDQwYwYkMV6yEEKITEKnaZpm7iAyq6ioKFxdXYmMjMxy4wc0bw7r1qkxrvr3N3c0QgghIGufl9JLpjima9dCixZgZ4d2/AQ13y/OP/9A377PxhkUQghhPUw9N5m9ZYDIfDTtWcsAmUlACCGEsHDNmkHjxhATg27gJ3zzjVo9dy5cvGje0IQQQqQfSQaINHflCly/rmYuqljR3NEIIYQQIlk6nWrKlz07/PEHDaM38PrrEBsLo0ebOzghhBDpRZIBIs0dOKBuy5cHR0fzxiKEEEIIE5QqBQMGqPsDBjB2ZDQAixbB6dPmC0sIIUT6kWSASHPxyQDpIiCEEEJYkWHDwNMTzp6lxr4ZNG8OcXEwYoS5AxNCCJEeJBkg0pzMJCCEEEJYIRcXmDBB3R8zhnF9r6HTwW+/QXCweUMTQgiR9iQZINJUXBwcPKjuSzJACCGEsDLvvQc1a8KDB5RdNJgOHdTqYcPMG5YQQoi0J8kAkabOnoWoKDVWQLly5o5GCCGEEKliYwOzZqlBBX/9lQnNd2NrCxs2wO7d5g5OCCFEWpJkgEhT8V0EKldWswkIIYQQwspUqQIffACA98S+dO+iB+CLL9T0wUIIITIHSQaINBU/eKB0ERBCCCGs2DffgJsbBAczrvhP2NvDjh0QFGTuwIQQQqQVSQaINCUzCQghhBCZQN688NVXAOSa/AWfdrsDSOsAIYTITCQZINJMTMyz0YalZYAQQghh5Xr1UgMA3b7NF9EjyZEDDh2C1avNHZgQQoi0IMkAkWaOH4foaHB3h2LFzB2NEEIIIV5JtmxqMEHAaeF3jH/3KKBmFtDrzRmYEEKItCDJAJFmnh8vQKczbyxCCCGESAMNGkDbthAXx0f/9sPdTePkSVi82NyBCSGEeFWSDBBpJn4mAekiIIQQQmQikyeDoyPZdv/NgrdWADBqlOoeKIQQwnpJMkCkGZlJQAghhMiEChWCoUMBaPb3IIrke0hoKPz0k5njEkII8UokGSDSxMOHcOKEui8zCQghhBCZzKefgo8PNlevsLzSeADGjIFHj8wclxBCiJcmyQCRJg4fhrg4KFgQ8uc3dzRCCCGESFOOjjB1KgBVt0+iToELhIfD7NlmjksIIcRLk2SASBPSRUAIIYTI5Fq2hDfeQBcdzWKPgQCMHw+RkeYNSwghxMuRZIBIE/HJAOkiIIQQQmRSOh3MmAHZslEo+He6e/3FnTswbZq5AxNCCPEyJBkg0oTMJCCEEEJkAWXKQL9+AEyN6092YpgyBW7dMnNcQgghUk2SAeKV3b4NFy6o+1WrmjcWIYQQQqSzESMgXz5crp1mfIFZPHigugsIIYSwLpIMEK/s4EF1W7IkuLmZNRQhhBBCpDdXV8PVf7+7o/EknFmzYMUKWLoUtm8Hvd68IQohhEiZJAPEK5MuAkIIIUQW07kzVK9Otsf3+c5lKDEx0K4dvPsuNGgAPj4QGGjuIIUQQiRHkgHilclMAkIIIUQWY2MDs2YB8E7UImqwz+jpq1ehTRtJCAghhCWTZIB4JZr2rGWAzCQghBBCZB36KtVZ5tQNgFn0oT5b6cBS6rEdnab6CQwYIF0GhBDCUkkyQLySK1fg+nXIlg0qVjR3NEIIIYTIKDt3Qv9HY3mII9U4xDYaspR32U4DwvChpRbI5cuqnBBCCMsjyQDxSuK7CJQvD46O5o1FCCGEEBknPBxqsxsnHid4riBXWUkb3iGQ8HAzBCeEECJFkgwQryQ+GSBdBIQQQoisJX8+PTPoj5bIczb/rZ3OAPLnk34CQghhiSQZIF6JzCQghBBCZE3+7MSbK0lWJm3QKMRl/JF+AkIIYYkkGSBeWlwcHDyo7ksyQAghhMhabG+Y1v7f1HJCCCEylkUkA2bPno2Pjw8ODg7UqFGD/fE/NydhxYoVlC5dGgcHB3x9fdmwYYPhudjYWAYPHoyvry85cuSgQIECdOrUiWvXrhnKhIWF0b17d4oUKYKjoyPFihVj5MiRxMTEGJXR6XQJln37jKfOycrOnoWoKDVWQLly5o5GCCGEEBkqf37Tyt26lb5xCCGEeClmTwYsX76cgQMHMnLkSA4fPoyfnx8BAQHcuHEj0fJ79uyhY8eOdO/eneDgYFq2bEnLli05fvw4AI8ePeLw4cMMHz6cw4cPExgYyOnTp2nevLlhG6dOnSIuLo7vv/+eEydOMG3aNObOncsXX3yRYH+bN28mPDzcsFSpUiV9DoQVis/ZVK6sZhMQQgghRBbi7w9eXqDTJfq0YSyBfv2gWze4eTPDQhNCCJEynaZpiY37kmFq1KhBtWrV+PbbbwGIi4vD29ubvn37MmTIkATl27dvz8OHD/njjz8M61577TUqVqzI3LlzE93HgQMHqF69OhcvXqRQoUKJlpk0aRJz5szhwoULgGoZUKRIEYKDg6n4knPmRUVF4erqSmRkJC4uLi+1DUvWrx/MmqXmEJ42zdzRCCGESElmPy+ZQ5Y/poGB0KaNuv98lVKnQ9NgM6/zBlvUOnd3GD8ePvgAbMz+e5QQQmRapp6bzPpNHBMTw6FDh2jUqJFhnY2NDY0aNWLv3r2Jvmbv3r1G5QECAgKSLA8QGRmJTqfDzc0t2TK5cuVKsL558+bky5ePOnXqsHbt2mTfT3R0NFFRUUZLZiYzCQghhBBZXKtWsHIlFCxovN7Li5glK+lXejM12cNFNz+4exc+/BBq1oTDh80TrxBCCAOzJgNu3bqFXq/Hw8PDaL2HhwcRERGJviYiIiJV5Z88ecLgwYPp2LFjklmRc+fOMWvWLD788EPDOmdnZ6ZMmcKKFStYv349derUoWXLlskmBMaNG4erq6th8fb2TrKstYuJgeBgdV8GDxRCCCGysFatICwMtm2DJUvUbWgo9h1bsXAh7LepSbF7BwnpOgNy5lT9DKtVg7594d49MwcvhBBZV6ZuoxUbG0u7du3QNI05c+YkWubq1as0btyYtm3b0qNHD8P6PHnyMHDgQEM3hvHjx/Pee+8xadKkJPc3dOhQIiMjDcvly5fT/D1ZiuPHITpatfgrVszc0QghhBDCrGxtoX596NhR3draAlCjBgwZAnqy8ca6ftzafVqViYuDb7+F0qXh11+NuxgIIYTIEGZNBuTJkwdbW1uuX79utP769et4enom+hpPT0+TyscnAi5evEhQUFCirQKuXbtGgwYNqFWrFj/88EOK8daoUYNz584l+by9vT0uLi5GS2YV30WgWrUkxw0SQgghhGDECPD1VZMKfDgqP9riJbBlC5QqBdevw/vvQ4MG8O+/5g5VCCGyFLMmA+zs7KhSpQpbtmwxrIuLi2PLli3UrFkz0dfUrFnTqDxAUFCQUfn4RMDZs2fZvHkzuXPnTrCdq1evUr9+fapUqcKCBQuwMWEgm5CQEPKbOo1OJhc/k4B0ERBCCCFEcuzt4eef1cxDgYGqJwGvvw5Hj8LYsWqO4r//Bj8/GDwYHjwwd8hCCJElmL2bwMCBA5k3bx6LFi3i5MmT9OrVi4cPH9K1a1cAOnXqxNChQw3l+/fvz8aNG5kyZQqnTp1i1KhRHDx4kD59+gAqEdCmTRsOHjzI4sWL0ev1REREEBERQUxMDPAsEVCoUCEmT57MzZs3DWXiLVq0iKVLl3Lq1ClOnTrF2LFjmT9/Pn379s3Ao2O5nm8ZIIQQQgiRnIoVVQsBgD594No1wM4Ohg5VLQJatICnT2HiRChbVmUNpOuAEEKkK7PPDt++fXtu3rzJiBEjiIiIoGLFimzcuNEwSOClS5eMfrWvVasWS5YsYdiwYXzxxReUKFGCNWvWUL58eUBd6McP8vfilIDbtm2jfv36BAUFce7cOc6dO4eXl5dRmednWhwzZgwXL14kW7ZslC5dmuXLl9MmfvqcLOzhQzhxQt2XmQSEEEIIYYohQ2DtWjh4UM0uuH79f10NfXxgzRpYt07NWxwWBq1bQ5Mmag5jGZxICCHShU7TJO2aXjLr3MM7d0LdumoWoStXzB2NEEIIU2XW85I5yTFNnX//hcqV1SDEP/4I3bu/UODRI9V1YOJEiI1VfQyGDlXdBxwczBKzEEJYG1PPTWbvJiCsj3QREEIIIcTLKFsWvv5a3f/kE7h48YUCTk6qwLFj0KiRyhqMGgXly8PGjRkdrhBCZGqSDBCpFp8MkC4CQgghhEitTz6B2rXh/n3o1k3NMphAqVKwaRMsWwb588P586rbQJs2kImnbhZCiIwkyQCRajKTgBBCCEu0Y8cOmjVrRoECBdDpdKxZsybZ8uHh4bz77ruULFkSGxsbBgwYkCFxZnW2trBwoWoEsHUrfPddEgV1OmjfHk6dUhkEW1tYtQrKlIHJk1U3AiGEEC9NkgEiVW7fhgsX1P2qVc0bixBCCPG8hw8f4ufnx+zZs00qHx0dTd68eRk2bBh+fn7pHJ14XvHiMGGCuj94MJw9m0xhFxeYOhUOH4ZatdRIxp99BpUqwY4dGRKvEEJkRqlOBvj4+PDVV19x6dKl9IhHWLiDB9VtiRLg5mbWUIQQQggjTZo04euvv+add94xqbyPjw8zZsygU6dOuLq6pnN04kW9e8Prr6sxA7t0Ab0+hRdUqKBGMZ4/H/LkUVMb1asHnTvD9esZEbIQQmQqqU4GDBgwgMDAQIoWLcobb7zBsmXLiI6OTo/YhAWK7yIg4wUIIYTIiqKjo4mKijJaxMuxsVHX9Tlzwp49MG2aiS/q2hVOn4YPP1RdCX7+WY0x8N13JmQUhBBCxHupZEBISAj79++nTJky9O3bl/z589OnTx8OHz6cHjEKCyIzCQghhMjKxo0bh6urq2Hx9vY2d0hWrXDhZ0mAYcPU1IMmyZUL5s6FvXtVd4HISPj4Y6hR41llRQghRLJeesyAypUrM3PmTK5du8bIkSP58ccfqVatGhUrVmT+/PlompaWcQoLoGkyeKAQQoisbejQoURGRhqWyzKy/Svr1g3eekvNIti5cyrHBYy/+P/2W3B1hUOH1LpeveDu3XSLWQghMoOXTgbExsby22+/0bx5cwYNGkTVqlX58ccfad26NV988QX/+9//0jJOYQGuXFFd8mxtVRJeCCGEyGrs7e1xcXExWsSr0elg3jxwd1djE40fn8oN2NqqVgGnTsF776lfL+bOVV0HFi5Uj4UQQiSQ6mTA4cOHjboGlCtXjuPHj7Nr1y66du3K8OHD2bx5M6tXr06PeIUZxbe68/UFR0fzxiKEEEKIzKNAAZg1S93/6isICXmJjXh6wi+/wLZtULYs3LypxheoWxeOHUvLcIUQIlNIdTKgWrVqnD17ljlz5nD16lUmT55M6dKljcoUKVKEDh06pFmQwjLIeAFCCCEs2YMHDwgJCSHkvyvJ0NBQQkJCDDMgDR06lE6dOhm9Jr78gwcPuHnzJiEhIfxrcsd1kZbefRdatYKnT1V3gZcen7p+fQgOVnMXOjnBrl2qSeOgQXD/flqGLIQQVk2npbJz/8WLFylcuHB6xZOpREVF4erqSmRkZKZoRtiwIWzdqpryffCBuaMRQgiRWpntvPSi7du306BBgwTrO3fuzMKFC+nSpQthYWFs377d8JxOp0tQvnDhwoSFhZm0z8x+TDPajRtQrhzcugVffAHffPOKG7x0CT75BAID1eMCBdSIhW3bqv4JQgiRCZl6bkp1MuDAgQPExcVRo0YNo/X//PMPtra2VK1a9eUizoQyUwUhLk715YuKUk33/PzMHZEQQojUykznJUshxzTtBQZC69ZqFsE9e9R4gK/szz+hTx+4cEE9fuMNNehgyZLqsV4PO3dCeDjkzw/+/mosAiGEsEKmnptS3U3g448/TnTk3KtXr/Lxxx+ndnPCSpw9qxIBjo4qYy+EEEIIkR5atVJdBuLiVHeBx4/TYKNNmsDx4zByJNjbQ1CQGgRp+HBYuhR8fKBBA7XjBg3U4/jWBEIIkUmlOhnw77//Urly5QTrK1WqJH3sMrH4KQUrV4Zs2cwbixBCCCEyt1mz1A/0p0/DsGFptFFHRxg1SiUFAgIgJga+/lolAK5cMS579Sq0aSMJASFEppbqZIC9vT3Xr19PsD48PJxscpWYacnggUIIIYTIKLlyqTGKQHXx37kzDTdevLjqNrB8ueqLkJj4XrQDBqguBEIIkQmlOhnw5ptvMnToUCIjIw3r7t27xxdffMEbb7yRpsEJyyHJACGEEEJkpKZNoVs3dV3epQs8eJCGG9fpIF8+1RchKZoGly/DDz/Aw4dpuHMhhLAMqU4GTJ48mcuXL1O4cGEaNGhAgwYNKFKkCBEREUyZMiU9YhRmFhOjZugBqF7dvLEIIYQQIuuYNg0KFVLj/n3+eRpvPDzctHK9e0POnFCihBrQYORIWLUKzpyRVgNCCKuW6nb9BQsW5OjRoyxevJgjR47g6OhI165d6dixI9mzZ0+PGIWZHT+u5vp1d4dixcwdjRBCCCGyChcXmD8fGjWCOXPgnXfURABpIn9+08q5ucG9e3DunFpWr372XPzIyr6+z5YKFVSrAyGEsHAv1ck/R44c9OzZM61jERYqvotA1aoyJa8QQgghMlbDhvDxxzB7tuo2cPw4uLqmwYb9/cHLSw0WmNhM2zqdej40FG7fhmPH4OhRdXvsGJw4oaY6OHhQLc/Ll884OeDrC2XLgpNTGgQuhBBp46VH/Pv333+5dOkSMTExRuubN2/+ykEJyxKfDJAuAkIIIYQwhwkTYONGOH8ePvlEtRZ4Zba2MGOGmjVApzNOCMT/+jF9uiqXL5/KSjRs+KyMXq8Cik8OxCcLzp+HGzdgyxa1PL/NEiUStiIoWjTpgQxNpderURbDw1WLB39/FbcQQiRDp2mJpUKTduHCBd555x2OHTuGTqcj/uW6/7409dJ3yiAqKgpXV1ciIyNxcXExdzgvrUIFdX5bswZatDB3NEIIIV6WpZ6XLl++jE6nw8vLC4D9+/ezZMkSypYta/EtES31mGZGu3ZB3brqmn3dOnj77TTacGAg9O9vPL2gt7dKBLRqlfrtPXwI//5r3Irg6FG4dSvx8k5OqqtBfAuC+CVv3peP38tLJTpeJn4hhNUz9dyU6mRAs2bNsLW15ccff6RIkSLs37+f27dvM2jQICZPnoy/v/8rB59ZZIYKwsOHqr9eXJxqRVeggLkjEkII8bIs9bzk7+9Pz549ef/994mIiKBUqVKUK1eOs2fP0rdvX0aMGGHuEJNkqcc0s/r0U5gyBTw9VXeB3LnTaMPp/cu6psH16wlbEfz7Lzx5kvhrPD0T72rg4PCsTGCgatnwYnU+vmXDypWSEBAiC0q3ZECePHnYunUrFSpUwNXVlf3791OqVCm2bt3KoEGDCI4fdl5kigrCrl3qfFiwoHHCWQghhPWx1POSu7s7+/bto1SpUsycOZPly5eze/duNm3axEcffcSFCxfMHWKSLPWYZlZPnkDlynDyJHToAEuXmjuiV6TXq0EJXxyP4MKFxMcxsLFRXQ0qVFCtCWbNUuMZJOb5MQ+ky4AQWYqp56ZUjxmg1+vJmTMnoBID165do1SpUhQuXJjTp0+/fMTCIu3fr26rVTNvHEIIITKv2NhY7O3tAdi8ebNh/KHSpUsTbur0byJLcHCARYugZk1Ytkz96N22rbmjegW2tlCqlFratHm2/sEDNUDhiy0Jbt+G06fVsmJF8tvWNLh8WTWlqF8fcuVSi6ur5SUHrH3MA2uPX5iXGT8/qU4GlC9fniNHjlCkSBFq1KjBxIkTsbOz44cffqBo0aLpEaMwo/jBAyUZIIQQIr2UK1eOuXPn0rRpU4KCghgzZgwA165dI3eatQMXmUW1ajB0KHz9NfTqpcYR8PAwd1RpzNkZatRQSzxNg4iIZ8mB339XFxApGTzY+LFOp6ZLjE8O5Mql5o9+/nFii7s72Nml6dsErH/MA2uPH6w/mWHN8Zv585PqbgJ//fUXDx8+pFWrVpw7d463336bM2fOkDt3bpYvX87rr7+eXrFanczQdLB4cTUoblCQmuNXCCGE9bLU89L27dt55513iIqKonPnzsz/b6j4L774glOnThEYGGjmCJNmqcc0s4uJUbMcHTkCLVuq+nSWm/54+3Zo0CDlcsWKQWws3LmjWhy8ihw5Uk4YJLbeySnxP5C1j3lg7fGD2S9GX5k1x5+On590GzMgMXfu3MHd3d0wo4BQrL2CcPs25Mmj7t+9q5LIQgghrJcln5f0ej1RUVG4u7sb1oWFheHk5ES+fPnMGFnyLPmYZnZHjqhWArGx8Msv8N575o4og+n14OOjRnhOrDqf2JgBMTGqUnfnzrPlxceJLffuJb4PU9nZJUwYuLnB6tXJJyjy5VPdIezt1XuwtYVs2Z7dT81jG5u0zRjFH/+kBtWyhjEbrD2ZYU3x6/Xq/y86Wt0+eqT6O0VEJF7+FT8/6ZIMiI2NxdHRkZCQEMqXL5/qoLIaa68g/PUXNG6sxqk5c8bc0QghhHhVlnpeevz4MZqm4eTkBMDFixdZvXo1ZcqUISAgwMzRJc9Sj2lW8c03MGyY6gZ/4oQa8DhLib8YAuMLorS+GNLrITLStMTBi8vTp6++/7TyfJIgpURCSkmG+/chJCTlfTZtCoUKqdcntsRv29QlNeWTK6vTqUq+NSYzNE19JosUST5+T0/YulWVjb8Ij442vp/Wt0k9p9e/3Hvdtk2N+ZFK6TKAYPbs2SlUqBD6l30zwqrEjxdQvbp54xBCCJG5tWjRglatWvHRRx9x7949atSoQfbs2bl16xZTp06lV69e5g5RWKjBg1XX+QMH4IMPYMOGLNZdoFUrdcGfWDPp6dPT7ldRW9tnv+wXK2b66zRNzVOdWJJg61ZYvjzlbeTLB46O6mIqfnn6NPHHcXHJbyu+fEZavz5j95dW4gegdHZWrSqeX2+O+6mlaWoMgTJlXn4b6cnW1rTPYjoPopvqAQS//PJLvvjiC3755Rdy5cqVJkHMnj2bSZMmERERgZ+fH7NmzaJ6MlegK1asYPjw4YSFhVGiRAkmTJjAW2+9BajWC8OGDWPDhg1cuHABV1dXGjVqxPjx4ylQoIBhG3fu3KFv376sW7cOGxsbWrduzYwZM3B2djaUOXr0KB9//DEHDhwgb9689O3bl88//zxN3rM1kJkEhBBCZITDhw8zbdo0AFauXImHhwfBwcGsWrWKESNGSDJAJClbNjW7QKVKsHEj/Pgj9Ohh7qgyWKtW0KKFZQ6gptOpi0lnZ/Xr+PNKljQtGbB8uem/jGqaSggklSwwJaFg6uOjR+G/wU6T1a2beu9Pnya/xO/D1CU15V8lCfLkycu9zlI4OqrPn52d6m6S2tuXeU1Kt9mzw99/mzbmR/786Xp4Uj1mQKVKlTh37hyxsbEULlyYHDlyGD1/+PDhVAWwfPlyOnXqxNy5c6lRowbTp09nxYoVnD59OtE+gnv27KFu3bqMGzeOt99+myVLljBhwgQOHz5M+fLliYyMpE2bNvTo0QM/Pz/u3r1L//790ev1HDx40LCdJk2aEB4ezvfff09sbCxdu3alWrVqLFmyBFBNK0qWLEmjRo0YOnQox44do1u3bkyfPp2ePXua9N6suemgpkGBAqoby+7dUKuWuSMSQgjxqiz1vOTk5MSpU6coVKgQ7dq1o1y5cowcOZLLly9TqlQpHj16ZO4Qk2SpxzSrmToVBg1Sdf5jx1RXbmHhXmbMA0tibfHHN62PTxBs3w7NmqX8uiVL4LXXjJvcWML9PXvU6KEpeclm9ukunT8/6TaA4OjRo5N9fuTIkanZHDVq1KBatWp8++23AMTFxeHt7U3fvn0ZMmRIgvLt27fn4cOH/PHHH4Z1r732GhUrVmTu3LmJ7uPAgQNUr16dixcvUqhQIU6ePEnZsmU5cOAAVatWBWDjxo289dZbXLlyhQIFCjBnzhy+/PJLIiIisPtvGpUhQ4awZs0aTp06ZdJ7s+YKwpUr4O39rEuUo6O5IxJCCPGqLPW8VKFCBT744APeeecdypcvz8aNG6lZsyaHDh2iadOmRCQ1wJIFsNRjmtXo9aq+v2uXut2yxbhls7BQGTXmQXqx5vitLZnxImuPH9L182PyuUkzo+joaM3W1lZbvXq10fpOnTppzZs3T/Q13t7e2rRp04zWjRgxQqtQoUKS+wkKCtJ0Op0WGRmpaZqm/fTTT5qbm5tRmdjYWM3W1lYLDAzUNE3T3n//fa1FixZGZbZu3aoB2p07dxLdz5MnT7TIyEjDcvnyZQ0w7NearFqlaaBpFSuaOxIhhBBpJTIy0iLPSytWrNCyZ8+u2djYaI0aNTKsHzt2rNa4cWMzRpYySz2mWdG5c5rm5KTqLzNnmjsaYbJVqzTNy0v94eIXb2+13hpYc/yrVmmaTqeW5+OPX2fp78Ha49e0dPv8mHpuMmvO9NatW+j1ejw8PIzWe3h4JPkrQERERKrKP3nyhMGDB9OxY0dDViQiIiJBF4Rs2bKRK1cuw3aS2k/8c4kZN24crq6uhsXb2zvRctYgfvBAGS9ACCFEemvTpg2XLl3i4MGD/PXXX4b1DRs2NIwlIERKihWDSZPU/cGDZSYkq9GqFYSFqebcS5ao29BQy/1F/UXWHH/8AJQvTsPh5WXZrRriWXv8YPbPT6oHELSxsUGXzDCtljTTQGxsLO3atUPTNObMmZPu+xs6dCgDBw40PI6KirLahIDMJCCEECIjeXp64unpyZX/RkT38vJKdjBhIRLz0Ueq5e2WLdClixpTz1JbCIvn2NpaZr9uU1lz/JY8AKUprD1+MOvnJ9XJgNWrVxs9jo2NJTg4mEWLFqU4nsCL8uTJg62tLdevXzdaf/36dTw9PRN9jaenp0nl4xMBFy9eZOvWrUZ9JTw9Pblx44ZR+adPn3Lnzh3DdpLaT/xzibG3t8fe3j6pt2s14uKkZYAQQoiMExcXx9dff82UKVN48OABADlz5mTQoEF8+eWX2Ejnb2EiGxuYPx/Kl4e9e2HKFMhCE0EJ8XKsOZkB1h+/GaX67NqiRQujpU2bNnzzzTdMnDiRtWvXpmpbdnZ2VKlShS1bthjWxcXFsWXLFmrWrJnoa2rWrGlUHiAoKMiofHwi4OzZs2zevJncuXMn2Ma9e/c4dOiQYd3WrVuJi4ujRo0ahjI7duwgNjbWaD+lSpXC3d09Ve/T2pw9C1FRatDAcuXMHY0QQojM7ssvv+Tbb79l/PjxBAcHExwczNixY5k1axbDhw83d3jCyhQqBNOnq/vDh8OJE2YNRwghLNcrjUzwnPPnz2s5cuRI9euWLVum2dvbawsXLtT+/fdfrWfPnpqbm5sWERGhaZoayG/IkCGG8rt379ayZcumTZ48WTt58qQ2cuRILXv27NqxY8c0TdO0mJgYrXnz5pqXl5cWEhKihYeHG5bo6GjDdho3bqxVqlRJ++eff7Rdu3ZpJUqU0Dp27Gh4/t69e5qHh4f2/vvva8ePH9eWLVumOTk5ad9//73J781aBxX65Rc1dkXt2uaORAghRFqy1PNS/vz5td9//z3B+jVr1mgFChQwQ0Sms9RjmtXFxWla06aqPlO5sqbFxJg7IiGEyDimnptS3U0gMY8fP2bmzJkUfHHwBhO0b9+emzdvMmLECCIiIqhYsSIbN240DNZ36dIlo+aBtWrVYsmSJQwbNowvvviCEiVKsGbNGsqXLw/A1atXDS0UKlasaLSvbdu2Uf+/JiSLFy+mT58+NGzYEBsbG1q3bs3MmTMNZV1dXdm0aRMff/wxVapUIU+ePIwYMYKePXum+j1am/371a10ERBCCJER7ty5Q+nSpROsL126NHfu3DFDRMLa6XQwb55q4Xj4MIwbByNGmDsqIYSwLDpNS2xixqS5u7sbDSCoaRr379/HycmJX3/9lebNm6d5kNbKWucerlkT9u2DxYvh3XfNHY0QQoi0YqnnpRo1alCjRg2jpDxA37592b9/P//884+ZIkuZpR5ToSxdquoy2bLBP/9A5crmjkgIIdKfqeemVLcMmDZtmlEywMbGhrx581KjRo1M35c+K4iNheBgdV8GcRZCCJERJk6cSNOmTdm8ebNhDKC9e/dy+fJlNmzYYObohDXr0AFWrVJL585w8CBkgrGehRAiTaS6ZYAwnTX+WnD4MFSpAu7ucPu2amYnhBAic7Dk89K1a9eYPXs2p06dAqBMmTL07NmTr7/+mh9++MHM0SXNko+pUG7eVN0Fbt6EIUNUlwEhhMjMTD03pToZsGDBApydnWnbtq3R+hUrVvDo0SM6d+78chFnQtZYQfj+ezVH7xtvwKZN5o5GCCFEWrK289KRI0eoXLkyer3e3KEkydqOaVa1erWajtzGBnbvhtdeM3dEQgiRfkw9N6V6asFx48aRJ0+eBOvz5cvH2LFjU7s5YWEOHFC30kVACCGEEJnFO+/Ae+9BXJzqLvDokbkjEkII80t1MuDSpUsUKVIkwfrChQtz6dKlNAlKmI/MJCCEEEKIzGjmTChQAM6cgS+/NHc0QghhfqlOBuTLl4+jR48mWH/kyBFy586dJkEJ83j4EE6cUPclGSCEEEKIzMTdHX78Ud2fMQP+/tu88QghhLmlejaBjh070q9fP3LmzEndunUB+Pvvv+nfvz8dOnRI8wBFxgkOVs3nChZUmXMhhBAiPbVq1SrZ5+/du5cxgYgso0kT+OADlRTo2hWOHgVnZ3NHJYQQ5pHqZMCYMWMICwujYcOGZMumXh4XF0enTp1kzAArJ10EhBBCZCRXV9cUn+/UqVMGRSOyiilTICgIQkPhs89gzhxzRySEEOaR6mSAnZ0dy5cv5+uvvyYkJARHR0d8fX0pXLhwesQnMlD84IGSDBBCCJERFixYYO4QRBbk4gILFsDrr8PcuWpwwTffNHdUQgiR8VKdDIhXokQJSpQokZaxCDOTmQSEEEIIkRU0aAB9+8KsWdC9Oxw7Bm5u5o5KCCEyVqoHEGzdujUTJkxIsH7ixIm0bds2TYISGe/2bTh/Xt2vWtW8sQghhBBCpLdx46B4cbhyBfr1g+3bYelSdavXmzs6IYRIf6lOBuzYsYO33norwfomTZqwY8eONAlKZLyDB9VtiRKSGRdCCCFE5pcjByxcqO7/8otqLfDuu+rWxwcCA80ZnRBCpL9UJwMePHiAnZ1dgvXZs2cnKioqTYISGU+6CAghhBAiq7l+PfH1V69CmzaSEBBCZG6pTgb4+vqyfPnyBOuXLVtG2bJl0yQokfFkJgEhhBBCZCV6PfTvn/hzmqZuBwyQLgNCiMwr1QMIDh8+nFatWnH+/Hlef/11ALZs2cKSJUtYuXJlmgco0p+myUwCQgghhMhadu5U4wUkRdPg8mVVrn79DAtLCCEyTKpbBjRr1ow1a9Zw7tw5evfuzaBBg7h69Spbt26lePHi6RGjSGdXr0JEBNjaQqVK5o5GCCGEECL9hYebVu7UqfSNQwghzCXVyQCApk2bsnv3bh4+fMiFCxdo164dn376KX5+fmkdn8gA8V0EfH3B0dG8sQghhBBCZIT8+U0r168f9OkDFy+mbzxCCJHRXioZAGpWgc6dO1OgQAGmTJnC66+/zr59+9IyNpFBpIuAEEIIIbIaf3/w8gKdLukydnYQGwuzZ6tpCLt0gZMnMyxEIYRIV6lKBkRERDB+/HhKlChB27ZtcXFxITo6mjVr1jB+/HiqydWkVZKZBIQQQgiR1djawowZ6v6LCQGdTi1LlsC2bfDGG/D0KSxaBOXKqZkGDh3K+JiFECItmZwMaNasGaVKleLo0aNMnz6da9euMWvWrPSMTWSAuDhpGSCEEEKIrKlVK1i5EgoWNF7v5aXWt26tBg/ctAn++QdatlQDC65aBVWrQuPGsGOHOSIXQohXZ3Iy4M8//6R79+6MHj2apk2bYmtrm55xiQxy9ixERamxAsqVM3c0QgghhBAZq1UrCAtTLQDiWwKEhqr1z6teHVavhuPH4b33VMuCv/6CevVUl4M//3w2JaEQQlgDk5MBu3bt4v79+1SpUoUaNWrw7bffcuvWrfSMTWSA+FYBlStDtlRPNCmEEEIIYf1sbVULgI4d1W1yv3mVKwe//AJnzsBHH6lxBXbtgrfeUvWpFStAr8+oyIUQ4uWZnAx47bXXmDdvHuHh4Xz44YcsW7aMAgUKEBcXR1BQEPfv30/POEU6iZ9JQLoICCGEsHY7duygWbNmFChQAJ1Ox5o1a1J8zfbt26lcuTL29vYUL16chQsXpnucInMoWhTmzFGtCAYNghw5ICQE2rWDsmVhwQKIiTF3lEIIkbRUzyaQI0cOunXrxq5duzh27BiDBg1i/Pjx5MuXj+bNm6dHjCIdyXgBQgghMouHDx/i5+fH7NmzTSofGhpK06ZNadCgASEhIQwYMIAPPviAv/76K50jFZlJgQIwebKaenDkSHB3V60GunVTMxDMmgWPHpk7SiGESEinaa/eu0mv17Nu3Trmz5/P2rVr0yKuTCEqKgpXV1ciIyNxcXExdzgJxMZCzpwQHa3GDihe3NwRCSGESE+Wfl5KSzqdjtWrV9OyZcskywwePJj169dz/Phxw7oOHTpw7949Nm7caNJ+stIxFaa5fx++/x6mTIGICLUub1745BPo3RtcXc0bnxAi8zP13JTqlgGJsbW1pWXLlpIIsDLHjqlEgLs7FCtm7miEEEKIjLV3714aNWpktC4gIIC9e/cm+Zro6GiioqKMFiGelzMnfPqp6j4wZw74+MDNm/DFF1CoEHz5pXoshBDmlibJAGGd4rsIVK2acH5dIYQQIrOLiIjAw8PDaJ2HhwdRUVE8fvw40deMGzcOV1dXw+Lt7Z0RoQor5OCgBhg8c0YNOFi2rJrBaexYKFwYBgyAy5fNHaUQIiuTZEAWFp8MqF7dvHEIIYQQ1mLo0KFERkYalstyNSdSkD27morw2DEIDFQ/wjx+DDNmqJaZH3ygumsKIURGk2RAFiYzCQghhMjKPD09uX79utG669ev4+LigqOjY6Kvsbe3x8XFxWgRwhQ2NvDOO6r+tWmTmsIwNhZ++glKl4YOHeDIEXNHKYTISiQZkEU9fAgnTqj7kgwQQgiRFdWsWZMtW7YYrQsKCqJmzZpmikhkBTodvPEGbNsGu3fD229DXBwsXw4VK6rHyQxbIYQQaUaSAVlUcLA68RQooBYhhBDC2j148ICQkBBCQkIANXVgSEgIly5dAlQT/06dOhnKf/TRR1y4cIHPP/+cU6dO8d133/Hbb7/xySefmCN8kQXVqgXr1kFICLRvrxIF69er9Q0aQFAQvPq8X0IIkTizJwNmz56Nj48PDg4O1KhRg/3xbdeTsGLFCkqXLo2DgwO+vr5s2LDB6PnAwEDefPNNcufOjU6nM1QI4oWFhaHT6RJdVqxYYSiX2PPLli1Ls/dtbvGHWcYLEEIIkVkcPHiQSpUqUalSJQAGDhxIpUqVGDFiBADh4eGGxABAkSJFWL9+PUFBQfj5+TFlyhR+/PFHAgICzBK/yLr8/GDZMjh1Crp3V+MMbN8Ob76p6mqrV6sfcYQQIi2ZNRmwfPlyBg4cyMiRIzl8+DB+fn4EBARw48aNRMvv2bOHjh070r17d4KDg2nZsiUtW7Y0mh/44cOH1KlThwkTJiS6DW9vb8LDw42W0aNH4+zsTJMmTYzKLliwwKhccnMVW5v4wQOli4AQQojMon79+mialmBZuHAhAAsXLmT79u0JXhMcHEx0dDTnz5+nS5cuGR63EPFKloQff4Tz56F/f3B0hIMHoVUrKF9ezUoQG5vwdXq9Sh4sXapu9fqMjlwIYY10mma+xkc1atSgWrVqfPvttwDExcXh7e1N3759GTJkSILy7du35+HDh/zxxx+Gda+99hoVK1Zk7ty5RmXDwsIoUqQIwcHBVKxYMdk4KlWqROXKlfnpp58M63Q6HatXr36lBEBUVBSurq5ERkZa3ABDxYurE82mTarfmhBCiMzPks9L1kqOqUhPN2+qWQdmzVLTEgL4+MDnn0PXrmr6wsBAlTi4cuXZ67y81OtatTJL2EIIMzP13GS2lgExMTEcOnSIRo0aPQvGxoZGjRqxN4lRU/bu3WtUHiAgICDJ8qY4dOgQISEhdO/ePcFzH3/8MXny5KF69erMnz+flPIm0dHRREVFGS2W6PZtlQgANb2NEEIIIYSwPHnzwtdfw6VLMHasehwWBr17Q5Ei0LkztGljnAgAuHpVrQ8MNEvYQggrYbZkwK1bt9Dr9Xh4eBit9/DwICIiItHXREREpKq8KX766SfKlClDrVq1jNZ/9dVX/PbbbwQFBdG6dWt69+7NrFmzkt3WuHHjcHV1NSze3t4vHVd6OnhQ3ZYoAe7u5o1FCCGEEEIkz9UVhg5ViYCZM8HbGyIi4OefEx9gMH7dgAHSZUAIkTSzDyBoTo8fP2bJkiWJtgoYPnw4tWvXplKlSgwePJjPP/+cSZMmJbu9oUOHEhkZaVguX76cXqG/EhkvQAghhBDC+jg5Qd++cO6c6iqQHE2Dy5dh586MiU0IYX3MlgzIkycPtra2XL9+3Wj99evX8fT0TPQ1np6eqSqfkpUrV/Lo0SOjaYaSUqNGDa5cuUJ0dHSSZezt7XFxcTFaLJHMJCCEEEIIYb3s7CCFIbEMwsPTNRQhhBUzWzLAzs6OKlWqsGXLFsO6uLg4tmzZQs2aNRN9Tc2aNY3KAwQFBSVZPiU//fQTzZs3J2/evCmWDQkJwd3dHXt7+5fal6XQNGkZIIQQQghh7fLnN63c/PmwbZtMTSiESCibOXc+cOBAOnfuTNWqValevTrTp0/n4cOHdO3aFYBOnTpRsGBBxo0bB0D//v2pV68eU6ZMoWnTpixbtoyDBw/yww8/GLZ5584dLl26xLVr1wA4ffo0oFoVPN+C4Ny5c+zYsYMNGzYkiGvdunVcv36d1157DQcHB4KCghg7diyffvppuh2LjHL1qupjZmtrekZZCCGEEEJYFn9/NWvA1auJjxsQb/NmtRQpAl26qKVQoYyKUghhycw6ZkD79u2ZPHkyI0aMoGLFioSEhLBx40bDIIGXLl0i/Lm2TbVq1WLJkiX88MMP+Pn5sXLlStasWUP58uUNZdauXUulSpVo2rQpAB06dKBSpUoJph6cP38+Xl5evPnmmwniyp49O7Nnz6ZmzZpUrFiR77//nqlTpzJy5Mj0OAwZKr6LgK+v6ncmhBBCCCGsj62tmj4QQKczfk6nU8u4cdCzJ+TMCaGhMHKkmprwzTdh2TJ48iTDwxZCWBCdltJ8eeKlWeLcw0OHwvjx0KMHPNegQgghRBZgieclayfHVJhbYCD07288vaC3N0yfDq1aqcePHqly8V0G4rm5wbvvQrduULlywqSCEMI6mXpuytKzCWRFMl6AEEIIIUTm0aqVmnJw2zZYskTdhoY+SwSAag363nuwdSucPw8jRqiEwb178N13ULUq+PmpBMLNm2Z6I0KIDCctA9KRpf1aEBcH7u4QFQUhIepLXwghRNZhaeelzECOqbBWer1KDsyfD6tXQ/yEWdmzQ/Pm0LUrBARANrOOMCaEeBnSMkAkcPasSgQ4OkK5cuaORgghhBBCmIutLbzxBixdqqYfnD0bqlSB2FhYtQreflsNNDh0KJw5Y+5ohRDpQZIBWUh8F4FKlSTLK4QQQgghFHd36N0bDh6EI0dgwADInVslCcaPh1KloE4d1Yrg/n1zRyuESCuSDMhC4mcSqF7dvHEIIYQQQgjLVKECTJsG166pFgJNm4KNDezeDd27Q/78qgvBzp3JT2kohLB8kgzIQmTwQCGEEEIIYQo7OzUI4R9/wOXLqoVAyZLw8CEsXAh166rHY8fC1avmjlYI8TIkGZBFxMZCcLC6L8kAIYQQQghhqgIFYPBgOHUKdu1SUxE6O8O5c/Dll2psgbfeghUrng1EKISwfJIMyCKOHVNfzm5uULy4uaMRQgghhBDWRqeD2rXhp5/UeAILFoC/v5qx6s8/oV07lTjo31/NXCWEsGySDMginu8ioNOZNxYhhBBCCGHdnJ2hSxfYsUPNNvDFFyoRcOcOzJypBqyuXBm+/VatE0JYHkkGZBEyXoAQQgghhEgPJUrAN9/ApUuwYQO0bQvZs6suqn37qkEH27eHv/4CvT7h6/V62L5dTXO4fXviZYQQaU+SAVmEzCQghBBCCCHSk60tNGkCv/2mZiOYMQP8/CAmRq1r3Bh8fGDYMDh/Xr0mMFCta9AA3n1X3fr4qPVCiPSl0zSZFCS9REVF4erqSmRkJC4uLmaL4+FDcHFR/bmuXlVNuIQQQmQ9lnJeykzkmAqRsuBgmD8fFi+Gu3efrS9bFv79N2H5+C6tK1eqGQ2EEKlj6rlJWgZkAcHBKhFQoIAkAoQQQgghRMaqVAlmzVKtBZYvh4AAtT6xRABA/E+VAwZIlwEh0pMkA7IA6SIghBBCCCHMzcFBzTiwcaNKCiRH0+DyZdi2LWNiEyIrymbuAET6k8EDhRBCCCGEJTH1F/+334Z69aB+fbVUraoGJxRCvDpJBmQBkgwQQgghhBCWJH9+08pFR8OmTWoByJED6tR5lhyoUkWSA0K8LEkGZHK3bz8brbVqVfPGIoQQQgghBIC/P3h5qcGtExvOXKdTz69bBzt2qCkH//5b1W3/+kstYJwcaNAAKleW5IAQppJkQCZ38KC6LVEC3N3NG4sQQgghhBCgpiGcMQPatFEX/s8nBOJnE5g+XU1N6OcHffuqAbFPnFCJgfjlzh3j5ICzc8KWA9nkikeIRMm/RiYnXQSEEEIIIYQlatVKTR/Yvz9cufJsvZeXSgS8OK2gjQ34+qolPjlw/PizxMDff6vkwMaNagGVHPD3f5YcqFxZkgNCxJN/hUxOZhIQQgghhBCWqlUraNECdu6E8HA1loC/v2o5kBIbG6hQQS39+hknB7ZtU8mBu3fhzz/VAsbJgQYN1LSHkhwQWZVO0xLrpSPSQlRUFK6urkRGRuLi4pLh+9c0KFAAIiJg926oVSvDQxBCCGFBzH1eyozkmAphueLi4Ngx45YDd+8al8mZ07jlgCQHRGZg6rlJkgHpyNwVhCtXwNtbZVajosDJKcNDEEIIYUHMfV7KjOSYCmE94pMD27Y9Sw7cu2dcJj450KCBSg5UrJhyckCvf7mWDUKkF1PPTZL3ysTiuwj4+koiQAghhBBCZG02Ns8GJBwwQF3Ev9hy4N492LBBLQAuLglbDjx/oR8YmPiYBzNmJBzzQAhLI8mATEwGDxRCCCGEECJxtrbql/+KFZ8lB44eNU4OREbC+vVqAZUcqFtXJQbi4mDw4IRTI169qmZJWLlSEgLCskkyIBOTZIAQQgghhBCmsbVVv/xXqgSffPIsORDfrWDHDpUc+OMPtSRF09T0iAMGqMERpcuAsFQ25g5ApI+4uGfJAJlJQAghhBBCiNSJTw4MHAhr18Lt23DoEEyZAjVrJv9aTYPLl9VYAkJYKmkZkEmdPasGDXR0hHLlzB2NEEIIIYQQ1s3WFipXVkv+/LB3b8qvmTsXfHzUIoSlkZYBmVR8qwCZHkUIIYQQQoi0lT+/aeWWL4ciRVRLgpkz1YwDQlgKSQZkUvEzCUgXASGEEEIIIdKWv7+aNUCnS/x5nQ7c3dUUhTod7NunZh3w8oKGDeHHH+HOnYyNWYgXSTIgk5LBA4UQQgghhEgftrZq+kBImBCIf/zjj7B1q5pdYMYMeO01Na7X1q3Qowd4ekKzZrB4MTx4kLHxCwEWkAyYPXs2Pj4+ODg4UKNGDfbH/6SdhBUrVlC6dGkcHBzw9fVlQ/wkoP8JDAzkzTffJHfu3Oh0OkJCQhJso379+uh0OqPlo48+Mipz6dIlmjZtipOTE/ny5eOzzz7j6dOnr/x+M0JsLAQHq/uSDBBCCCGEECLttWqlpg8sWNB4vZeX8bSC+fNDv35qjIELF2DcOPDzU3X2P/6A996DfPmgXTtYvRqePMn49yKyJrMmA5YvX87AgQMZOXIkhw8fxs/Pj4CAAG7cuJFo+T179tCxY0e6d+9OcHAwLVu2pGXLlhw/ftxQ5uHDh9SpU4cJEyYku+8ePXoQHh5uWCZOnGh4Tq/X07RpU2JiYtizZw+LFi1i4cKFjBgxIm3eeDo7dgyio8HNDYoXN3c0QgghhBBCZE6tWkFYmJp+cMkSdRsa+iwR8KIiRWDIEAgJgRMnYPhwVV9//BhWrFCv8/CALl3gr79UwkCI9KLTNE0z185r1KhBtWrV+PbbbwGIi4vD29ubvn37MmTIkATl27dvz8OHD/njuYk9X3vtNSpWrMjcuXONyoaFhVGkSBGCg4OpWLGi0XP169enYsWKTJ8+PdG4/vzzT95++22uXbuGh4cHAHPnzmXw4MHcvHkTOzs7k95fVFQUrq6uREZG4uLiYtJr0sL338NHH8Ebb8CmTRm2WyGEEBbOXOelzEyOqRDiVWkaHD4My5ap5cqVZ8/lyQNt20KHDlCnDtiYvV23sAamnpvM9nGKiYnh0KFDNGr0//buPCzKev//+GsYZZEjhKECouKW5l5qhlbmkcDleOVSWmERevLS0CyPLbiEZWmL2uqx9Jy0NJcs9dgiRqalHb+lKS5pHrVcktBME6TChM/vj/sHNqk5KnBzM8/Hdd3XMPfcM/Oayas395v78/nEnQ7j56e4uDitO8c6HevWrfM4XpISEhLOefyfefPNNxUeHq7mzZsrNTVVP//8s8f7tGjRorgRUPQ+OTk5+uqrr875mvn5+crJyfHY7MB8AQAAAIAzuFxSmzbSs89K+/ZJa9ZI994rVa8uHTkiTZ8udeok1a0rjRolbdhgNRCAS2VbM+DIkSMqKCjwOOGWpJo1ayo7O/usz8nOzr6g48/ljjvu0Ny5c7Vq1SqlpqZqzpw5GjBgwHnfp+ixc5k0aZJCQ0OLt9q1a19QrpLCSgIAAACA8/j5WVcATJsmZWVZQwXuvlsKCbGuGJgyxfqD3xVXWEMMtm+3OzGczCcvNBk8eLASEhLUokULJSYm6o033tCSJUu0Z8+eS3rd1NRUHT9+vHg7cOBACSX2Xl6eNf5I4soAAAAAwKkqVZLi46VZs6RDh6zJBfv1k4KCpN27pSeekJo1syYjnDTJmqsAuBC2NQPCw8Pldrt16NAhj/2HDh1SRETEWZ8TERFxQcd7q3379pKk3bt3/+n7FD12LgEBAQoJCfHYytqmTdaSJVFR1gYAAADA2QIDpV69pIULpcOHreUIe/aUKleWtmyRRo+W6te3li984QXp++/tTgwnsK0Z4O/vrzZt2mjlypXF+woLC7Vy5UrFxsae9TmxsbEex0tSRkbGOY/3VtHyg5GRkcXvs3XrVo9VDTIyMhQSEqKmTZte0nuVNoYIAAAAABXXX/4i3XGHtGyZlJ0t/etfUpcu1hCDzz+X7r/fWu7wr3+VZs6Ufvzxz1+voEBavVqaP9+6LSgogw+BcsHWYQIjR47UzJkz9frrr2vHjh0aOnSo8vLylJycLEm66667lJqaWnz8iBEjlJ6erilTpujrr7/W+PHjtWHDBg0bNqz4mKNHjyozM1Pb//8Amp07dyozM7N4rP+ePXs0YcIEffnll9q7d6+WLVumu+66SzfccINatmwpSYqPj1fTpk115513avPmzVqxYoXGjh2rlJQUBQQElNXXc1GYPBAAAADwDdWqSYMGSR99JB08KL34ohQba00wuGqVNHiwFBEh/e1v0ty5Um6u5/MXL5ZiYqTOna0GQ+fO1v3Fi+34NChrti4tKEkvv/yynn32WWVnZ6t169Z68cUXiy/bv/HGGxUTE6PZs2cXH79o0SKNHTtWe/fuVaNGjfTMM8+oe/fuxY/Pnj27uJnwe2lpaRo/frwOHDigAQMGaNu2bcrLy1Pt2rXVu3dvjR071uOy/n379mno0KFavXq1goODlZSUpKeeekqVKlXy+rPZsdxQw4bSnj3WkoI33VQmbwkAcAiWwSt5fKcAyqO9e60hBfPnS5s3n94fGGg1Bm6/XTp50moA/PFs0OWybt9+W+rTp8wiowR5W5tsbwZUZGX9C8KPP1prkUrS0aNSWFipvyUAwEE4cS15fKcAyrsdO6QFC6zGwK5dp/e7XOdeotDlkqKjrUkJ3e6yyYmS421t8snVBCqqDRus20aNaAQAAHzTtGnTFBMTo8DAQLVv315fFE2mcxa//fabHn/8cTVo0ECBgYFq1aqV0tPTyzAtAJS+K6+UHntM2rlT+vJLadQoqXr1czcCJOuxAwekNWvKLifKHs2ACoT5AgAAvmzhwoUaOXKk0tLStHHjRrVq1UoJCQkeEwL/3tixY/Xqq6/qpZde0vbt2zVkyBD17t1bmzZtKuPkAFD6XC7p6qulZ5+VnnvOu+ewKkHFRjOgAmElAQCAL5s6daruueceJScnq2nTpnrllVdUpUoVvfbaa2c9fs6cORo9erS6d++u+vXra+jQoerevbumTJlSxskBoGzVquXdca+8IqWns8JARUUzoIIwhisDAAC+6+TJk/ryyy8VFxdXvM/Pz09xcXFat27dWZ+Tn5+vwMBAj31BQUFau3btOd8nPz9fOTk5HhsAOM3111tzAhRNFngun34qdetmrTAwbpz0zTdlEg9lhGZABXHwoLXOqNsttW5tdxoAAMrWkSNHVFBQoJo1a3rsr1mzZvHywn+UkJCgqVOnateuXSosLFRGRoYWL16s7//kuthJkyYpNDS0eKtdu3aJfg4AKAtut/TCC9bPf2wIuFzWNmWKNHy4NRfZd99JTzwhNWggdekivfmm9MsvZZ8bJYtmQAVRNESgRQupShV7swAA4AQvvPCCGjVqpCZNmsjf31/Dhg1TcnKy/PzO/etRamqqjh8/XrwdOHCgDBMDQMnp08daPvCPQwaio639I0dKL74oZWVZqxHcdJPVJPj4Y2nAACkqSkpJsSYlZH06Z6IZUEEwRAAA4MvCw8Pldrt16NAhj/2HDh1SRETEWZ9TvXp1LV26VHl5edq3b5++/vpr/eUvf1H9+vXP+T4BAQEKCQnx2ADAqfr0kfbulVatkubNs26//dbaXyQwUOrfX/rwQ+ux8eOlunWln36S/vlPqW1b6aqrpJdespY3h3PQDKggaAYAAHyZv7+/2rRpo5UrVxbvKyws1MqVKxUbG/unzw0MDFStWrV06tQpvfPOO7r55ptLOy4AlBtut3TjjdLtt1u3bve5j61bV0pLs+YOyMiQbrtN8veXNm+W7rtPioy09mVkSIWFZfUJcLFoBlQAhYWnmwGsJAAA8FUjR47UzJkz9frrr2vHjh0aOnSo8vLylJycLEm66667lJqaWnz8559/rsWLF+ubb77RmjVr1LVrVxUWFuqhhx6y6yMAgCP4+UlxcdL8+dbygy+9ZM1bdvKktHChFB8v1atnXUWwb5/daXEuNAMqgF27pJwcKShIatbM7jQAANijf//+mjx5sh599FG1bt1amZmZSk9PL55UcP/+/R6TA/76668aO3asmjZtqt69e6tWrVpau3atLrvsMps+AQA4T7Vq0rBh0qZN1vwBKSnSZZdJ+/dLjz1mNQXi4615B3791e60+D2XMUz3UFpycnIUGhqq48ePl+qYwrlzpTvvlDp0kD77rNTeBgDgcGVVl3wJ3ykAnOmXX6QlS6TXXpN+N3pLYWFSYqI0aBAroJUmb2sTVwZUAEUrCTBEAAAAAIDdgoKkO+6QPvrIml9g3DhrlYJjx6SXX7YmHGzTxpqA8Ngxu9P6LpoBFQCTBwIAAAAoj+rVkx5/3Fq1ID1duvVWqXJlaeNGa0hBVJR1tcDHHzPpYFmjGeBwv/1mjc+RaAYAAAAAKJ/cbikhQXrrLSkrS3r+ealFC2segXnzpC5dpIYNpQkTpAMH7E7rG2gGONzWrVJ+vjVJR8OGdqcBAAAAgD8XHi6NGGEtSbh+vTRkiBQSIn37rfToo9YShl27SosWWec6KB00Axzu90MEXC57swAAAACAt1wuqW1bafp0a4nCOXOkG2+UjJFWrJD69ZNq1ZIeeMD6I+jZFBRIq1dbyxyuXm3dh3doBjgc8wUAAAAAcLoqVaQBA6RVq6yl08eMsRoBP/5oDSlo2dKaMP3VV6Xjx63nLF4sxcRInTtbExZ27mzdX7zYxg/iIDQDHK5oJQGaAQAAAAAqgoYNpSeekPbtk95/X+rTR6pU6fSQgshI6wqCvn2l777zfO7Bg9Itt9AQ8AbNAAfLy5O++sr6mWUFAQAAAFQkbrfUvbv0zjvWpINTpkhNm0q//CJ98snZn2OMdXv//QwZOB+aAQ62aZO1/EZUlLUBAAAAQEVUvbo0cqS0bZs0bdqfH2uMtSLBmjVlk82paAY4GEMEAAAAAPgSl0sKC/Pu2O+/L90sTkczwMGKJg9kiAAAAAAAXxEZ6d1xv/5aujmcjmaAg7GSAAAAAABfc/31UnT0+ZdWv+ceadQoKTe3bHI5Dc0Ah/rxR2nPHuvntm3tzQIAAAAAZcXtll54wfr5jw0Bl8varrnGmkBwyhSpcWNp/vzTkwvCQjPAoTZssG4bNfJ+zAwAAAAAVAR9+khvvy3VquW5Pzra2v/559IHH0gNGlhzB9xxh9S58+nV2EAzwLEYIgAAAADAl/XpI+3dK61aJc2bZ91++621X5K6dbNWH5gwQQoKspYjbNVK+sc/pJwcW6OXCzQDHIqVBAAAAAD4OrdbuvFG6fbbrVu32/PxwEBp7Fhp+3apVy9r6MDUqVKTJtKbb/r20AGaAQ5kDCsJAAAAAIC3YmKkJUuk5culhg2toQMDBlgNhG3b7E5nD5oBDnTwoJSdbXW9Wre2Ow0AAAAAOEPXrtbJ/xNPWEMHPv3UOqcaOdL3hg7QDHCgoiECzZtLVarYmwUAAAAAnCQgQBozRtqxQ+rd2xo68Nxz1qoDvjR0wPZmwLRp0xQTE6PAwEC1b99eXxSd6Z7DokWL1KRJEwUGBqpFixb64IMPPB5fvHix4uPjdfnll8vlcikzM9Pj8aNHj2r48OFq3LixgoKCVKdOHd133306fvy4x3Eul+uMbcGCBSXymS8VQwQAAAAA4NLUrSstXiylp1urtGVnnx46sHWr3elKn63NgIULF2rkyJFKS0vTxo0b1apVKyUkJOjw4cNnPf6///2vbr/9dg0aNEibNm1Sr1691KtXL2373SCPvLw8XXfddXr66afP+hpZWVnKysrS5MmTtW3bNs2ePVvp6ekaNGjQGcfOmjVL33//ffHWq1evEvncl4qVBAAAAACgZCQkWCf/EyeeHjpw1VXSAw9If/ibcYXiMsa+iyDat2+vdu3a6eWXX5YkFRYWqnbt2ho+fLgeeeSRM47v37+/8vLy9N577xXvu/baa9W6dWu98sorHsfu3btX9erV06ZNm9T6PAPrFy1apAEDBigvL0+VKlWSZF0ZsGTJkktqAOTk5Cg0NFTHjx9XSEjIRb/O7xUWSmFh1niWTZuYMwAA4L3SqEu+ju8UACqW/fut+QPeece6X7OmNHmylJgouVz2ZvOWt7XJtisDTp48qS+//FJxcXGnw/j5KS4uTuvWrTvrc9atW+dxvCQlJCSc83hvFX1JRY2AIikpKQoPD9c111yj1157Tefrm+Tn5ysnJ8djK2m7dlmNgKAgqVmzEn95AAAAAPBZdepIb78trVghXXGFdOiQdOedUqdO0pYtdqcrWbY1A44cOaKCggLVrFnTY3/NmjWVnZ191udkZ2df0PHe5pgwYYIGDx7ssf/xxx/XW2+9pYyMDPXt21f33nuvXnrppT99rUmTJik0NLR4q1279kXnOpeiIQJXXSVVrlziLw8AAAAAPi8+3jr5nzjRmrR9zRrp6qulESMqztAB2ycQtFNOTo569Oihpk2bavz48R6PjRs3Th07dtRVV12lhx9+WA899JCeffbZP3291NRUHT9+vHg7cOBAiWcuml+R+QIAAAAAoPQEBEipqdaqA7fcYq068OKL1qoDb7zh/FUHbGsGhIeHy+1269ChQx77Dx06pIiIiLM+JyIi4oKO/zO5ubnq2rWrqlatqiVLlqjyef7M3r59e3333XfKz88/5zEBAQEKCQnx2EoaKwkAAAAAQNmpU0datEj68MPTQweSkqQbbpA2b7Y73cWzrRng7++vNm3aaOXKlcX7CgsLtXLlSsXGxp71ObGxsR7HS1JGRsY5jz+XnJwcxcfHy9/fX8uWLVNgYOB5n5OZmamwsDAFBARc0HuVpN9+syYNlLgyAAAAAADK0k03WUMHJk2yhg6sXWsNHbjvPumnn+xOd+Eqnf+Q0jNy5EglJSWpbdu2uuaaa/T8888rLy9PycnJkqS77rpLtWrV0qRJkyRJI0aMUKdOnTRlyhT16NFDCxYs0IYNGzRjxozi1zx69Kj279+vrKwsSdLOnTslWVcVREREFDcCfv75Z82dO9djor/q1avL7Xbr3Xff1aFDh3TttdcqMDBQGRkZmjhxokaNGlWWX88Ztm6V8vOlyy6TGja0NQoAAAAA+JyAAOmRR6zVBf7xD+uKgZdekhYulJ55xpps0M8hg/FtbQb0799fP/zwgx599FFlZ2erdevWSk9PL54kcP/+/fL73TfZoUMHzZs3T2PHjtXo0aPVqFEjLV26VM2bNy8+ZtmyZcXNBEm67bbbJElpaWkaP368Nm7cqM8//1yS1PAPZ9TffvutYmJiVLlyZU2bNk0PPPCAjDFq2LChpk6dqnvuuafUvgtvFA0RaNfOOctaAAAAAEBFU7u29NZb0kcfScOGSTt3SnffLc2cKU2bJrVqZXfC83OZ862Xh4tW0msP//3v0r//LY0eLT35ZAkEBAD4lJKuS+A7BQBIJ09Kzz8vPf64lJdnXRmQkmLdv+yyss/jbW1yyAUMkFhJAAAAAADKG39/6aGHpK+/lvr1kwoLraEDjRtLs2db98sjmgEOkZcnffWV9TMrCQAAAABA+RIdbc0dkJEhNWkiHT4sJSdL1113eiL48oRmgENs2mR1lKKirA0AAAAAUP7ExVlLDj7zjBQcLK1bJ7Vta80tcOyY3elOoxngAAUF0rx51s/16ln3AQAAAADlk7+/9OCD1tCB/v2tP+xOm2YNHZg16/TQgYICafVqaf5867Ysz/VoBpRzixdLMTHS9OnW/c8+s+4vXmxnKgAAAADA+URHSwsWWKsOXHml9MMP0sCB1tCByZOtc7vOnaU77rBuy/Jcj2ZAObZ4sXTLLdJ333nuP3jQ2k9DAAAAAADKvy5dpMxMz6EDDz5o77kezYByqqBAGjFCOtvCj0X77r+fIQMAAAAA4ARFQwe2b5eCgs5+TFme69EMKKfWrDmzS/R7xkgHDljHAQAAAACc4ZtvpF9+OffjZXWuRzOgnPr++5I9DgAAAABgv/JyrkczoJyKjCzZ4wAAAAAA9isv53o0A8qp66+3Zp50uc7+uMsl1a5tHQcAAAAAcIbycq5HM6CccrulF16wfv7jP5Ki+88/bx0HAAAAAHCG8nKuRzOgHOvTR3r7balWLc/90dHW/j597MkFAAAAALh45eFcr1LpvwUuRZ8+0s03WzNJfv+9NW7k+uu5IgAAAAAAnMzucz2aAQ7gdks33mh3CgAAAABASbLzXI9hAgAAAAAA+BiaAQAAAAAA+BiaAQAAAAAA+BiaAQAAAAAA+BiaAQAAAAAA+BiaAQAAAAAA+BiWFixFxhhJUk5Ojs1JAAA4XY+K6hMuHbUeAFDeeFvvaQaUotzcXElS7dq1bU4CAMBpubm5Cg0NtTtGhUCtBwCUV+er9y7DnwdKTWFhobKyslS1alW5XK5Leq2cnBzVrl1bBw4cUEhISAklLDvktxf57UV+e5H/NGOMcnNzFRUVJT8/RgqWBGr9aeS3F/ntRX57kd+Tt/WeKwNKkZ+fn6Kjo0v0NUNCQhz5D7wI+e1FfnuR317kt3BFQMmi1p+J/PYiv73Iby/yn+ZNvefPAgAAAAAA+BiaAQAAAAAA+BiaAQ4REBCgtLQ0BQQE2B3lopDfXuS3F/ntRX44hdP/W5PfXuS3F/ntRf6LwwSCAAAAAAD4GK4MAAAAAADAx9AMAAAAAADAx9AMAAAAAADAx9AMAAAAAADAx9AMKOc+/fRT9ezZU1FRUXK5XFq6dKndkbw2adIktWvXTlWrVlWNGjXUq1cv7dy50+5YF2T69Olq2bKlQkJCFBISotjYWC1fvtzuWBflqaeeksvl0v333293FK+NHz9eLpfLY2vSpIndsbx28OBBDRgwQJdffrmCgoLUokULbdiwwe5YXouJiTnj+3e5XEpJSbE7mlcKCgo0btw41atXT0FBQWrQoIEmTJggp8ybm5ubq/vvv19169ZVUFCQOnTooPXr19sdC6XAybVecn69p9bby+m1XnJ2vafW28/Oel+pTN4FFy0vL0+tWrXSwIED1adPH7vjXJBPPvlEKSkpateunU6dOqXRo0crPj5e27dvV3BwsN3xvBIdHa2nnnpKjRo1kjFGr7/+um6++WZt2rRJzZo1szue19avX69XX31VLVu2tDvKBWvWrJk++uij4vuVKjnjf1vHjh1Tx44d1blzZy1fvlzVq1fXrl27FBYWZnc0r61fv14FBQXF97dt26abbrpJt956q42pvPf0009r+vTpev3119WsWTNt2LBBycnJCg0N1X333Wd3vPP6+9//rm3btmnOnDmKiorS3LlzFRcXp+3bt6tWrVp2x0MJcnKtl5xf76n19nNqrZecX++p9faztd4bOIYks2TJErtjXLTDhw8bSeaTTz6xO8olCQsLM//617/sjuG13Nxc06hRI5ORkWE6depkRowYYXckr6WlpZlWrVrZHeOiPPzww+a6666zO0aJGjFihGnQoIEpLCy0O4pXevToYQYOHOixr0+fPiYxMdGmRN77+eefjdvtNu+9957H/quvvtqMGTPGplQoC06v9cZUjHpPrS87Tq71xlS8ek+tL1t213uGCaDMHD9+XJJUrVo1m5NcnIKCAi1YsEB5eXmKjY21O47XUlJS1KNHD8XFxdkd5aLs2rVLUVFRql+/vhITE7V//367I3ll2bJlatu2rW699VbVqFFDV111lWbOnGl3rIt28uRJzZ07VwMHDpTL5bI7jlc6dOiglStX6n//+58kafPmzVq7dq26detmc7LzO3XqlAoKChQYGOixPygoSGvXrrUpFeAdJ9d7ar09nFrrpYpV76n1Zc/2el/q7QaUGDn4rwUFBQWmR48epmPHjnZHuWBbtmwxwcHBxu12m9DQUPP+++/bHclr8+fPN82bNze//PKLMcY47q8FH3zwgXnrrbfM5s2bTXp6uomNjTV16tQxOTk5dkc7r4CAABMQEGBSU1PNxo0bzauvvmoCAwPN7Nmz7Y52URYuXGjcbrc5ePCg3VG8VlBQYB5++GHjcrlMpUqVjMvlMhMnTrQ7ltdiY2NNp06dzMGDB82pU6fMnDlzjJ+fn7niiivsjoZS5ORab4xz6z213j5OrvXGVKx6T623h531nmaAgzj5F4QhQ4aYunXrmgMHDtgd5YLl5+ebXbt2mQ0bNphHHnnEhIeHm6+++sruWOe1f/9+U6NGDbN58+bifU77BeGPjh07ZkJCQhxx6WblypVNbGysx77hw4eba6+91qZElyY+Pt787W9/szvGBZk/f76Jjo428+fPN1u2bDFvvPGGqVatmmN+Qdu9e7e54YYbjCTjdrtNu3btTGJiomnSpInd0VCKnFzrjXFuvafWlx9OqvXGVKx6T623h531nmaAgzj1F4SUlBQTHR1tvvnmG7ujlIguXbqYwYMH2x3jvJYsWVL8P5WiTZJxuVzG7XabU6dO2R3xorRt29Y88sgjdsc4rzp16phBgwZ57PvnP/9poqKibEp08fbu3Wv8/PzM0qVL7Y5yQaKjo83LL7/ssW/ChAmmcePGNiW6OCdOnDBZWVnGGGP69etnunfvbnMilCan1npjKla9p9bbyym13piKU++p9fazo94zZwBKjTFGw4YN05IlS/Txxx+rXr16dkcqEYWFhcrPz7c7xnl16dJFW7duVWZmZvHWtm1bJSYmKjMzU2632+6IF+zEiRPas2ePIiMj7Y5yXh07djxjaa3//e9/qlu3rk2JLt6sWbNUo0YN9ejRw+4oF+Tnn3+Wn59nmXO73SosLLQp0cUJDg5WZGSkjh07phUrVujmm2+2OxLgoSLWe2q9fZxU66WKU++p9fazo947Z90OH3XixAnt3r27+P63336rzMxMVatWTXXq1LEx2fmlpKRo3rx5+s9//qOqVasqOztbkhQaGqqgoCCb03knNTVV3bp1U506dZSbm6t58+Zp9erVWrFihd3Rzqtq1apq3ry5x77g4GBdfvnlZ+wvr0aNGqWePXuqbt26ysrKUlpamtxut26//Xa7o53XAw88oA4dOmjixInq16+fvvjiC82YMUMzZsywO9oFKSws1KxZs5SUlOSopZ4kqWfPnnryySdVp04dNWvWTJs2bdLUqVM1cOBAu6N5ZcWKFTLGqHHjxtq9e7cefPBBNWnSRMnJyXZHQwlzcq2XnF/vqfX2cnKtlypGvafW28vWel/q1x7gkqxatcpIOmNLSkqyO9p5nS23JDNr1iy7o3lt4MCBpm7dusbf399Ur17ddOnSxXz44Yd2x7poThtH2L9/fxMZGWn8/f1NrVq1TP/+/c3u3bvtjuW1d9991zRv3twEBASYJk2amBkzZtgd6YKtWLHCSDI7d+60O8oFy8nJMSNGjDB16tQxgYGBpn79+mbMmDEmPz/f7mheWbhwoalfv77x9/c3ERERJiUlxfz00092x0IpcHKtN8b59Z5aby+n13pjnF/vqfX2srPeu4wxpvRbDgAAAAAAoLxgzgAAAAAAAHwMzQAAAAAAAHwMzQAAAAAAAHwMzQAAAAAAAHwMzQAAAAAAAHwMzQAAAAAAAHwMzQAAAAAAAHwMzQAAAAAAAHwMzQAAFYbL5dLSpUvtjgEAAEoR9R4oGTQDAJSIu+++Wy6X64yta9eudkcDAAAlhHoPVByV7A4AoOLo2rWrZs2a5bEvICDApjQAAKA0UO+BioErAwCUmICAAEVERHhsYWFhkqxL+qZPn65u3bopKChI9evX19tvv+3x/K1bt+qvf/2rgoKCdPnll2vw4ME6ceKExzGvvfaamjVrpoCAAEVGRmrYsGEejx85ckS9e/dWlSpV1KhRIy1btqz4sWPHjikxMVHVq1dXUFCQGjVqdMYvMwAA4M9R74GKgWYAgDIzbtw49e3bV5s3b1ZiYqJuu+027dixQ5KUl5enhIQEhYWFaf369Vq0aJE++ugjj+I/ffp0paSkaPDgwdq6dauWLVumhg0berzHY489pn79+mnLli3q3r27EhMTdfTo0eL33759u5YvX64dO3Zo+vTpCg8PL7svAAAAH0C9BxzCAEAJSEpKMm632wQHB3tsTz75pDHGGElmyJAhHs9p3769GTp0qDHGmBkzZpiwsDBz4sSJ4sfff/994+fnZ7Kzs40xxkRFRZkxY8acM4MkM3bs2OL7J06cMJLM8uXLjTHG9OzZ0yQnJ5fMBwYAwAdR74GKgzkDAJSYzp07a/r06R77qlWrVvxzbGysx2OxsbHKzMyUJO3YsUOtWrVScHBw8eMdO3ZUYWGhdu7cKZfLpaysLHXp0uVPM7Rs2bL45+DgYIWEhOjw4cOSpKFDh6pv377auHGj4uPj1atXL3Xo0OGiPisAAL6Keg9UDDQDAJSY4ODgMy7jKylBQUFeHVe5cmWP+y6XS4WFhZKkbt26ad++ffrggw+UkZGhLl26KCUlRZMnTy7xvAAAVFTUe6BiYM4AAGXm//7v/864f+WVV0qSrrzySm3evFl5eXnFj3/22Wfy8/NT48aNVbVqVcXExGjlypWXlKF69epKSkrS3Llz9fzzz2vGjBmX9HoAAMAT9R5wBq4MAFBi8vPzlZ2d7bGvUqVKxZP2LFq0SG3bttV1112nN998U1988YX+/e9/S5ISExOVlpampKQkjR8/Xj/88IOGDx+uO++8UzVr1pQkjR8/XkOGDFGNGjXUrVs35ebm6rPPPtPw4cO9yvfoo4+qTZs2atasmfLz8/Xee+8V/3ICAAC8Q70HKgaaAQBKTHp6uiIjIz32NW7cWF9//bUka+bfBQsW6N5771VkZKTmz5+vpk2bSpKqVKmiFStWaMSIEWrXrp2qVKmivn37aurUqcWvlZSUpF9//VXPPfecRo0apfDwcN1yyy1e5/P391dqaqr27t2roKAgXX/99VqwYEEJfHIAAHwH9R6oGFzGGGN3CAAVn8vl0pIlS9SrVy+7owAAgFJCvQecgzkDAAAAAADwMTQDAAAAAADwMQwTAAAAAADAx3BlAAAAAAAAPoZmAAAAAAAAPoZmAAAAAAAAPoZmAAAAAAAAPoZmAAAAAAAAPoZmAAAAAAAAPoZmAAAAAAAAPoZmAAAAAAAAPub/Aen11xFJQTK2AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. An Initial Training Analysis & Conclusion\n",
        "\n",
        "### Accuracy (정확도):\n",
        "초기 정확도는 0.0116에서 시작하여 10번째 에포크에서 0.0286까지 증가했는데 이는 모델이 학습을 통해 점차 개선되고 있음을 의미한다.\n",
        "\n",
        "### Loss (손실):\n",
        "초기 손실 값은 1.2847에서 시작하여 10번째 에포크에서 0.8265까지 감소했는데 손실 값이 감소하는 것은 모델이 점차 더 나은 예측을 하고 있음을 의미한다.\n",
        "\n",
        "### Validation Accuracy (검증 정확도):\n",
        "검증 정확도는 0.0256에서 시작하여 10번째 에포크에서 0.0251로 약간 감소했는데 이는 모델이 학습 데이터에 과적합(overfitting)되고 있을 가능성을 시사한다.\n",
        "\n",
        "### Validation Loss (검증 손실):\n",
        "검증 손실 값은 1.2631에서 시작하여 10번째 에포크에서 1.0620으로 감소했다. 검증 손실 값이 감소하는 것은 긍정적인 신호이지만, 검증 정확도가 크게 개선되지 않은 점을 고려할 때 모델의 일반화 성능이 충분하지 않을 수 있다고 생각된다.\n",
        "\n",
        "### Conclusion (결론):\n",
        "모델의 학습은 진행되고 있지만, 검증 정확도가 크게 개선되지 않는 점을 고려할 때 모델이 과적합되고 있을 가능성이 있다.\n",
        "\n",
        "더 많은 학습 데이터를 사용하여 모델의 일반화 성능을 향상시키거나 정규화 기법을 사용하여 과적합을 방지할 수 있습니다. 또는, HP 튜닝을 통해 모델을 성능을 최적화할 수 있도록 실행해 보자.\n",
        "\n"
      ],
      "metadata": {
        "id": "VaCty3S6H1LQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## G. 챗봇 테스트하기 (추론하기 기능 Maximization)\n",
        "\n",
        "### a. decoder_inference() 함수 만들기"
      ],
      "metadata": {
        "id": "kHYy90AT4eE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder_inference(sentence):\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  # 입력된 문장을 정수 인코딩 후, 시작 토큰과 종료 토큰을 앞뒤로 추가.\n",
        "  # ex) Where have you been? → [[8331   86   30    5 1059    7 8332]]\n",
        "  sentence = tf.expand_dims(\n",
        "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
        "\n",
        "  # 디코더의 현재까지의 예측한 출력 시퀀스가 지속적으로 저장되는 변수.\n",
        "  # 처음에는 예측한 내용이 없음으로 시작 토큰만 별도 저장. ex) 8331\n",
        "  output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
        "\n",
        "  # 디코더의 인퍼런스 단계\n",
        "  for i in range(MAX_LENGTH):\n",
        "    # 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복합니다.\n",
        "    predictions = model(inputs=[sentence, output_sequence], training=False)\n",
        "    predictions = predictions[:, -1:, :]\n",
        "\n",
        "    # 현재 예측한 단어의 정수\n",
        "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "    # 만약 현재 예측한 단어가 종료 토큰이라면 for문을 종료\n",
        "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
        "      break\n",
        "\n",
        "    # 예측한 단어들은 지속적으로 output_sequence에 추가됩니다.\n",
        "    # 이 output_sequence는 다시 디코더의 입력이 됩니다.\n",
        "    output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
        "\n",
        "  return tf.squeeze(output_sequence, axis=0)\n",
        "print(\"슝=3\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5No2PG165Etv",
        "outputId": "bfa7f504-fb56-4cc5-a538-ae47e06804c1"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_generation(sentence):\n",
        "  # 입력 문장에 대해서 디코더를 동작 시켜 예측된 정수 시퀀스를 리턴받습니다.\n",
        "  prediction = decoder_inference(sentence)\n",
        "\n",
        "  # 정수 시퀀스를 다시 텍스트 시퀀스로 변환합니다.\n",
        "  predicted_sentence = tokenizer.decode(\n",
        "      [i for i in prediction if i < tokenizer.vocab_size])\n",
        "\n",
        "  print('입력 : {}'.format(sentence))\n",
        "  print('출력 : {}'.format(predicted_sentence))\n",
        "\n",
        "  return predicted_sentence\n",
        "print(\"슝=3\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JlXoaFxDji1",
        "outputId": "e6a4d568-87ca-4034-f4f7-0c65d068d653"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5. 모델 평가하기\n",
        "\n",
        "##임의의 한국어 입력문장에 대해 한국어로 답변하는 함수를 구현하기\n",
        "\n"
      ],
      "metadata": {
        "id": "LAbqoKXM4kdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Cs8cKkG0jRIZ",
        "outputId": "f77d7026-c064-4f98-c73e-1922a9c55e96"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.5.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (24.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "# 데이터 로드 및 전처리\n",
        "data = pd.read_csv('ChatbotData.csv')\n",
        "data = data.dropna()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]', '', text)\n",
        "    return text\n",
        "\n",
        "data['Q'] = data['Q'].apply(clean_text)\n",
        "data['A'] = data['A'].apply(clean_text)\n",
        "data['Q'] = data['Q'].str.strip()\n",
        "data['A'] = data['A'].str.strip()\n",
        "\n",
        "okt = Okt()\n",
        "stopwords = ['은', '는', '이', '가', '을', '를', '에', '의', '와', '한', '하다']\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = okt.morphs(text)\n",
        "    tokens = [word for word in tokens if word not in stopwords]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "data['Q'] = data['Q'].apply(preprocess_text)\n",
        "data['A'] = data['A'].apply(preprocess_text)\n",
        "\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    train_data['Q'].tolist() + train_data['A'].tolist(), target_vocab_size=2**13)\n",
        "\n",
        "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
        "MAX_LENGTH = 40\n",
        "\n",
        "def tokenize_and_filter(inputs, outputs):\n",
        "    tokenized_inputs, tokenized_outputs = [], []\n",
        "\n",
        "    for (sentence1, sentence2) in zip(inputs, outputs):\n",
        "        sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
        "        sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
        "\n",
        "        if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
        "            tokenized_inputs.append(sentence1)\n",
        "            tokenized_outputs.append(sentence2)\n",
        "\n",
        "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
        "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "    return tokenized_inputs, tokenized_outputs\n",
        "\n",
        "train_inputs, train_outputs = tokenize_and_filter(train_data['Q'], train_data['A'])\n",
        "test_inputs, test_outputs = tokenize_and_filter(test_data['Q'], test_data['A'])\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'inputs': train_inputs,\n",
        "        'dec_inputs': train_outputs[:, :-1]\n",
        "    },\n",
        "    {\n",
        "        'outputs': train_outputs[:, 1:]\n",
        "    },\n",
        "))\n",
        "\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "print(\"전처리 및 데이터셋 생성 완료!\")\n",
        "\n",
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    if not isinstance(size, int):\n",
        "        size = tf.shape(size)[0]\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\n",
        "            'd_model': self.d_model.numpy(),  # Ensure Tensor is converted to a value\n",
        "            'warmup_steps': self.warmup_steps\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "# Custom loss function\n",
        "def loss_function(y_true, y_pred):\n",
        "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
        "    loss = loss_object(y_true, y_pred)\n",
        "    loss = mask * loss\n",
        "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
        "\n",
        "# Custom accuracy function\n",
        "def accuracy(y_true, y_pred):\n",
        "    y_pred = tf.argmax(y_pred, axis=-1)\n",
        "    accuracies = tf.equal(y_true, y_pred)\n",
        "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
        "    accuracies = tf.cast(accuracies, tf.float32)\n",
        "    accuracies = mask * accuracies\n",
        "    return tf.reduce_sum(accuracies) / tf.reduce_sum(mask)\n",
        "\n",
        "# Specify the CORRECT path to your saved model\n",
        "model_path = 'chatbot_transformer_model.keras'  # Update with your actual file path\n",
        "\n",
        "# 예시: Transformer 모델 또는 원하는 모델을 정의하는 코드\n",
        "def create_model():\n",
        "    # 여기서 모델을 정의합니다. (예: Transformer 또는 다른 구조)\n",
        "    model = tf.keras.Sequential([\n",
        "        # 레이어 추가 예시\n",
        "        tf.keras.layers.InputLayer(input_shape=(MAX_LENGTH,)),\n",
        "        tf.keras.layers.Embedding(input_dim=tokenizer.vocab_size + 2, output_dim=128),\n",
        "        # 필요한 추가 레이어들\n",
        "        tf.keras.layers.Dense(tokenizer.vocab_size)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# 모델 정의\n",
        "model = create_model()\n",
        "\n",
        "# 모델 전체 저장\n",
        "model.save(\"chatbot_transformer_model.keras\")\n",
        "\n",
        "# 모델 불러오기\n",
        "model = tf.keras.models.load_model(\"chatbot_transformer_model.keras\", custom_objects={\n",
        "    'CustomSchedule': CustomSchedule,\n",
        "    'create_padding_mask': create_padding_mask,\n",
        "    'create_look_ahead_mask': create_look_ahead_mask,\n",
        "    'loss_function': loss_function,\n",
        "    'accuracy': accuracy\n",
        "})\n",
        "\n",
        "\n",
        "# Verify if the file exists at the specified path\n",
        "import os\n",
        "if os.path.exists(model_path):\n",
        "    print(\"Model file found!\")\n",
        "else:\n",
        "    print(\"Model file not found. Please check the path.\")\n",
        "\n",
        "def decoder_inference(sentence):\n",
        "    sentence = preprocess_text(sentence)\n",
        "    sentence = tf.expand_dims(START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
        "\n",
        "    # Ensure shape matches expected input\n",
        "    output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
        "\n",
        "    for i in range(MAX_LENGTH):\n",
        "        # Prepare input with correct shape\n",
        "        predictions = model([sentence, output_sequence], training=False)\n",
        "        predictions = predictions[:, -1:, :]  # Extract the last word's prediction\n",
        "\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "        if tf.equal(predicted_id, END_TOKEN[0]):\n",
        "            break\n",
        "\n",
        "        output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output_sequence, axis=0)\n",
        "\n",
        "def sentence_generation(sentence):\n",
        "    prediction = decoder_inference(sentence)\n",
        "    predicted_sentence = tokenizer.decode([i for i in prediction if i < tokenizer.vocab_size])\n",
        "\n",
        "    print('입력 : {}'.format(sentence))\n",
        "    print('출력 : {}'.format(predicted_sentence))\n",
        "\n",
        "    return predicted_sentence\n",
        "\n",
        "# Test the chatbot with an example input\n",
        "user_input = \"사내커플인데 비밀연애임 답답해\"\n",
        "response = sentence_generation(user_input)\n",
        "print(f\"User: {user_input}\")\n",
        "print(f\"Chatbot: {response}\")\n",
        "\n",
        "print(\"슝=3\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lcq_1ywBuV7S",
        "outputId": "aaaaaef8-c403-4833-f6a3-ebe4c7df0e0a"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전처리 및 데이터셋 생성 완료!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model file found!\n",
            "입력 : 사내커플인데 비밀연애임 답답해\n",
            "출력 : 헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다\n",
            "User: 사내커플인데 비밀연애임 답답해\n",
            "Chatbot: 헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다헤어진다\n",
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "# 데이터 로드 및 전처리\n",
        "data = pd.read_csv('ChatbotData.csv')\n",
        "data = data.dropna()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]', '', text)\n",
        "    return text\n",
        "\n",
        "data['Q'] = data['Q'].apply(clean_text)\n",
        "data['A'] = data['A'].apply(clean_text)\n",
        "data['Q'] = data['Q'].str.strip()\n",
        "data['A'] = data['A'].str.strip()\n",
        "\n",
        "okt = Okt()\n",
        "stopwords = ['은', '는', '이', '가', '을', '를', '에', '의', '와', '한', '하다']\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = okt.morphs(text)\n",
        "    tokens = [word for word in tokens if word not in stopwords]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "data['Q'] = data['Q'].apply(preprocess_text)\n",
        "data['A'] = data['A'].apply(preprocess_text)\n",
        "\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    train_data['Q'].tolist() + train_data['A'].tolist(), target_vocab_size=2**13)\n",
        "\n",
        "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
        "MAX_LENGTH = 40\n",
        "\n",
        "def tokenize_and_filter(inputs, outputs):\n",
        "    tokenized_inputs, tokenized_outputs = [], []\n",
        "\n",
        "    for (sentence1, sentence2) in zip(inputs, outputs):\n",
        "        sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
        "        sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
        "\n",
        "        if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
        "            tokenized_inputs.append(sentence1)\n",
        "            tokenized_outputs.append(sentence2)\n",
        "\n",
        "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
        "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "    return tokenized_inputs, tokenized_outputs\n",
        "\n",
        "train_inputs, train_outputs = tokenize_and_filter(train_data['Q'], train_data['A'])\n",
        "test_inputs, test_outputs = tokenize_and_filter(test_data['Q'], test_data['A'])\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'inputs': train_inputs,\n",
        "        'dec_inputs': train_outputs[:, :-1]\n",
        "    },\n",
        "    {\n",
        "        'outputs': train_outputs[:, 1:]\n",
        "    },\n",
        "))\n",
        "\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "print(\"전처리 및 데이터셋 생성 완료!\")\n",
        "\n",
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    if not isinstance(size, int):\n",
        "        size = tf.shape(size)[0]\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\n",
        "            'd_model': self.d_model.numpy(),  # Ensure Tensor is converted to a value\n",
        "            'warmup_steps': self.warmup_steps\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "# Custom loss function\n",
        "def loss_function(y_true, y_pred):\n",
        "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
        "    loss = loss_object(y_true, y_pred)\n",
        "    loss = mask * loss\n",
        "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
        "\n",
        "# Custom accuracy function\n",
        "def accuracy(y_true, y_pred):\n",
        "    y_pred = tf.argmax(y_pred, axis=-1)\n",
        "    accuracies = tf.equal(y_true, y_pred)\n",
        "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
        "    accuracies = tf.cast(accuracies, tf.float32)\n",
        "    accuracies = mask * accuracies\n",
        "    return tf.reduce_sum(accuracies) / tf.reduce_sum(mask)\n",
        "\n",
        "# Specify the CORRECT path to your saved model\n",
        "model_path = 'chatbot_transformer_model.keras'  # Update with your actual file path\n",
        "\n",
        "# 예시: Transformer 모델 또는 원하는 모델을 정의하는 코드\n",
        "def create_model():\n",
        "    # 여기서 모델을 정의합니다. (예: Transformer 또는 다른 구조)\n",
        "    model = tf.keras.Sequential([\n",
        "        # 레이어 추가 예시\n",
        "        tf.keras.layers.InputLayer(input_shape=(MAX_LENGTH,)),\n",
        "        tf.keras.layers.Embedding(input_dim=tokenizer.vocab_size + 2, output_dim=128),\n",
        "        # 필요한 추가 레이어들\n",
        "        tf.keras.layers.Dense(tokenizer.vocab_size)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# 모델 정의\n",
        "model = create_model()\n",
        "\n",
        "# 모델 전체 저장\n",
        "model.save(\"chatbot_transformer_model.keras\")\n",
        "\n",
        "# 모델 불러오기\n",
        "model = tf.keras.models.load_model(\"chatbot_transformer_model.keras\", custom_objects={\n",
        "    'CustomSchedule': CustomSchedule,\n",
        "    'create_padding_mask': create_padding_mask,\n",
        "    'create_look_ahead_mask': create_look_ahead_mask,\n",
        "    'loss_function': loss_function,\n",
        "    'accuracy': accuracy\n",
        "})\n",
        "\n",
        "\n",
        "# Verify if the file exists at the specified path\n",
        "import os\n",
        "if os.path.exists(model_path):\n",
        "    print(\"Model file found!\")\n",
        "else:\n",
        "    print(\"Model file not found. Please check the path.\")\n",
        "\n",
        "def decoder_inference(sentence):\n",
        "    sentence = preprocess_text(sentence)\n",
        "    sentence = tf.expand_dims(START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
        "\n",
        "    # Ensure shape matches expected input\n",
        "    output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
        "\n",
        "    for i in range(MAX_LENGTH):\n",
        "        # Prepare input with correct shape\n",
        "        predictions = model([sentence, output_sequence], training=False)\n",
        "        predictions = predictions[:, -1:, :]  # Extract the last word's prediction\n",
        "\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "        if tf.equal(predicted_id, END_TOKEN[0]):\n",
        "            break\n",
        "\n",
        "        output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output_sequence, axis=0)\n",
        "\n",
        "def sentence_generation(sentence):\n",
        "    prediction = decoder_inference(sentence)\n",
        "    predicted_sentence = tokenizer.decode([i for i in prediction if i < tokenizer.vocab_size])\n",
        "\n",
        "    print('입력 : {}'.format(sentence))\n",
        "    print('출력 : {}'.format(predicted_sentence))\n",
        "\n",
        "    return predicted_sentence\n",
        "\n",
        "# Test the chatbot with an example input\n",
        "user_input = \"사내커플인데 비밀연애임 답답해\"\n",
        "response = sentence_generation(user_input)\n",
        "print(f\"User: {user_input}\")\n",
        "print(f\"Chatbot: {response}\")\n",
        "\n",
        "print(\"슝=3\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wrIaOOMQ0-Jr",
        "outputId": "54e9610b-b6ca-4de7-e888-c75a8add9795"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전처리 및 데이터셋 생성 완료!\n",
            "Model file found!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 : 사내커플인데 비밀연애임 답답해\n",
            "출력 : 서서서서서서서서서서서서서서서서서서서서서서서서서서서서서서서서서서서서서서서서\n",
            "User: 사내커플인데 비밀연애임 답답해\n",
            "Chatbot: 서서서서서서서서서서서서서서서서서서서서서서서서서서서서서서서서서서서서서서서서\n",
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "# 데이터 로드 및 전처리\n",
        "data = pd.read_csv('ChatbotData.csv')\n",
        "data = data.dropna()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]', '', text)\n",
        "    return text\n",
        "\n",
        "data['Q'] = data['Q'].apply(clean_text)\n",
        "data['A'] = data['A'].apply(clean_text)\n",
        "data['Q'] = data['Q'].str.strip()\n",
        "data['A'] = data['A'].str.strip()\n",
        "\n",
        "okt = Okt()\n",
        "stopwords = ['은', '는', '이', '가', '을', '를', '에', '의', '와', '한', '하다']\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = okt.morphs(text)\n",
        "    tokens = [word for word in tokens if word not in stopwords]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "data['Q'] = data['Q'].apply(preprocess_text)\n",
        "data['A'] = data['A'].apply(preprocess_text)\n",
        "\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    train_data['Q'].tolist() + train_data['A'].tolist(), target_vocab_size=2**13)\n",
        "\n",
        "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
        "MAX_LENGTH = 40\n",
        "\n",
        "def tokenize_and_filter(inputs, outputs):\n",
        "    tokenized_inputs, tokenized_outputs = [], []\n",
        "\n",
        "    for (sentence1, sentence2) in zip(inputs, outputs):\n",
        "        sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
        "        sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
        "\n",
        "        if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
        "            tokenized_inputs.append(sentence1)\n",
        "            tokenized_outputs.append(sentence2)\n",
        "\n",
        "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
        "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "    return tokenized_inputs, tokenized_outputs\n",
        "\n",
        "train_inputs, train_outputs = tokenize_and_filter(train_data['Q'], train_data['A'])\n",
        "test_inputs, test_outputs = tokenize_and_filter(test_data['Q'], test_data['A'])\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'inputs': train_inputs,\n",
        "        'dec_inputs': train_outputs[:, :-1]\n",
        "    },\n",
        "    {\n",
        "        'outputs': train_outputs[:, 1:]\n",
        "    },\n",
        "))\n",
        "\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "print(\"전처리 및 데이터셋 생성 완료!\")\n",
        "\n",
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    if not isinstance(size, int):\n",
        "        size = tf.shape(size)[0]\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\n",
        "            'd_model': self.d_model.numpy(),  # Ensure Tensor is converted to a value\n",
        "            'warmup_steps': self.warmup_steps\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "# Custom loss function\n",
        "def loss_function(y_true, y_pred):\n",
        "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
        "    loss = loss_object(y_true, y_pred)\n",
        "    loss = mask * loss\n",
        "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
        "\n",
        "# Custom accuracy function\n",
        "def accuracy(y_true, y_pred):\n",
        "    y_pred = tf.argmax(y_pred, axis=-1)\n",
        "    accuracies = tf.equal(y_true, y_pred)\n",
        "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
        "    accuracies = tf.cast(accuracies, tf.float32)\n",
        "    accuracies = mask * accuracies\n",
        "    return tf.reduce_sum(accuracies) / tf.reduce_sum(mask)\n",
        "\n",
        "# Specify the CORRECT path to your saved model\n",
        "model_path = 'chatbot_transformer_model.keras'  # Update with your actual file path\n",
        "\n",
        "# 예시: Transformer 모델 또는 원하는 모델을 정의하는 코드\n",
        "def create_model():\n",
        "    # 여기서 모델을 정의합니다. (예: Transformer 또는 다른 구조)\n",
        "    model = tf.keras.Sequential([\n",
        "        # 레이어 추가 예시\n",
        "        tf.keras.layers.InputLayer(input_shape=(MAX_LENGTH,)),\n",
        "        tf.keras.layers.Embedding(input_dim=tokenizer.vocab_size + 2, output_dim=128),\n",
        "        # 필요한 추가 레이어들\n",
        "        tf.keras.layers.Dense(tokenizer.vocab_size)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# 모델 정의\n",
        "model = create_model()\n",
        "\n",
        "# 모델 전체 저장\n",
        "model.save(\"chatbot_transformer_model.keras\")\n",
        "\n",
        "# 모델 불러오기\n",
        "model = tf.keras.models.load_model(\"chatbot_transformer_model.keras\", custom_objects={\n",
        "    'CustomSchedule': CustomSchedule,\n",
        "    'create_padding_mask': create_padding_mask,\n",
        "    'create_look_ahead_mask': create_look_ahead_mask,\n",
        "    'loss_function': loss_function,\n",
        "    'accuracy': accuracy\n",
        "})\n",
        "\n",
        "# Verify if the file exists at the specified path\n",
        "import os\n",
        "if os.path.exists(model_path):\n",
        "    print(\"Model file found!\")\n",
        "else:\n",
        "    print(\"Model file not found. Please check the path.\")\n",
        "\n",
        "def decoder_inference(sentence):\n",
        "    sentence = preprocess_text(sentence)\n",
        "    sentence = tf.expand_dims(START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
        "\n",
        "    # Ensure shape matches expected input\n",
        "    output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
        "\n",
        "    for i in range(MAX_LENGTH):\n",
        "        # Prepare input with correct shape\n",
        "        predictions = model([sentence, output_sequence], training=False)\n",
        "        predictions = predictions[:, -1:, :]  # Extract the last word's prediction\n",
        "\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "        if tf.equal(predicted_id, END_TOKEN[0]):\n",
        "            break\n",
        "\n",
        "        output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output_sequence, axis=0)\n",
        "\n",
        "def sentence_generation(sentence):\n",
        "    prediction = decoder_inference(sentence)\n",
        "    predicted_sentence = tokenizer.decode([i for i in prediction if i < tokenizer.vocab_size])\n",
        "\n",
        "    print('입력 : {}'.format(sentence))\n",
        "    print('출력 : {}'.format(predicted_sentence))\n",
        "\n",
        "    return predicted_sentence\n",
        "\n",
        "# Test the chatbot with an example input\n",
        "user_input = \"사내커플인데 비밀연애임 답답해\"\n",
        "response = sentence_generation(user_input)\n",
        "print(f\"User: {user_input}\")\n",
        "print(f\"Chatbot: {response}\")\n",
        "\n",
        "print(\"슝=3\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UjRGaunX6Iao",
        "outputId": "fb59464b-015b-4dc2-ab1d-71770ef44c3d"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전처리 및 데이터셋 생성 완료!\n",
            "Model file found!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 : 사내커플인데 비밀연애임 답답해\n",
            "출력 : 똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지\n",
            "User: 사내커플인데 비밀연애임 답답해\n",
            "Chatbot: 똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지똑같지\n",
            "슝=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6.  Streamlit을 사용하여 웹 기반의 챗봇 인터페이스를 생성하기\n",
        "### 사용자가 입력창에 문장을 입력하고 “Send” 버튼을 클릭하면 챗봇이 응답을 생성하여 화면에 표시"
      ],
      "metadata": {
        "id": "T0-U9Etl9Dsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NzVfIWgS83LN",
        "outputId": "0cc00b96-989e-4e0a-90d3-a4ff4ccaef2d"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.37.1-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.1.4)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.1)\n",
            "Collecting tenacity<9,>=8.1.0 (from streamlit)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Collecting watchdog<5,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-4.0.2-py3-none-manylinux2014_x86_64.whl.metadata (38 kB)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.7.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.20.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
            "Downloading streamlit-1.37.1-py2.py3-none-any.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading watchdog-4.0.2-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: watchdog, tenacity, smmap, pydeck, gitdb, gitpython, streamlit\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.43 pydeck-0.9.1 smmap-5.0.1 streamlit-1.37.1 tenacity-8.5.0 watchdog-4.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "# 데이터 로드 및 전처리\n",
        "data = pd.read_csv('ChatbotData.csv')\n",
        "data = data.dropna()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]', '', text)\n",
        "    return text\n",
        "\n",
        "data['Q'] = data['Q'].apply(clean_text)\n",
        "data['A'] = data['A'].apply(clean_text)\n",
        "data['Q'] = data['Q'].str.strip()\n",
        "data['A'] = data['A'].str.strip()\n",
        "\n",
        "okt = Okt()\n",
        "stopwords = ['은', '는', '이', '가', '을', '를', '에', '의', '와', '한', '하다']\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = okt.morphs(text)\n",
        "    tokens = [word for word in tokens if word not in stopwords]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "data['Q'] = data['Q'].apply(preprocess_text)\n",
        "data['A'] = data['A'].apply(preprocess_text)\n",
        "\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    train_data['Q'].tolist() + train_data['A'].tolist(), target_vocab_size=2**13)\n",
        "\n",
        "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
        "MAX_LENGTH = 40\n",
        "\n",
        "def tokenize_and_filter(inputs, outputs):\n",
        "    tokenized_inputs, tokenized_outputs = [], []\n",
        "\n",
        "    for (sentence1, sentence2) in zip(inputs, outputs):\n",
        "        sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
        "        sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
        "\n",
        "        if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
        "            tokenized_inputs.append(sentence1)\n",
        "            tokenized_outputs.append(sentence2)\n",
        "\n",
        "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
        "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "    return tokenized_inputs, tokenized_outputs\n",
        "\n",
        "train_inputs, train_outputs = tokenize_and_filter(train_data['Q'], train_data['A'])\n",
        "test_inputs, test_outputs = tokenize_and_filter(test_data['Q'], test_data['A'])\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'inputs': train_inputs,\n",
        "        'dec_inputs': train_outputs[:, :-1]\n",
        "    },\n",
        "    {\n",
        "        'outputs': train_outputs[:, 1:]\n",
        "    },\n",
        "))\n",
        "\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "print(\"전처리 및 데이터셋 생성 완료!\")\n",
        "\n",
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    if not isinstance(size, int):\n",
        "        size = tf.shape(size)[0]\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\n",
        "            'd_model': self.d_model.numpy(),  # Ensure Tensor is converted to a value\n",
        "            'warmup_steps': self.warmup_steps\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "# Custom loss function\n",
        "def loss_function(y_true, y_pred):\n",
        "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
        "    loss = loss_object(y_true, y_pred)\n",
        "    loss = mask * loss\n",
        "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
        "\n",
        "# Custom accuracy function\n",
        "def accuracy(y_true, y_pred):\n",
        "    y_pred = tf.argmax(y_pred, axis=-1)\n",
        "    accuracies = tf.equal(y_true, y_pred)\n",
        "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
        "    accuracies = tf.cast(accuracies, tf.float32)\n",
        "    accuracies = mask * accuracies\n",
        "    return tf.reduce_sum(accuracies) / tf.reduce_sum(mask)\n",
        "\n",
        "# Specify the CORRECT path to your saved model\n",
        "model_path = 'chatbot_transformer_model.keras'  # Update with your actual file path\n",
        "\n",
        "# 예시: Transformer 모델 또는 원하는 모델을 정의하는 코드\n",
        "def create_model():\n",
        "    # 여기서 모델을 정의합니다. (예: Transformer 또는 다른 구조)\n",
        "    model = tf.keras.Sequential([\n",
        "        # 레이어 추가 예시\n",
        "        tf.keras.layers.InputLayer(input_shape=(MAX_LENGTH,)),\n",
        "        tf.keras.layers.Embedding(input_dim=tokenizer.vocab_size + 2, output_dim=128),\n",
        "        # 필요한 추가 레이어들\n",
        "        tf.keras.layers.Dense(tokenizer.vocab_size)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# 모델 정의\n",
        "model = create_model()\n",
        "\n",
        "# 모델 전체 저장\n",
        "model.save(\"chatbot_transformer_model.keras\")\n",
        "\n",
        "# 모델 불러오기\n",
        "model = tf.keras.models.load_model(\"chatbot_transformer_model.keras\", custom_objects={\n",
        "    'CustomSchedule': CustomSchedule,\n",
        "    'create_padding_mask': create_padding_mask,\n",
        "    'create_look_ahead_mask': create_look_ahead_mask,\n",
        "    'loss_function': loss_function,\n",
        "    'accuracy': accuracy\n",
        "})\n",
        "\n",
        "# Verify if the file exists at the specified path\n",
        "import os\n",
        "if os.path.exists(model_path):\n",
        "    print(\"Model file found!\")\n",
        "else:\n",
        "    print(\"Model file not found. Please check the path.\")\n",
        "\n",
        "def decoder_inference(sentence):\n",
        "    sentence = preprocess_text(sentence)\n",
        "    sentence = tf.expand_dims(START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
        "\n",
        "    # Ensure shape matches expected input\n",
        "    output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
        "\n",
        "    for i in range(MAX_LENGTH):\n",
        "        # Prepare input with correct shape\n",
        "        predictions = model([sentence, output_sequence], training=False)\n",
        "        predictions = predictions[:, -1:, :]  # Extract the last word's prediction\n",
        "\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "        if tf.equal(predicted_id, END_TOKEN[0]):\n",
        "            break\n",
        "\n",
        "        output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output_sequence, axis=0)\n",
        "\n",
        "def sentence_generation(sentence):\n",
        "    prediction = decoder_inference(sentence)\n",
        "    predicted_sentence = tokenizer.decode([i for i in prediction if i < tokenizer.vocab_size])\n",
        "\n",
        "    return predicted_sentence\n",
        "\n",
        "# Streamlit 앱 생성\n",
        "st.title(\"Korean Chatbot\")\n",
        "st.write(\"챗봇과 대화를 나눠보세요!\")\n",
        "\n",
        "user_input = st.text_input(\"입력:\")\n",
        "if st.button(\"Send\"):\n",
        "    if user_input:\n",
        "        response = sentence_generation(user_input)\n",
        "        st.write(f\"User: {user_input}\")\n",
        "        st.write(f\"Chatbot: {response}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ad7Q7PvJ8tLr",
        "outputId": "34d014d4-7de0-430f-ddb8-0de11695fde2"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전처리 및 데이터셋 생성 완료!\n",
            "Model file found!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
            "  warnings.warn(\n",
            "2024-08-23 06:38:05.979 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
            "2024-08-23 06:38:05.982 Session state does not function when running a script without `streamlit run`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "\n",
        "st.write('Hello, *World!* :sunglasses:')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrgF7t5f-Plc",
        "outputId": "964373ed-5cb2-4c61-8055-3fdf691c046b"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAWCfon4-R-J",
        "outputId": "57838c03-82c6-41f2-e9e7-602f159f3aea"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\n",
            "added 22 packages, and audited 23 packages in 2s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "2 \u001b[33m\u001b[1mmoderate\u001b[22m\u001b[39m severity vulnerabilities\n",
            "\n",
            "To address all issues, run:\n",
            "  npm audit fix\n",
            "\n",
            "Run `npm audit` for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/content/logs.txt &"
      ],
      "metadata": {
        "id": "qplNliF8-XqA"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rutIW2C5-caI",
        "outputId": "fec46021-dac6-4134-e7cc-5b00fbd50165"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "your url is: https://curly-bars-bathe.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}